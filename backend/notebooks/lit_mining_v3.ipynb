{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02dc5fb",
   "metadata": {},
   "source": [
    "# DOCLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.base_models import ConversionStatus\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = False\n",
    "pipeline_options.images_scale = 1.0\n",
    "\n",
    "pdf_format_options = PdfFormatOption(pipeline_options=pipeline_options)\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: pdf_format_options}\n",
    ")\n",
    "\n",
    "directory = \"pdf_pub\"\n",
    "\n",
    "# Get all PDF files in the directory\n",
    "pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "\n",
    "conv_results = converter.convert_all(pdf_files, raises_on_error=False)\n",
    "\n",
    "# Process results\n",
    "for result in conv_results:\n",
    "    if result.status == ConversionStatus.SUCCESS:\n",
    "        # Save output\n",
    "        output_path = Path(\"./outputs/full_text_v2/\") / f\"{result.input.file.stem}.md\"\n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(result.document.export_to_markdown())\n",
    "        print(f\"‚úÖ Converted: {result.input.file.name}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Failed: {result.input.file.name} - {result.errors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627baf3a",
   "metadata": {},
   "source": [
    "# Gemini Research Paper Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize the Gemini Client\n",
    "try:\n",
    "    client = genai.Client()\n",
    "except KeyError:\n",
    "    print(\"Error: Please set the GEMINI_API_KEY environment variable.\")\n",
    "    exit()\n",
    "\n",
    "def sanitize_markdown(messy_markdown_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses Gemini 2.5 Flash to clean and standardize messy markdown based on strict rules.\n",
    "    \"\"\"\n",
    "    print(f\"üîÑ Starting sanitization... (Input length: {len(messy_markdown_content)} characters)\")\n",
    "    \n",
    "    # 1. Define the Cleaning Prompt (System and User Instructions)\n",
    "    system_prompt = (\n",
    "        \"You are a highly specialized text sanitization expert. Your sole task is to clean and \"\n",
    "        \"reformat the user-provided Markdown document according to a set of strict rules. \"\n",
    "        \"Return ONLY the cleaned Markdown text. Do not add any conversational commentary, explanations, \"\n",
    "        \"or prefixes.\"\n",
    "    )\n",
    "\n",
    "    cleaning_instructions = f\"\"\"\n",
    "    REVIEW AND CLEAN THE FOLLOWING MARKDOWN DOCUMENT.\n",
    "\n",
    "    CRITICAL RULES FOR CLEANING:\n",
    "    1. Citation Removal: Remove ALL citation markers like [1], [2], (Smith et al., 2020), etc.\n",
    "    2. Heading Repair: Fix any merged headers (e.g., \"## SectionHeadingThe text...\" ‚Üí \"## Section Heading\\n\\nThe text...\").\n",
    "    3. Exclusion: Ensure the following sections are completely REMOVED if present:\n",
    "        - Table data and captions.\n",
    "        - Figure captions and image descriptions.\n",
    "        - Mathematical equations and LaTeX notation.\n",
    "        - References/Bibliography section.\n",
    "        - Acknowledgments section.\n",
    "        - Appendices.\n",
    "        - Page numbers, headers, and footers.\n",
    "    4. Fidelity: Preserve Hawaiian terms EXACTLY as written ( ªokina, kahak≈ç, diacriticals).\n",
    "    5. Formatting: Maintain correct heading hierarchy (## for main sections, ### for subsections) and ensure a double line break (empty line) between paragraphs.\n",
    "\n",
    "    --- DOCUMENT TO CLEAN ---\n",
    "    {messy_markdown_content}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"üì° Sending request to Gemini API...\")\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=[\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": cleaning_instructions}]},\n",
    "            ],\n",
    "            config=genai.types.GenerateContentConfig(\n",
    "                system_instruction=system_prompt,\n",
    "            )\n",
    "        )\n",
    "        sanitized_text = response.text\n",
    "        if sanitized_text is None:\n",
    "            print(\"‚ö†Ô∏è Warning: API returned None, returning empty string\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"API returned None\",\n",
    "                \"sanitized_text\": \"\",\n",
    "                \"input_length\": len(messy_markdown_content),\n",
    "                \"output_length\": 0\n",
    "            }\n",
    "        print(f\"‚úÖ Sanitization complete! (Output length: {len(sanitized_text)} characters)\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"sanitized_text\": sanitized_text,\n",
    "            \"input_length\": len(messy_markdown_content),\n",
    "            \"output_length\": len(sanitized_text)\n",
    "        }\n",
    "    except APIError as e:\n",
    "        print(f\"‚ùå API Error during sanitization: {e}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"sanitized_text\": f\"API Error during Sanitization: {e}\",\n",
    "            \"input_length\": len(messy_markdown_content),\n",
    "            \"output_length\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total files: 112\n",
      "üîß Max workers: 10, Rate limit: 1.0s\n",
      "------------------------------------------------------------\n",
      "‚è≠Ô∏è Skipped (cached): Cooper_etal_2013_2.md\n",
      "‚è≠Ô∏è Skipped (cached): Spirandellietal2016_ImprovingAdaptationPlanningforSLR.md\n",
      "‚è≠Ô∏è Skipped (cached): JCOASTRES-D-11-00114.md\n",
      "‚è≠Ô∏è Skipped (cached): Vitousek_SCD08.md\n",
      "‚è≠Ô∏è Skipped (cached): Bochicchio_Marine_Geo09.md\n",
      "‚è≠Ô∏è Skipped (cached): Neil_Tiffany_Chip_2009.md\n",
      "‚è≠Ô∏è Skipped (cached): Vitouseketal_NatureSR2017.md\n",
      "‚è≠Ô∏è Skipped (cached): FletcherFiersten_Hawaiichaptercoasts.md\n",
      "‚è≠Ô∏è Skipped (cached): Rubin_Fletcher_Sherman2001.md\n",
      "‚è≠Ô∏è Skipped (cached): fletcher2009_sealevelreview.md\n",
      "‚è≠Ô∏è Skipped (cached): Harney_Fletcher_JSR_2003.md\n",
      "‚è≠Ô∏è Skipped (cached): HabelEtal_WR_2017.md\n",
      "‚è≠Ô∏è Skipped (cached): Andrade_et_al_2023-coas-40-02-338-352.md\n",
      "‚è≠Ô∏è Skipped (cached): Vitousek_PSC08.md\n",
      "‚è≠Ô∏è Skipped (cached): CS2003_Norcross_LongshoreTransport.md\n",
      "‚è≠Ô∏è Skipped (cached): Conger_marinegeo_2009.md\n",
      "‚è≠Ô∏è Skipped (cached): Anderson_et_al_2015_NaturalHazards.md\n",
      "‚è≠Ô∏è Skipped (cached): Fletcher-Chapter7-climate-change.md\n",
      "‚è≠Ô∏è Skipped (cached): Rotzoll Fletcher NCC 2012.md\n",
      "‚è≠Ô∏è Skipped (cached): CNMI Climate 2016.md\n",
      "‚è≠Ô∏è Skipped (cached): computation_of_energetic_nearshore_waves.md\n",
      "‚è≠Ô∏è Skipped (cached): Act-238_HSEO_Decarbonization_Report.md\n",
      "‚è≠Ô∏è Skipped (cached): d41586-024-00917-9.md\n",
      "‚è≠Ô∏è Skipped (cached): ClimateBrief_low.md\n",
      "‚è≠Ô∏è Skipped (cached): KCAP_ClimateWP_22_0302.md\n",
      "‚è≠Ô∏è Skipped (cached): GeologyofHawaiiReefs.md\n",
      "‚è≠Ô∏è Skipped (cached): Anderson_etal_2015_JCR.md\n",
      "‚è≠Ô∏è Skipped (cached): Kane_et_al_2015_ClimateChange.md\n",
      "‚è≠Ô∏è Skipped (cached): ClimateChange_in_FSM.md\n",
      "‚è≠Ô∏è Skipped (cached): MappingShorelineCh106-124.md\n",
      "‚è≠Ô∏è Skipped (cached): GeologyGeomorphology_NWHI_Coral_Reefs2008.md\n",
      "‚è≠Ô∏è Skipped (cached): CoastalSedimentary.md\n",
      "‚è≠Ô∏è Skipped (cached): HarneyCoralReefs2000.md\n",
      "‚è≠Ô∏è Skipped (cached): ClimateChange_in_FSM_Exec_Summary.md\n",
      "‚è≠Ô∏è Skipped (cached): Norcross_SCD08.md\n",
      "‚è≠Ô∏è Skipped (cached): EngelsJSR04.md\n",
      "‚è≠Ô∏è Skipped (cached): VitouseketalProceeding07.md\n",
      "‚è≠Ô∏è Skipped (cached): ClimateChangeFSM.md\n",
      "‚è≠Ô∏è Skipped (cached): Fletcher_KaPili_Kai_09.md\n",
      "‚è≠Ô∏è Skipped (cached): Coyne-MappingCoastalErosion-1999.md\n",
      "‚è≠Ô∏è Skipped (cached): 1-s2.0-S002532272200041X-main.md\n",
      "‚è≠Ô∏è Skipped (cached): 230125_Final Booklet.md\n",
      "‚è≠Ô∏è Skipped (cached): Romine_Fletcher_inpress_HI_ShoreChange_Summary_JCR.md\n",
      "‚è≠Ô∏è Skipped (cached): annurev-marine-020923-120737.md\n",
      "‚è≠Ô∏è Skipped (cached): 230131_Final Booklet.md\n",
      "‚è≠Ô∏è Skipped (cached): KaneEtAl2014_SLRCriticalElevation.md\n",
      "‚è≠Ô∏è Skipped (cached): sherman_JSR_1999.md\n",
      "‚è≠Ô∏è Skipped (cached): Sherman et al_QuatRes_v81_p138-150.md\n",
      "‚è≠Ô∏è Skipped (cached): Conger_TGARS.md\n",
      "‚è≠Ô∏è Skipped (cached): remotesensing-12-00154.md\n",
      "‚è≠Ô∏è Skipped (cached): Genz_06-0756.md\n",
      "‚è≠Ô∏è Skipped (cached): WaikikiUAS_Defense_OnlineVersion.md\n",
      "‚è≠Ô∏è Skipped (cached): Earth s Future - 2020 - Kane - Rethinking Reef Island Stability in Relation to Anthropogenic Sea Level Rise.md\n",
      "‚è≠Ô∏è Skipped (cached): RooneyCoastalSed2003.md\n",
      "‚è≠Ô∏è Skipped (cached): Cooper_etal_2012.md\n",
      "‚è≠Ô∏è Skipped (cached): Kane2017_QuaternaryResearch.md\n",
      "‚è≠Ô∏è Skipped (cached): coastal_land_subsidence.md\n",
      "‚è≠Ô∏è Skipped (cached): Anderson_Frazer_JCR_preprint.md\n",
      "‚è≠Ô∏è Skipped (cached): i2761.md\n",
      "‚è≠Ô∏è Skipped (cached): Romine Fletcher 2013 Oahu Armoring.md\n",
      "‚è≠Ô∏è Skipped (cached): anderson_et_al_GRL_2009.md\n",
      "‚è≠Ô∏è Skipped (cached): GenzetalProceeding.md\n",
      "‚è≠Ô∏è Skipped (cached): Romine_coas-25-04-17.md\n",
      "‚è≠Ô∏è Skipped (cached): Jrooney2000.md\n",
      "‚è≠Ô∏è Skipped (cached): Genz_06-0757.md\n",
      "‚è≠Ô∏è Skipped (cached): RichmondHCH2001.md\n",
      "‚è≠Ô∏è Skipped (cached): ofr2011-1051_report_508.md\n",
      "‚è≠Ô∏è Skipped (cached): OCCL23-Sea-Level-Rise-Report-FY22-1.md\n",
      "‚è≠Ô∏è Skipped (cached): Fletcher-Chapter6-slr-hawaii.md\n",
      "‚è≠Ô∏è Skipped (cached): anderson_et_al_auxiliary_materials.md\n",
      "‚è≠Ô∏è Skipped (cached): Anderson_et_al_SciRep_2018_SLR_modeling.md\n",
      "‚è≠Ô∏è Skipped (cached): KaneEtAl2014_RankedManagementConcerns.md\n",
      "‚è≠Ô∏è Skipped (cached): Maui Shoreline Rules Chapter 203 - Dr. Chip Fletcher Testimony.md\n",
      "‚è≠Ô∏è Skipped (cached): s10584-018-2327-7.md\n",
      "‚è≠Ô∏è Skipped (cached): Cooper_etal_2013.md\n",
      "‚è≠Ô∏è Skipped (cached): IsounCoralReefs03.md\n",
      "‚è≠Ô∏è Skipped (cached): wave_driven_cross_shore.md\n",
      "‚è≠Ô∏è Skipped (cached): Romine et al 2016 Beach Erosion Under Rising Sea Level.md\n",
      "‚è≠Ô∏è Skipped (cached): ARCC2023Proceedings.md\n",
      "‚è≠Ô∏è Skipped (cached): Anderson_etal_2014_JCR.md\n",
      "‚è≠Ô∏è Skipped (cached): Kurylyketal.2025NCities.md\n",
      "‚è≠Ô∏è Skipped (cached): Habel_2019_Environ._Res._Commun._1_041005.md\n",
      "‚è≠Ô∏è Skipped (cached): CoralReefsEngels.md\n",
      "‚è≠Ô∏è Skipped (cached): 1-s2.0-S2213305421000163-main.md\n",
      "‚è≠Ô∏è Skipped (cached): AmSamoa Climate 2016.md\n",
      "‚è≠Ô∏è Skipped (cached): FletcherEtAl1990.md\n",
      "‚è≠Ô∏è Skipped (cached): remotesensing-14-05108.md\n",
      "‚è≠Ô∏è Skipped (cached): Habel_Waikiki_replen_Coastal_eng_2016.md\n",
      "‚è≠Ô∏è Skipped (cached): s41597-024-03160-z.md\n",
      "‚è≠Ô∏è Skipped (cached): Paoa_et_al-2023-Scientific_Reports.md\n",
      "‚è≠Ô∏è Skipped (cached): 43UHawLRev464.md\n",
      "‚è≠Ô∏è Skipped (cached): Use surplus to protect Sunset Beach.md\n",
      "‚è≠Ô∏è Skipped (cached): s41598-020-70577-y.md\n",
      "‚è≠Ô∏è Skipped (cached): SLR_Constraint_District_Ordinance.md\n",
      "‚è≠Ô∏è Skipped (cached): Romine_SCD08.md\n",
      "‚è≠Ô∏è Skipped (cached): Lander_et al_Envisioning_In-Situ_Sea_Level_Rise_Adaptation_for_Coastal_Cities.md\n",
      "‚è≠Ô∏è Skipped (cached): Hawaii_Natural_Resources_Law_Enforcement_Manual.10.17.2022.md\n",
      "‚è≠Ô∏è Skipped (cached): Romine et al 2013 Beach Erosion and SLR in HI.md\n",
      "‚è≠Ô∏è Skipped (cached): Bochicchio_etal_2009.md\n",
      "‚è≠Ô∏è Skipped (cached): Guam Climate 2016.md\n",
      "‚è≠Ô∏è Skipped (cached): Cochrane_etal2015.md\n",
      "‚è≠Ô∏è Skipped (cached): FletcherEtAl1993_SLRAccelerationandDrowningofDelawareBayCoast18k.md\n",
      "‚è≠Ô∏è Skipped (cached): s10584-023-03602-4.md\n",
      "‚è≠Ô∏è Skipped (cached): Habel_et_al_flood_comparison.md\n",
      "‚è≠Ô∏è Skipped (cached): Kaneetal2012.md\n",
      "üîÑ Starting sanitization... (Input length: 910 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "üîÑ Starting sanitization... (Input length: 75114 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "üîÑ Starting sanitization... (Input length: 118224 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "üîÑ Starting sanitization... (Input length: 62246 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "üîÑ Starting sanitization... (Input length: 66169 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "üîÑ Starting sanitization... (Input length: 72882 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "üîÑ Starting sanitization... (Input length: 121565 characters)\n",
      "üì° Sending request to Gemini API...\n",
      "‚ö†Ô∏è Warning: API returned None, returning empty string\n",
      "‚úÖ Processed: BeachManagementPlan_1992_scanned.md\n",
      "‚úÖ Sanitization complete! (Output length: 41179 characters)\n",
      "‚úÖ Processed: Andrade et al 2023.md\n",
      "‚úÖ Sanitization complete! (Output length: 44132 characters)\n",
      "‚úÖ Processed: 2024_Illustrating_Urban_Plans_Meguro_et_al.md\n",
      "‚úÖ Sanitization complete! (Output length: 40929 characters)\n",
      "‚úÖ Processed: Grossman_Fletcher_JSR_2003.md\n",
      "‚úÖ Sanitization complete! (Output length: 82293 characters)\n",
      "‚úÖ Processed: HawaiiReef_07.md\n",
      "‚úÖ Sanitization complete! (Output length: 46620 characters)\n",
      "‚úÖ Processed: KailuaBay.md\n",
      "‚úÖ Sanitization complete! (Output length: 74303 characters)\n",
      "‚úÖ Processed: fletcher_2024_pnas_nexus.md\n",
      "------------------------------------------------------------\n",
      "üéâ Completed in 214.86 seconds\n",
      "üìà Stats: 7 processed, 105 cached, 0 failed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from threading import Semaphore\n",
    "\n",
    "markdown_file_path = Path('outputs/full_text_v2/')\n",
    "markdown_files = list(markdown_file_path.glob(\"*.md\"))\n",
    "output_file_path = Path('outputs/cleaned_full_text_v2/')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 10  # Number of parallel requests (adjust based on your API quota)\n",
    "RATE_LIMIT_DELAY = 1.0  # Delay between requests in seconds\n",
    "\n",
    "# Semaphore to control rate limiting\n",
    "rate_limiter = Semaphore(MAX_WORKERS)\n",
    "\n",
    "def process_file(file_path: Path) -> tuple[str, bool, str]:\n",
    "    \"\"\"\n",
    "    Process a single markdown file with caching support.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filename, success, message)\n",
    "    \"\"\"\n",
    "    output_file = output_file_path / file_path.name\n",
    "    \n",
    "    # Check cache: skip if already processed\n",
    "    if output_file.exists():\n",
    "        return (file_path.name, True, \"‚è≠Ô∏è Skipped (cached)\")\n",
    "    \n",
    "    try:\n",
    "        # Read input file\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Rate limiting\n",
    "        with rate_limiter:\n",
    "            sanitized_text = sanitize_markdown(content)\n",
    "            time.sleep(RATE_LIMIT_DELAY)  # Prevent overwhelming API\n",
    "        \n",
    "        # Write output file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(sanitized_text)\n",
    "        \n",
    "        return (file_path.name, True, \"‚úÖ Processed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (file_path.name, False, f\"‚ùå Error: {str(e)}\")\n",
    "\n",
    "# Process files in parallel\n",
    "print(f\"üìä Total files: {len(markdown_files)}\")\n",
    "print(f\"üîß Max workers: {MAX_WORKERS}, Rate limit: {RATE_LIMIT_DELAY}s\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "cached_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_file = {executor.submit(process_file, file): file for file in markdown_files}\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for future in as_completed(future_to_file):\n",
    "        filename, success, message = future.result()\n",
    "        print(f\"{message}: {filename}\")\n",
    "        \n",
    "        if \"cached\" in message.lower():\n",
    "            cached_count += 1\n",
    "        elif success:\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"üéâ Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"üìà Stats: {processed_count} processed, {cached_count} cached, {failed_count} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f169e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "from enum import Enum\n",
    "from google.genai.errors import APIError\n",
    "import dotenv\n",
    "from google import genai\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize the Gemini Client\n",
    "try:\n",
    "    client = genai.Client()\n",
    "except KeyError:\n",
    "    print(\"Error: Please set the GEMINI_API_KEY environment variable.\")\n",
    "    exit()\n",
    "\n",
    "# Define the confidence levels as an enum\n",
    "class ConfidenceLevel(str, Enum):\n",
    "    HIGH = \"HIGH\"\n",
    "    MEDIUM = \"MEDIUM\"\n",
    "    LOW = \"LOW\"\n",
    "\n",
    "# Define valid layer types\n",
    "class LayerType(str, Enum):\n",
    "    PASSIVE_MARINE_FLOODING = \"passive_marine_flooding\"\n",
    "    GROUNDWATER_INUNDATION = \"groundwater_inundation\"\n",
    "    LOW_LYING_FLOODING = \"low_lying_flooding\"\n",
    "    COMPOUND_FLOODING = \"compound_flooding\"\n",
    "    DRAINAGE_BACKFLOW = \"drainage_backflow\"\n",
    "    FUTURE_EROSION_HAZARD_ZONE = \"future_erosion_hazard_zone\"\n",
    "    ANNUAL_HIGH_WAVE_FLOODING = \"annual_high_wave_flooding\"\n",
    "    EMERGENT_AND_SHALLOW_GROUNDWATER = \"emergent_and_shallow_groundwater\"\n",
    "\n",
    "# Layer validation keywords - used to verify layer assignments\n",
    "LAYER_KEYWORDS = {\n",
    "    \"passive_marine_flooding\": [\"marine inundation\", \"coastal flooding\", \"inundation zone\", \"bathtub model\", \"mhhw\", \"hydrologically connected\"],\n",
    "    \"groundwater_inundation\": [\"modflow\", \"groundwater\", \"water table rise\", \"subsurface flooding\", \"flood depth\", \"aquifer\"],\n",
    "    \"low_lying_flooding\": [\"critical elevation\", \"elevation threshold\", \"low-lying\", \"not hydrologically connected\", \"dem analysis\"],\n",
    "    \"compound_flooding\": [\"compound flooding\", \"combined effects\", \"multiple flood\", \"concurrent flooding\"],\n",
    "    \"drainage_backflow\": [\"storm drain\", \"drainage backflow\", \"sewer flooding\", \"drainage network\"],\n",
    "    \"future_erosion_hazard_zone\": [\"erosion rate\", \"m/year\", \"shoreline change\", \"coastal retreat\", \"shoreline retreat\"],\n",
    "    \"annual_high_wave_flooding\": [\"bosz\", \"wave runup\", \"wave-driven flooding\", \"extreme wave\", \"overwash\", \"gev\"],\n",
    "    \"emergent_and_shallow_groundwater\": [\"shallow groundwater\", \"water table depth\", \"groundwater level\", \"subsurface water\"]\n",
    "}\n",
    "\n",
    "# Define the Pydantic model for the analysis result\n",
    "class PaperAnalysis(BaseModel):\n",
    "    relevant: bool = Field(..., description=\"Whether the paper is relevant for the database\")\n",
    "    confidence: ConfidenceLevel = Field(..., description=\"Confidence level of the analysis\")\n",
    "    relevant_layers: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        max_length=2,\n",
    "        description=\"Most relevant data layers (max 2)\"\n",
    "    )\n",
    "    reasoning: str = Field(..., description=\"Explanation of the classification\")\n",
    "    key_findings: Optional[List[str]] = Field(default_factory=list, description=\"Key findings from the paper\")\n",
    "    quantitative_data: Dict[str, Any] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Quantitative data extracted from the paper\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('relevant_layers')\n",
    "    @classmethod\n",
    "    def validate_layers(cls, v):\n",
    "        \"\"\"Validate that layers are from the allowed set\"\"\"\n",
    "        valid_layers = [layer.value for layer in LayerType]\n",
    "        for layer in v:\n",
    "            if layer not in valid_layers:\n",
    "                print(f\"‚ö†Ô∏è Warning: Invalid layer '{layer}' - will be ignored\")\n",
    "        return [layer for layer in v if layer in valid_layers]\n",
    "    \n",
    "    class Config:\n",
    "        use_enum_values = True\n",
    "\n",
    "def validate_layer_assignment(full_text: str, assigned_layers: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates that assigned layers have supporting evidence in the text.\n",
    "    \n",
    "    Args:\n",
    "        full_text: The full text of the paper\n",
    "        assigned_layers: List of layers assigned by the AI\n",
    "        \n",
    "    Returns:\n",
    "        Dict with validation results and confidence scores\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    text_lower = full_text.lower()\n",
    "    \n",
    "    for layer in assigned_layers:\n",
    "        if layer not in LAYER_KEYWORDS:\n",
    "            validation_results[layer] = {\n",
    "                \"valid\": False,\n",
    "                \"confidence\": 0.0,\n",
    "                \"found_keywords\": [],\n",
    "                \"warning\": \"Unknown layer type\"\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        keywords = LAYER_KEYWORDS[layer]\n",
    "        found_keywords = []\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text_lower:\n",
    "                found_keywords.append(keyword)\n",
    "        \n",
    "        # Calculate confidence based on keyword matches\n",
    "        confidence = len(found_keywords) / len(keywords) if keywords else 0.0\n",
    "        \n",
    "        validation_results[layer] = {\n",
    "            \"valid\": len(found_keywords) > 0,\n",
    "            \"confidence\": round(confidence, 2),\n",
    "            \"found_keywords\": found_keywords,\n",
    "            \"warning\": None if len(found_keywords) > 0 else f\"No supporting keywords found for {layer}\"\n",
    "        }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def analyze_paper(full_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes a scientific paper and returns a validated dictionary with the analysis results.\n",
    "    Uses Gemini 2.0 Flash Thinking for deeper reasoning and includes few-shot examples.\n",
    "    \n",
    "    Args:\n",
    "        full_text (str): The full text of the paper to analyze.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A validated dictionary containing the analysis results with keys:\n",
    "            - relevant (bool)\n",
    "            - confidence (str): HIGH, MEDIUM, or LOW\n",
    "            - relevant_layers (list): Up to 2 most relevant layers\n",
    "            - reasoning (str)\n",
    "            - quantitative_data (dict)\n",
    "            - layer_validation (dict): Validation results for assigned layers\n",
    "            - key_findings (list, optional)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- SYSTEM PROMPT WITH FEW-SHOT EXAMPLES ---\n",
    "    system_prompt = \"\"\"\n",
    "    **SYSTEM INSTRUCTION: Geospatial Database Analyst (Strict JSON Output)**\n",
    "\n",
    "    Your role is to act as a specialized data analyst indexing scientific papers for the 'Hawaiian Sea Level Rise Database.' \n",
    "    You MUST adhere to all rules below and return ONLY a single, valid JSON object. \n",
    "    Do not include any text outside the JSON structure.\n",
    "    \n",
    "    **FEW-SHOT EXAMPLES:**\n",
    "    \n",
    "    Example 1 - HIGH Confidence:\n",
    "    Paper: \"Sea level rise impacts on groundwater inundation in Honolulu\"\n",
    "    Abstract mentions: \"MODFLOW modeling of Oahu aquifer shows 0.5m SLR causes water table rise of 0.3-0.4m in urban Honolulu, \n",
    "    affecting 2,500 properties by 2050.\"\n",
    "    Classification: HIGH confidence, relevant=true, layers=[\"groundwater_inundation\"]\n",
    "    Reasoning: Hawaii-specific location (Honolulu, Oahu), quantitative projections (0.5m SLR, 2,500 properties, 2050), \n",
    "    specific methodology (MODFLOW).\n",
    "    \n",
    "    Example 2 - MEDIUM Confidence:\n",
    "    Paper: \"Beach erosion patterns in tropical island environments\"\n",
    "    Abstract mentions: \"Study of 15 tropical islands including Hawaii shows erosion rates correlate with wave exposure. \n",
    "    Framework applicable to Pacific islands.\"\n",
    "    Classification: MEDIUM confidence, relevant=true, layers=[\"future_erosion_hazard_zone\"]\n",
    "    Reasoning: Hawaii mentioned but broader geographic focus, methodology applicable to Hawaii but not Hawaii-specific data.\n",
    "    \n",
    "    Example 3 - LOW Confidence:\n",
    "    Paper: \"Global sea level rise projections for the 21st century\"\n",
    "    Abstract mentions: \"IPCC AR6 scenarios project 0.5-1.0m global SLR by 2100. Hawaii tide gauge data referenced briefly.\"\n",
    "    Classification: LOW confidence, relevant=false, layers=[]\n",
    "    Reasoning: Hawaii only mentioned in passing, global focus without Hawaii-specific findings or actionable local data.\n",
    "    \"\"\"\n",
    "    \n",
    "    analysis_instructions = f\"\"\"\n",
    "    === FULL TEXT FOR ANALYSIS ===\n",
    "    {full_text}\n",
    "\n",
    "    === CORE EXECUTION STEPS ===\n",
    "    1.  **Review:** Scan the full text, prioritizing the **Methods, Results, and Discussion** sections.\n",
    "    2.  **Identify:** Extract all specific Hawaiian locations, quantitative measurements, and time projections.\n",
    "    3.  **Classify Confidence:** Determine the **Final Confidence** (HIGH/MEDIUM/LOW) using the **CONFIDENCE CRITERIA** table below.\n",
    "    4.  **Assign Layers:** Select the **1 or 2 MOST RELEVANT** layers from the **LAYER DEFINITIONS** table, based ONLY on quantitative findings in the Results/Discussion. **DO NOT** select layers based solely on methodology.\n",
    "    5.  **Extract Data:** Pull out specific quantitative data (measurements, rates, dates, locations) into the quantitative_data object.\n",
    "    6.  **Justify:** Write clear reasoning explaining your classification.\n",
    "\n",
    "    === CONFIDENCE CRITERIA (Reference Table) ===\n",
    "\n",
    "    | Level | Requirement |\n",
    "    | :--- | :--- |\n",
    "    | **HIGH** | Focuses specifically on Hawaiian locations **AND** contains quantitative data/projections **AND** includes clear, Hawaii-specific methodology. |\n",
    "    | **MEDIUM** | Methodology is applicable to Hawaii but not Hawaii-specific data **OR** mentions Hawaii but focuses on broader Pacific/global context **OR** findings are qualitative/conceptual. |\n",
    "    | **LOW** | Hawaii mentioned only in passing, no actionable data, or methodology is irrelevant to the Hawaiian context. |\n",
    "\n",
    "    === LAYER DEFINITIONS (Max 2 Layers) ===\n",
    "\n",
    "    | Layer ID | Mechanism/Focus | Keywords & Evidence (MUST be present in Results/Discussion) |\n",
    "    | :--- | :--- | :--- |\n",
    "    | **passive_marine_flooding** | Direct ocean water inundation (marine connected) | \"marine inundation\", \"coastal flooding\", \"inundation zone\", \"bathtub model\", \"MHHW datum\", \"hydrologically connected\" |\n",
    "    | **groundwater_inundation** | Flooding from rising groundwater table | \"**MODFLOW**\", \"groundwater\", \"water table rise\", \"subsurface flooding\", \"flood depths\", \"aquifer\" |\n",
    "    | **low_lying_flooding** | Low elevation areas (not marine connected) | \"critical elevation\", \"below [X]m/ft\", \"elevation threshold\", \"low-lying areas\", \"not hydrologically connected\", \"DEM analysis\" |\n",
    "    | **compound_flooding** | Multiple simultaneous flood mechanisms | \"compound flooding\", \"combined effects\", \"rainfall + high tide\", \"storm surge + rain\", \"concurrent flooding\" |\n",
    "    | **drainage_backflow** | Stormwater/sewer system flooding | \"storm drain\", \"drainage backflow\", \"sewer flooding\", \"urban coastal drainage\", \"gravity-flow networks\" |\n",
    "    | **future_erosion_hazard_zone** | Shoreline retreat rates/predictions | \"erosion rate\", \"[X] m/year\", \"shoreline change\", \"coastal retreat\" |\n",
    "    | **annual_high_wave_flooding** | Wave-driven coastal flooding events | \"**BOSZ**\", \"wave runup\", \"wave-driven flooding\", \"extreme waves\", \"overwash\", \"**GEV** analysis\" |\n",
    "    | **emergent_and_shallow_groundwater** | Groundwater near or at surface (depth to water table) | \"shallow groundwater\", \"water table depth\", \"groundwater level\", \"subsurface water\", \"GWI modeling output\" |\n",
    "\n",
    "    === LAYER SELECTION RULES ===\n",
    "    1. Select ONLY layers with explicit evidence in Results/Discussion sections.\n",
    "    2. Maximum **2 layers** per paper - choose the most prominent findings.\n",
    "    3. If paper covers multiple aspects, prioritize quantitative results over methodology.\n",
    "    4. Don't assign layers based solely on Methods - findings must be present.\n",
    "    5. If uncertain between layers, choose the one with more quantitative support.\n",
    "    6. Only assign a layer if you found specific keywords or evidence from the LAYER DEFINITIONS table above.\n",
    "\n",
    "    === RELEVANCE CRITERION ===\n",
    "    The 'relevant' field must be set to **true** ONLY if the final confidence level is determined to be **HIGH** or **MEDIUM**. \n",
    "    If the confidence is **LOW**, the paper is considered not relevant for indexing, and the field must be set to **false**.\n",
    "\n",
    "    === QUANTITATIVE DATA EXTRACTION ===\n",
    "    Extract into quantitative_data object:\n",
    "    - locations: List of specific Hawaiian place names mentioned\n",
    "    - slr_projections: Sea level rise values and years (e.g., \"0.5m by 2050\")\n",
    "    - measurements: Specific measurements (erosion rates, flood depths, etc.)\n",
    "    - timeframes: Study periods or projection years\n",
    "    \n",
    "    === TARGET JSON SCHEMA ===\n",
    "    Return a JSON object with these exact fields.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gemini API JSON schema for structured output\n",
    "    JSON_SCHEMA = {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"relevant\": {\"type\": \"BOOLEAN\"},\n",
    "            \"confidence\": {\"type\": \"STRING\", \"enum\": [\"HIGH\", \"MEDIUM\", \"LOW\"]},\n",
    "            \"relevant_layers\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"items\": {\"type\": \"STRING\"},\n",
    "                \"maxItems\": 2\n",
    "            },\n",
    "            \"reasoning\": {\"type\": \"STRING\"},\n",
    "            \"key_findings\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"items\": {\"type\": \"STRING\"}\n",
    "            },\n",
    "            \"quantitative_data\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"locations\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "                    \"slr_projections\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "                    \"measurements\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "                    \"timeframes\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"relevant\", \"confidence\", \"relevant_layers\", \"reasoning\", \n",
    "            \"quantitative_data\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"üì° Sending request to Gemini 2.5 Flash for paper analysis...\")\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=[\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": analysis_instructions}]},\n",
    "            ],\n",
    "            config=genai.types.GenerateContentConfig(\n",
    "                system_instruction=system_prompt,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=JSON_SCHEMA,\n",
    "                temperature=0.1,  # Lower temperature for more consistent analysis\n",
    "                max_output_tokens=4096  # Allow longer responses for detailed analysis\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result_json_str = response.text\n",
    "        if result_json_str is None:\n",
    "            print(\"‚ö†Ô∏è Warning: API returned None\")\n",
    "            return PaperAnalysis(\n",
    "                relevant=False,\n",
    "                confidence=ConfidenceLevel.LOW,\n",
    "                relevant_layers=[],\n",
    "                reasoning=\"API error - no response received\"\n",
    "            ).model_dump()\n",
    "        \n",
    "        # Parse JSON and validate with Pydantic\n",
    "        result_dict = json.loads(result_json_str)\n",
    "        validated_result = PaperAnalysis(**result_dict)\n",
    "        \n",
    "        # Validate layer assignments against actual text\n",
    "        if validated_result.relevant_layers:\n",
    "            layer_validation = validate_layer_assignment(full_text, validated_result.relevant_layers)\n",
    "            \n",
    "            # Log validation warnings\n",
    "            for layer, validation in layer_validation.items():\n",
    "                if not validation[\"valid\"]:\n",
    "                    print(f\"‚ö†Ô∏è Layer Validation Warning: {validation['warning']}\")\n",
    "                else:\n",
    "                    print(f\"‚úÖ Layer '{layer}' validated (confidence: {validation['confidence']}, keywords: {len(validation['found_keywords'])})\")\n",
    "        else:\n",
    "            layer_validation = {}\n",
    "        \n",
    "        print(f\"‚úÖ Analysis complete! Confidence: {validated_result.confidence}\")\n",
    "        print(f\"   Relevant: {validated_result.relevant}, Layers: {validated_result.relevant_layers}\")\n",
    "        \n",
    "        # Return as dictionary with validation results\n",
    "        result = validated_result.model_dump()\n",
    "        result['layer_validation'] = layer_validation\n",
    "        return result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"‚ùå JSON Parsing Error: {e}\")\n",
    "        return PaperAnalysis(\n",
    "            relevant=False,\n",
    "            confidence=ConfidenceLevel.LOW,\n",
    "            relevant_layers=[],\n",
    "            reasoning=f\"Failed to parse API response: {str(e)}\"\n",
    "        ).model_dump()\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        print(f\"‚ùå Pydantic Validation Error: {e}\")\n",
    "        # Try to return the raw data with error info\n",
    "        return {\n",
    "            \"status\": \"validation_error\",\n",
    "            \"error\": str(e),\n",
    "            \"relevant\": False,\n",
    "            \"confidence\": \"LOW\",\n",
    "            \"relevant_layers\": [],\n",
    "            \"reasoning\": f\"Data validation failed: {str(e)}\"\n",
    "        }\n",
    "        \n",
    "    except APIError as e:\n",
    "        print(f\"‚ùå API Error during analysis: {e}\")\n",
    "        return PaperAnalysis(\n",
    "            relevant=False,\n",
    "            confidence=ConfidenceLevel.LOW,\n",
    "            relevant_layers=[],\n",
    "            reasoning=f\"API Error: {str(e)}\"\n",
    "        ).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd968037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total files: 117\n",
      "üîß Max workers: 10, Rate limit: 1s\n",
      "üìù Output will be written to: outputs/combined_analysis_results.json\n",
      "------------------------------------------------------------\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: MEDIUM\n",
      "   Relevant: True, Layers: ['groundwater_inundation']\n",
      "‚úÖ Layer 'annual_high_wave_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "‚úÖ Layer 'passive_marine_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Vitousek_SCD08.md\n",
      "‚úÖ Processed: d41586-024-00917-9.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: ClimateBrief_low.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: CS2003_Norcross_LongshoreTransport.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: GeologyofHawaiiReefs.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: JCOASTRES-D-11-00114.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Layer 'annual_high_wave_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding']\n",
      "‚úÖ Processed: CNMI Climate 2016.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: computation_of_energetic_nearshore_waves.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 18 column 7 (char 1638)\n",
      "‚úÖ Processed: Spirandellietal2016_ImprovingAdaptationPlanningforSLR.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 11 column 5 (char 789)\n",
      "‚úÖ Processed: Rubin_Fletcher_Sherman2001.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Conger_marinegeo_2009.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'passive_marine_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.33, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'groundwater_inundation']\n",
      "‚úÖ Processed: Cooper_etal_2013_2.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Layer 'emergent_and_shallow_groundwater' validated (confidence: 0.5, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 1 column 1772 (char 1771)\n",
      "‚úÖ Processed: HabelEtal_WR_2017.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Bochicchio_Marine_Geo09.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Harney_Fletcher_JSR_2003.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Expecting value: line 66 column 1 (char 3171)\n",
      "‚úÖ Processed: ClimateChange_in_FSM.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Anderson_etal_2015_JCR.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: MEDIUM\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: GeologyGeomorphology_NWHI_Coral_Reefs2008.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: FletcherFiersten_Hawaiichaptercoasts.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Fletcher-Chapter7-climate-change.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 1 column 2537 (char 2536)\n",
      "‚úÖ Processed: Rotzoll Fletcher NCC 2012.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Expecting value: line 39 column 7 (char 1815)\n",
      "‚úÖ Processed: MappingShorelineCh106-124.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 15 column 5 (char 1464)\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.8, keywords: 4)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: KCAP_ClimateWP_22_0302.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Anderson_et_al_2015_NaturalHazards.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'future_erosion_hazard_zone']\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 56 column 7 (char 3089)\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: Kane_et_al_2015_ClimateChange.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'annual_high_wave_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: MEDIUM\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding']\n",
      "‚úÖ Processed: fletcher2009_sealevelreview.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Andrade_et_al_2023-coas-40-02-338-352.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Vitouseketal_NatureSR2017.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'annual_high_wave_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding']\n",
      "‚úÖ Processed: Vitousek_PSC08.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: HarneyCoralReefs2000.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: CoastalSedimentary.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: ClimateChange_in_FSM_Exec_Summary.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Neil_Tiffany_Chip_2009.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 8 column 16 (char 152)\n",
      "‚úÖ Processed: Norcross_SCD08.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 74 column 7 (char 2942)\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚úÖ Processed: EngelsJSR04.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Act-238_HSEO_Decarbonization_Report.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "‚úÖ Layer 'annual_high_wave_flooding' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone', 'annual_high_wave_flooding']\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.33, keywords: 2)\n",
      "‚úÖ Layer 'emergent_and_shallow_groundwater' validated (confidence: 0.5, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "‚úÖ Processed: 1-s2.0-S002532272200041X-main.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: remotesensing-12-00154.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: annurev-marine-020923-120737.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 11 column 5 (char 875)\n",
      "‚ùå JSON Parsing Error: Expecting value: line 28 column 1 (char 1513)\n",
      "‚úÖ Processed: 230131_Final Booklet.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'low_lying_flooding' validated (confidence: 0.4, keywords: 2)\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['low_lying_flooding', 'groundwater_inundation']\n",
      "‚úÖ Processed: Coyne-MappingCoastalErosion-1999.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: KaneEtAl2014_SLRCriticalElevation.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for passive_marine_flooding\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for emergent_and_shallow_groundwater\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'emergent_and_shallow_groundwater']\n",
      "‚úÖ Processed: Fletcher_KaPili_Kai_09.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: sherman_JSR_1999.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: ClimateChangeFSM.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: groundwater_inundation.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Layer 'drainage_backflow' validated (confidence: 0.25, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'drainage_backflow']\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: 230125_Final Booklet.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Kane2017_QuaternaryResearch.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚úÖ Processed: VitouseketalProceeding07.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Genz_06-0756.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: WaikikiUAS_Defense_OnlineVersion.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 25 column 7 (char 1421)\n",
      "‚úÖ Processed: coastal_land_subsidence.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 66 column 7 (char 3033)\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚úÖ Processed: Fletcher-Chapter6-slr-hawaii.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Romine_Fletcher_inpress_HI_ShoreChange_Summary_JCR.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 29 column 7 (char 2109)\n",
      "‚úÖ Processed: Anderson_et_al_SciRep_2018_SLR_modeling.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Conger_TGARS.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: Anderson_Frazer_JCR_preprint.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 7 column 5 (char 398)\n",
      "‚úÖ Processed: Earth s Future - 2020 - Kane - Rethinking Reef Island Stability in Relation to Anthropogenic Sea Level Rise.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'passive_marine_flooding' validated (confidence: 0.67, keywords: 4)\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for emergent_and_shallow_groundwater\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'emergent_and_shallow_groundwater']\n",
      "‚úÖ Processed: Cooper_etal_2012.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: GenzetalProceeding.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Expecting value: line 51 column 25 (char 2860)\n",
      "‚úÖ Processed: Anderson_etal_2014_JCR.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Sherman et al_QuatRes_v81_p138-150.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: []\n",
      "‚úÖ Processed: IsounCoralReefs03.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Jrooney2000.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: annual_wavedriven.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 1 column 481 (char 480)\n",
      "‚úÖ Layer 'passive_marine_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for groundwater_inundation\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'groundwater_inundation']\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 57 column 7 (char 2990)\n",
      "‚úÖ Processed: RooneyCoastalSed2003.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: Cooper_etal_2013.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: OCCL23-Sea-Level-Rise-Report-FY22-1.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: anderson_et_al_GRL_2009.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Romine et al 2016 Beach Erosion Under Rising Sea Level.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 17 column 5 (char 1658)\n",
      "‚úÖ Processed: RichmondHCH2001.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: BeachManagementPlan_1992_scanned.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 55 column 7 (char 2727)\n",
      "‚úÖ Processed: wave_driven_cross_shore.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Genz_06-0757.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Romine Fletcher 2013 Oahu Armoring.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚úÖ Processed: Maui Shoreline Rules Chapter 203 - Dr. Chip Fletcher Testimony.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: i2761.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Romine_coas-25-04-17.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: anderson_et_al_auxiliary_materials.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: ARCC2023Proceedings.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: ofr2011-1051_report_508.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Habel_Waikiki_replen_Coastal_eng_2016.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'future_erosion_hazard_zone']\n",
      "‚úÖ Processed: KaneEtAl2014_RankedManagementConcerns.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Layer 'emergent_and_shallow_groundwater' validated (confidence: 0.25, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "‚úÖ Layer 'passive_marine_flooding' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'groundwater_inundation']\n",
      "‚úÖ Processed: Habel_2019_Environ._Res._Commun._1_041005.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for annual_high_wave_flooding\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for passive_marine_flooding\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding', 'passive_marine_flooding']\n",
      "‚úÖ Processed: 1-s2.0-S2213305421000163-main.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: SLR_Constraint_District_Ordinance.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: Paoa_et_al-2023-Scientific_Reports.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: AmSamoa Climate 2016.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 8 column 3 (char 477)\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: s10584-018-2327-7.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 33 column 7 (char 1708)\n",
      "‚úÖ Processed: Kurylyketal.2025NCities.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: CoralReefsEngels.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 1 column 678 (char 677)\n",
      "‚úÖ Processed: drainage_backflow.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: remotesensing-14-05108.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 57 column 7 (char 2862)\n",
      "‚úÖ Layer 'passive_marine_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "‚úÖ Layer 'low_lying_flooding' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'low_lying_flooding']\n",
      "‚úÖ Processed: s41597-024-03160-z.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: passive_marine_low_lying.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'compound_flooding' validated (confidence: 0.5, keywords: 2)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['compound_flooding']\n",
      "‚úÖ Processed: compound_flooding.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.8, keywords: 4)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: s41598-020-70577-y.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for passive_marine_flooding\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone', 'passive_marine_flooding']\n",
      "‚úÖ Processed: Hawaii_Natural_Resources_Law_Enforcement_Manual.10.17.2022.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: FletcherEtAl1990.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: 43UHawLRev464.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: Cochrane_etal2015.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for annual_high_wave_flooding\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone', 'annual_high_wave_flooding']\n",
      "‚úÖ Processed: Kaneetal2012.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: Use surplus to protect Sunset Beach.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 58 column 7 (char 3293)\n",
      "‚úÖ Processed: Romine_SCD08.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "‚úÖ Processed: Romine et al 2013 Beach Erosion and SLR in HI.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Layer 'drainage_backflow' validated (confidence: 0.25, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'drainage_backflow']\n",
      "‚úÖ Processed: Lander_et al_Envisioning_In-Situ_Sea_Level_Rise_Adaptation_for_Coastal_Cities.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: Guam Climate 2016.md\n",
      "üì° Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "‚úÖ Processed: fletcher_2024_pnas_nexus.md\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 49 column 7 (char 2953)\n",
      "‚úÖ Processed: Andrade et al 2023.md\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 53 column 7 (char 2472)\n",
      "‚úÖ Processed: Bochicchio_etal_2009.md\n",
      "‚úÖ Processed: Grossman_Fletcher_JSR_2003.md\n",
      "‚úÖ Processed: s10584-023-03602-4.md\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for compound_flooding\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.5, keywords: 3)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['compound_flooding', 'groundwater_inundation']\n",
      "‚úÖ Processed: Habel_et_al_flood_comparison.md\n",
      "‚úÖ Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "‚úÖ Processed: FletcherEtAl1993_SLRAccelerationandDrowningofDelawareBayCoast18k.md\n",
      "‚ö†Ô∏è Layer Validation Warning: No supporting keywords found for compound_flooding\n",
      "‚úÖ Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "‚úÖ Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['compound_flooding', 'groundwater_inundation']\n",
      "‚ö†Ô∏è Warning: API returned None\n",
      "‚úÖ Processed: 2024_Illustrating_Urban_Plans_Meguro_et_al.md\n",
      "‚úÖ Processed: HawaiiReef_07.md\n",
      "‚ùå JSON Parsing Error: Unterminated string starting at: line 55 column 7 (char 3117)\n",
      "‚úÖ Processed: KailuaBay.md\n",
      "------------------------------------------------------------\n",
      "üíæ Writing all results to outputs/combined_analysis_results.json...\n",
      "------------------------------------------------------------\n",
      "üéâ Completed in 185.66 seconds\n",
      "üìà Stats: 117 processed, 0 failed\n",
      "üìÑ All results saved to: outputs/combined_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from threading import Semaphore\n",
    "import json\n",
    "\n",
    "input_file_path = Path('outputs/cleaned_full_text_v2/')\n",
    "input_files = list(input_file_path.glob(\"*.md\"))\n",
    "output_file_path = Path('outputs/')\n",
    "output_file = output_file_path / 'combined_analysis_results.json'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 10  # Number of parallel requests (adjust based on your API quota)\n",
    "RATE_LIMIT_DELAY = 1  # Delay between requests in seconds\n",
    "\n",
    "# Semaphore to control rate limiting\n",
    "rate_limiter = Semaphore(MAX_WORKERS)\n",
    "\n",
    "# Dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "def process_file(file_path: Path) -> tuple[str, bool, str, dict]:\n",
    "    \"\"\"\n",
    "    Process a single markdown file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filename, success, message, result_dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read input file\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Rate limiting\n",
    "        with rate_limiter:\n",
    "            analysis_result = analyze_paper(content)\n",
    "            time.sleep(RATE_LIMIT_DELAY)  # Prevent overwhelming API\n",
    "        \n",
    "        # Store result with filename as key\n",
    "        return (file_path.name, True, \"‚úÖ Processed\", analysis_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_result = {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"relevant\": False,\n",
    "            \"confidence\": \"LOW\"\n",
    "        }\n",
    "        return (file_path.name, False, f\"‚ùå Error: {str(e)}\", error_result)\n",
    "\n",
    "# Process files in parallel\n",
    "print(f\"üìä Total files: {len(input_files)}\")\n",
    "print(f\"üîß Max workers: {MAX_WORKERS}, Rate limit: {RATE_LIMIT_DELAY}s\")\n",
    "print(f\"üìù Output will be written to: {output_file}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_file = {executor.submit(process_file, file): file for file in input_files}\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for future in as_completed(future_to_file):\n",
    "        filename, success, message, result = future.result()\n",
    "        print(f\"{message}: {filename}\")\n",
    "        \n",
    "        # Store result in dictionary\n",
    "        all_results[filename] = result\n",
    "        \n",
    "        if success:\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "# Write all results to a single JSON file\n",
    "print(\"-\" * 60)\n",
    "print(f\"üíæ Writing all results to {output_file}...\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"üéâ Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"üìà Stats: {processed_count} processed, {failed_count} failed\")\n",
    "print(f\"üìÑ All results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e92f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the analysis results json\n",
    "analysis_results = json.load(open('outputs/combined_analysis_results.json'))\n",
    "cleaned_analysis_results = {}\n",
    "\n",
    "for key, value in analysis_results.items():\n",
    "    if value['relevant']:\n",
    "        cleaned_analysis_results[key] = value\n",
    "    \n",
    "    for layer in value['relevant_layers']:\n",
    "        if not value['layer_validation'][layer]['valid']:\n",
    "            cleaned_analysis_results[key]['relevant_layers'].remove(layer)\n",
    "\n",
    "with open('outputs/cleaned_analysis_results.json', 'w') as f:\n",
    "    json.dump(cleaned_analysis_results, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t9hdkg9knom",
   "metadata": {},
   "source": [
    "# LlamaIndex Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4me78u9eznr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages are now in pyproject.toml:\n",
    "# - llama-index-core\n",
    "# - llama-index-embeddings-openai\n",
    "# \n",
    "# If you need to sync dependencies, run: uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2axoznjmi7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Successfully imported OpenAIEmbedding\n",
      "üìä Loading metadata...\n",
      "   Found metadata for 49 documents\n",
      "\n",
      "üìö Creating Document objects...\n",
      "‚úÖ Loaded Vitousek_SCD08.md with 13541 characters\n",
      "‚è≠Ô∏è Skipping Spirandellietal2016_ImprovingAdaptationPlanningforSLR.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded JCOASTRES-D-11-00114.md with 26926 characters\n",
      "‚úÖ Loaded ClimateBrief_low.md with 14313 characters\n",
      "‚úÖ Loaded computation_of_energetic_nearshore_waves.md with 44927 characters\n",
      "‚úÖ Loaded CS2003_Norcross_LongshoreTransport.md with 20790 characters\n",
      "‚è≠Ô∏è Skipping Rubin_Fletcher_Sherman2001.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping GeologyofHawaiiReefs.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded d41586-024-00917-9.md with 5655 characters\n",
      "‚è≠Ô∏è Skipping Conger_marinegeo_2009.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Bochicchio_Marine_Geo09.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping CNMI Climate 2016.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded HabelEtal_WR_2017.md with 44938 characters\n",
      "‚úÖ Loaded Cooper_etal_2013_2.md with 51085 characters\n",
      "‚è≠Ô∏è Skipping ClimateChange_in_FSM.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Harney_Fletcher_JSR_2003.md with 45295 characters\n",
      "‚úÖ Loaded FletcherFiersten_Hawaiichaptercoasts.md with 20415 characters\n",
      "‚è≠Ô∏è Skipping GeologyGeomorphology_NWHI_Coral_Reefs2008.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Anderson_etal_2015_JCR.md with 41711 characters\n",
      "‚è≠Ô∏è Skipping Rotzoll Fletcher NCC 2012.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping MappingShorelineCh106-124.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping KCAP_ClimateWP_22_0302.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Anderson_et_al_2015_NaturalHazards.md with 46960 characters\n",
      "‚è≠Ô∏è Skipping fletcher2009_sealevelreview.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Vitouseketal_NatureSR2017.md with 25743 characters\n",
      "‚è≠Ô∏è Skipping Fletcher-Chapter7-climate-change.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Vitousek_PSC08.md with 26411 characters\n",
      "‚è≠Ô∏è Skipping HarneyCoralReefs2000.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Andrade_et_al_2023-coas-40-02-338-352.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Kane_et_al_2015_ClimateChange.md with 26152 characters\n",
      "‚è≠Ô∏è Skipping CoastalSedimentary.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Norcross_SCD08.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping EngelsJSR04.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Act-238_HSEO_Decarbonization_Report.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Neil_Tiffany_Chip_2009.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping ClimateChange_in_FSM_Exec_Summary.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded 1-s2.0-S002532272200041X-main.md with 58359 characters\n",
      "‚è≠Ô∏è Skipping 230131_Final Booklet.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Coyne-MappingCoastalErosion-1999.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded annurev-marine-020923-120737.md with 44965 characters\n",
      "‚è≠Ô∏è Skipping remotesensing-12-00154.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded KaneEtAl2014_SLRCriticalElevation.md with 26577 characters\n",
      "‚úÖ Loaded sherman_JSR_1999.md with 11683 characters\n",
      "‚è≠Ô∏è Skipping VitouseketalProceeding07.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping ClimateChangeFSM.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Fletcher_KaPili_Kai_09.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded 230125_Final Booklet.md with 12159 characters\n",
      "‚è≠Ô∏è Skipping coastal_land_subsidence.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Kane2017_QuaternaryResearch.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Romine_Fletcher_inpress_HI_ShoreChange_Summary_JCR.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Conger_TGARS.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Anderson_et_al_SciRep_2018_SLR_modeling.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Genz_06-0756.md with 52742 characters\n",
      "‚úÖ Loaded WaikikiUAS_Defense_OnlineVersion.md with 6110 characters\n",
      "‚è≠Ô∏è Skipping groundwater_inundation.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Earth s Future - 2020 - Kane - Rethinking Reef Island Stability in Relation to Anthropogenic Sea Level Rise.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Sherman et al_QuatRes_v81_p138-150.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Cooper_etal_2012.md with 38186 characters\n",
      "‚è≠Ô∏è Skipping Anderson_Frazer_JCR_preprint.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded IsounCoralReefs03.md with 37017 characters\n",
      "‚è≠Ô∏è Skipping Fletcher-Chapter6-slr-hawaii.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded GenzetalProceeding.md with 21758 characters\n",
      "‚è≠Ô∏è Skipping RooneyCoastalSed2003.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping OCCL23-Sea-Level-Rise-Report-FY22-1.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Jrooney2000.md with 33056 characters\n",
      "‚úÖ Loaded Anderson_etal_2014_JCR.md with 45076 characters\n",
      "‚è≠Ô∏è Skipping RichmondHCH2001.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Cooper_etal_2013.md with 11941 characters\n",
      "‚è≠Ô∏è Skipping i2761.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Romine Fletcher 2013 Oahu Armoring.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping anderson_et_al_GRL_2009.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Romine et al 2016 Beach Erosion Under Rising Sea Level.md with 27075 characters\n",
      "‚è≠Ô∏è Skipping annual_wavedriven.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Romine_coas-25-04-17.md with 56031 characters\n",
      "‚úÖ Loaded Maui Shoreline Rules Chapter 203 - Dr. Chip Fletcher Testimony.md with 8171 characters\n",
      "‚úÖ Loaded Genz_06-0757.md with 41544 characters\n",
      "‚úÖ Loaded wave_driven_cross_shore.md with 61754 characters\n",
      "‚úÖ Loaded ofr2011-1051_report_508.md with 74165 characters\n",
      "‚úÖ Loaded Habel_Waikiki_replen_Coastal_eng_2016.md with 34397 characters\n",
      "‚è≠Ô∏è Skipping BeachManagementPlan_1992_scanned.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping anderson_et_al_auxiliary_materials.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping ARCC2023Proceedings.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded KaneEtAl2014_RankedManagementConcerns.md with 25851 characters\n",
      "‚úÖ Loaded SLR_Constraint_District_Ordinance.md with 30298 characters\n",
      "‚è≠Ô∏è Skipping s10584-018-2327-7.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Habel_2019_Environ._Res._Commun._1_041005.md with 29851 characters\n",
      "‚è≠Ô∏è Skipping CoralReefsEngels.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded 1-s2.0-S2213305421000163-main.md with 26302 characters\n",
      "‚è≠Ô∏è Skipping AmSamoa Climate 2016.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping remotesensing-14-05108.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Kurylyketal.2025NCities.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping s41597-024-03160-z.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping drainage_backflow.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Paoa_et_al-2023-Scientific_Reports.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded 43UHawLRev464.md with 93858 characters\n",
      "‚è≠Ô∏è Skipping Hawaii_Natural_Resources_Law_Enforcement_Manual.10.17.2022.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded s41598-020-70577-y.md with 28116 characters\n",
      "‚úÖ Loaded passive_marine_low_lying.md with 3452 characters\n",
      "‚úÖ Loaded compound_flooding.md with 2362 characters\n",
      "‚è≠Ô∏è Skipping FletcherEtAl1990.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Kaneetal2012.md with 26875 characters\n",
      "‚è≠Ô∏è Skipping Romine_SCD08.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Lander_et al_Envisioning_In-Situ_Sea_Level_Rise_Adaptation_for_Coastal_Cities.md with 30527 characters\n",
      "‚è≠Ô∏è Skipping Cochrane_etal2015.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Use surplus to protect Sunset Beach.md with 4514 characters\n",
      "‚úÖ Loaded Romine et al 2013 Beach Erosion and SLR in HI.md with 33301 characters\n",
      "‚è≠Ô∏è Skipping Bochicchio_etal_2009.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Grossman_Fletcher_JSR_2003.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Guam Climate 2016.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping s10584-023-03602-4.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping fletcher_2024_pnas_nexus.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping HawaiiReef_07.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping Andrade et al 2023.md - not in metadata (not relevant)\n",
      "‚úÖ Loaded Habel_et_al_flood_comparison.md with 40004 characters\n",
      "‚úÖ Loaded 2024_Illustrating_Urban_Plans_Meguro_et_al.md with 44132 characters\n",
      "‚è≠Ô∏è Skipping FletcherEtAl1993_SLRAccelerationandDrowningofDelawareBayCoast18k.md - not in metadata (not relevant)\n",
      "‚è≠Ô∏è Skipping KailuaBay.md - not in metadata (not relevant)\n",
      "   Created 49 documents\n",
      "\n",
      "‚úÇÔ∏è Chunking documents semantically...\n",
      "üîß Initializing SemanticSplitterNodeParser...\n",
      "   - Embedding model: text-embedding-3-small\n",
      "   - Buffer size: 1\n",
      "   - Breakpoint threshold: 95\n",
      "‚úÖ Configured OpenAI embedding model\n",
      "üìÑ Processing 49 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e0ae315b54f29beaae7f3d21527b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e329a7043e9346238ff3626db30644ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136e6b21bf5248cb9a6758da44a5cfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0007a879c6b94b04a37eb0c354a45ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28793a6fe33d4345a648e09a8078158d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee23b3c5a1a44febcf63396572bafcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c491d9ccb734685b81b94b3041a8c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff94e84292914ed799553f4237f29d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f13d6dbf2848059146589076b8295e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f617171973b4339a81c1c02e7a91633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e5aedd58a5466a88d60938dc2b7e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b0e34bb8204477a324274d1c00314d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0075773112b459b9be2bd31abb5cff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6aee321464897b63c52b664aafa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0c4a4fe79249e18e59b8b64d610739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b976c0912f4a1ebe183859fca0d831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a03655a1c54c578fe226b147a867d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb41ae0a325b42feb7c88fc8fe46453f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3ae8aa6077477e9e33dc4e2db2abd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f19cf3bba3491ba490fbd7a75cbf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc31822ce240a382d1bab832ae5b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883fd5ab2fd1419f84ffadbd9f6baafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0bf33b9e864297af3cee00901eb840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c457b89f42a04e2f8a8a969ef753167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a11314332c34c2b8516d20057b25b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbc719bb0484fd6ac3b89c1314c5cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40822a1abb1246949aaf9133c18b6118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ce63b9152c40e29f9f7077fc912f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a27f04abcd4ef8ab8a8d0c0816396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f4fd963f12453cb93870ef6679f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdb069dc305476d9781418bcdb9c55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382c93cd543c488293345eaca399da67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6720d46357445c92f481508e3e5669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fc760befcc4ccbbd653c7158f08880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9c4b3fc8c84ee89583f38a4765911a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e09694cbbca4cfebeb1339c3c035e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d8a0a5e1a34d4e820d9e31cd2e3a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f966952b9b4ad9a805c312e69fdc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316cd32db30840049ad0f8107452a4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b3cabde5504a789eeabe7147fa65c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dae81a3b6b43ba848a8b9a5ef1e61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194f1baa2be54c55bfbd3e6c777cd693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d2e48682ad41bd80e1feba3d2effd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df15dca0184244bfb02e2dbcc6cc13bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11570e0be14e450caa0736fb3cd90a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eb7591273e4adf85529f746b2990ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84f1953b7947b1b291e53f094d6c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97389b5639ad4ee889eb85474f975f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb92b7c50d4f4c94a4787f442eaa016b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0290c9230b4d447b8aeec38f7b124f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 568 semantic chunks\n",
      "\n",
      "üìã Sample chunk:\n",
      "   Chunk ID: 44cd5c1b-65d2-4b6c-9ffd-9c51370814ed\n",
      "   Source: Vitousek_SCD08.md\n",
      "   Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "   Text preview: ## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "This paper outlines a practical approach to mapping extreme wave inundation and the inf...\n",
      "\n",
      "üíæ Saving chunks to JSON...\n",
      "üíæ Saved 568 chunks to outputs/semantic_chunks.json\n",
      "\n",
      "‚úÖ Done!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from llama_index.core import Document, Settings\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "# Import and configure OpenAI embedding\n",
    "try:\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    print(\"‚úÖ Successfully imported OpenAIEmbedding\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"Please run: pip install llama-index-embeddings-openai\")\n",
    "    raise\n",
    "\n",
    "def load_metadata(metadata_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the cleaned analysis results metadata.\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to the cleaned_analysis_results.json file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping filenames to their metadata\n",
    "    \"\"\"\n",
    "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_documents_from_markdown(\n",
    "    markdown_dir: str,\n",
    "    metadata_dict: Dict[str, Any]\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create LlamaIndex Document objects from markdown files with attached metadata.\n",
    "    \n",
    "    Args:\n",
    "        markdown_dir: Directory containing cleaned markdown files\n",
    "        metadata_dict: Dictionary of metadata from cleaned_analysis_results.json\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects with metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    markdown_path = Path(markdown_dir)\n",
    "    \n",
    "    for md_file in markdown_path.glob(\"*.md\"):\n",
    "        filename = md_file.name\n",
    "        \n",
    "        # Skip files not in metadata (not relevant)\n",
    "        if filename not in metadata_dict:\n",
    "            print(f\"‚è≠Ô∏è Skipping {filename} - not in metadata (not relevant)\")\n",
    "            continue\n",
    "        \n",
    "        # Read the markdown content\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Get metadata for this file\n",
    "        file_metadata = metadata_dict[filename]\n",
    "        \n",
    "        # Create metadata dict for the document\n",
    "        # LlamaIndex will attach this to all chunks from this document\n",
    "        doc_metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"source_file\": str(md_file),\n",
    "            \"relevant\": file_metadata.get(\"relevant\", False),\n",
    "            \"confidence\": file_metadata.get(\"confidence\", \"UNKNOWN\"),\n",
    "            \"relevant_layers\": file_metadata.get(\"relevant_layers\", []),\n",
    "            \"reasoning\": file_metadata.get(\"reasoning\", \"\"),\n",
    "            \"key_findings\": file_metadata.get(\"key_findings\", []),\n",
    "            \"locations\": file_metadata.get(\"quantitative_data\", {}).get(\"locations\", []),\n",
    "            \"slr_projections\": file_metadata.get(\"quantitative_data\", {}).get(\"slr_projections\", []),\n",
    "            \"measurements\": file_metadata.get(\"quantitative_data\", {}).get(\"measurements\", []),\n",
    "            \"timeframes\": file_metadata.get(\"quantitative_data\", {}).get(\"timeframes\", []),\n",
    "        }\n",
    "        \n",
    "        # Create Document object\n",
    "        doc = Document(\n",
    "            text=content,\n",
    "            metadata=doc_metadata,\n",
    "            id_=filename  # Use filename as document ID\n",
    "        )\n",
    "        \n",
    "        documents.append(doc)\n",
    "        print(f\"‚úÖ Loaded {filename} with {len(content)} characters\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def chunk_documents_semantic(\n",
    "    documents: List[Document],\n",
    "    buffer_size: int = 1,\n",
    "    breakpoint_percentile_threshold: int = 95,\n",
    "    embed_model_name: str = \"text-embedding-3-small\"\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Chunk documents using LlamaIndex's SemanticSplitterNodeParser.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of Document objects to chunk\n",
    "        buffer_size: Number of sentences to group together for embedding comparison\n",
    "        breakpoint_percentile_threshold: Percentile of cosine dissimilarity to use as breakpoint\n",
    "        embed_model_name: OpenAI embedding model to use\n",
    "        \n",
    "    Returns:\n",
    "        List of Node objects (chunks) with metadata\n",
    "    \"\"\"\n",
    "    print(f\"üîß Initializing SemanticSplitterNodeParser...\")\n",
    "    print(f\"   - Embedding model: {embed_model_name}\")\n",
    "    print(f\"   - Buffer size: {buffer_size}\")\n",
    "    print(f\"   - Breakpoint threshold: {breakpoint_percentile_threshold}\")\n",
    "    \n",
    "    # Initialize the OpenAI embedding model\n",
    "    embed_model = OpenAIEmbedding(\n",
    "        model=embed_model_name,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Set global embedding model to avoid HuggingFace default\n",
    "    Settings.embed_model = embed_model\n",
    "    \n",
    "    print(f\"‚úÖ Configured OpenAI embedding model\")\n",
    "    \n",
    "    # Initialize the semantic splitter\n",
    "    splitter = SemanticSplitterNodeParser(\n",
    "        buffer_size=buffer_size,\n",
    "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    print(f\"üìÑ Processing {len(documents)} documents...\")\n",
    "    \n",
    "    # Split documents into nodes (chunks)\n",
    "    nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(nodes)} semantic chunks\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "def save_chunks_to_json(nodes: List[Any], output_path: str):\n",
    "    \"\"\"\n",
    "    Save the chunks and their metadata to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        nodes: List of Node objects\n",
    "        output_path: Path to save the JSON file\n",
    "    \"\"\"\n",
    "    chunks_data = []\n",
    "    \n",
    "    for i, node in enumerate(nodes):\n",
    "        chunk_dict = {\n",
    "            \"chunk_id\": node.node_id,\n",
    "            \"text\": node.get_content(),\n",
    "            \"metadata\": node.metadata,\n",
    "            \"chunk_index\": i\n",
    "        }\n",
    "        chunks_data.append(chunk_dict)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"üíæ Saved {len(chunks_data)} chunks to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    MARKDOWN_DIR = \"outputs/cleaned_full_text_v2/\"\n",
    "    METADATA_PATH = \"outputs/cleaned_analysis_results.json\"\n",
    "    OUTPUT_PATH = \"outputs/semantic_chunks.json\"\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"üìä Loading metadata...\")\n",
    "    metadata = load_metadata(METADATA_PATH)\n",
    "    print(f\"   Found metadata for {len(metadata)} documents\")\n",
    "    \n",
    "    # Create documents\n",
    "    print(\"\\nüìö Creating Document objects...\")\n",
    "    documents = create_documents_from_markdown(MARKDOWN_DIR, metadata)\n",
    "    print(f\"   Created {len(documents)} documents\")\n",
    "    \n",
    "    # Chunk documents\n",
    "    print(\"\\n‚úÇÔ∏è Chunking documents semantically...\")\n",
    "    chunks = chunk_documents_semantic(\n",
    "        documents,\n",
    "        buffer_size=1,  # Group 1 sentence at a time (more granular)\n",
    "        breakpoint_percentile_threshold=95  # Use 95th percentile as breakpoint\n",
    "    )\n",
    "    \n",
    "    # Display sample chunk\n",
    "    if chunks:\n",
    "        print(\"\\nüìã Sample chunk:\")\n",
    "        print(f\"   Chunk ID: {chunks[0].node_id}\")\n",
    "        print(f\"   Source: {chunks[0].metadata.get('filename')}\")\n",
    "        print(f\"   Layers: {chunks[0].metadata.get('relevant_layers')}\")\n",
    "        print(f\"   Text preview: {chunks[0].get_content()[:200]}...\")\n",
    "    \n",
    "    # Save chunks\n",
    "    print(\"\\nüíæ Saving chunks to JSON...\")\n",
    "    save_chunks_to_json(chunks, OUTPUT_PATH)\n",
    "    \n",
    "    print(\"\\n‚úÖ Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhxyvm3kc9i",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "This implementation uses **LlamaIndex's SemanticSplitterNodeParser** to intelligently chunk your documents based on semantic meaning rather than arbitrary character/token limits.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Semantic Chunking**: Uses embeddings to identify natural breakpoints in text where the topic changes\n",
    "2. **Metadata Preservation**: All metadata from `cleaned_analysis_results.json` is attached to every chunk\n",
    "3. **Configurable Parameters**:\n",
    "   - `buffer_size`: Number of sentences grouped together for comparison (default: 1)\n",
    "   - `breakpoint_percentile_threshold`: Percentile threshold for determining splits (default: 95)\n",
    "   - `embed_model_name`: OpenAI embedding model to use (default: \"text-embedding-3-small\")\n",
    "\n",
    "### Metadata Attached to Each Chunk:\n",
    "\n",
    "- `filename`: Source markdown file\n",
    "- `confidence`: HIGH/MEDIUM/LOW confidence from analysis\n",
    "- `relevant_layers`: List of relevant data layers (e.g., \"groundwater_inundation\")\n",
    "- `reasoning`: Why the paper is relevant\n",
    "- `key_findings`: Key findings from the paper\n",
    "- `locations`: Hawaiian locations mentioned\n",
    "- `slr_projections`: Sea level rise projections\n",
    "- `measurements`: Specific measurements\n",
    "- `timeframes`: Study periods or projection years\n",
    "\n",
    "### Adjusting Chunk Size:\n",
    "\n",
    "- **Smaller chunks**: Increase `breakpoint_percentile_threshold` (90-99)\n",
    "- **Larger chunks**: Decrease `breakpoint_percentile_threshold` (80-90)\n",
    "- **More context**: Increase `buffer_size` (2-5 sentences)\n",
    "- **More granular**: Decrease `buffer_size` (1 sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ura8dcivz1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunk Analysis\n",
      "   Total chunks: 568\n",
      "\n",
      "üìè Chunk Size Statistics:\n",
      "   Min size: 2 characters\n",
      "   Max size: 26661 characters\n",
      "   Average size: 2777 characters\n",
      "\n",
      "üìÑ Top 10 Documents by Chunk Count:\n",
      "   ofr2011-1051_report_508.md: 26 chunks\n",
      "   43UHawLRev464.md: 26 chunks\n",
      "   Romine_coas-25-04-17.md: 23 chunks\n",
      "   1-s2.0-S002532272200041X-main.md: 21 chunks\n",
      "   wave_driven_cross_shore.md: 20 chunks\n",
      "   Cooper_etal_2013_2.md: 19 chunks\n",
      "   Genz_06-0756.md: 19 chunks\n",
      "   Genz_06-0757.md: 19 chunks\n",
      "   computation_of_energetic_nearshore_waves.md: 17 chunks\n",
      "   Anderson_etal_2014_JCR.md: 16 chunks\n",
      "\n",
      "üóÇÔ∏è Chunks by Layer:\n",
      "   future_erosion_hazard_zone: 340 chunks\n",
      "   groundwater_inundation: 138 chunks\n",
      "   passive_marine_flooding: 67 chunks\n",
      "   annual_high_wave_flooding: 65 chunks\n",
      "   emergent_and_shallow_groundwater: 45 chunks\n",
      "   drainage_backflow: 15 chunks\n",
      "   low_lying_flooding: 12 chunks\n",
      "   compound_flooding: 2 chunks\n",
      "\n",
      "‚≠ê Chunks by Confidence:\n",
      "   HIGH: 549 chunks\n",
      "   MEDIUM: 19 chunks\n",
      "\n",
      "üîç Example: Finding chunks about 'groundwater_inundation'\n",
      "   Found 138 chunks\n",
      "   Sample chunk from: d41586-024-00917-9.md\n",
      "   Preview: ## Correspondence\n",
      "\n",
      "## Opportunity in ' Anthropocene' rejection\n",
      "\n",
      "After a decade and a half of discussion, the International Commission on Stratigraphy (ICS) has rejected the proposed definition of an A...\n"
     ]
    }
   ],
   "source": [
    "# Example: Analyzing the chunks after creation\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_chunks(chunks_path: str = \"outputs/semantic_chunks.json\"):\n",
    "    \"\"\"\n",
    "    Analyze the generated chunks to understand the distribution.\n",
    "    \"\"\"\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"üìä Chunk Analysis\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print()\n",
    "    \n",
    "    # Analyze chunk sizes\n",
    "    chunk_sizes = [len(chunk['text']) for chunk in chunks]\n",
    "    print(f\"üìè Chunk Size Statistics:\")\n",
    "    print(f\"   Min size: {min(chunk_sizes)} characters\")\n",
    "    print(f\"   Max size: {max(chunk_sizes)} characters\")\n",
    "    print(f\"   Average size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\n",
    "    print()\n",
    "    \n",
    "    # Count chunks by source document\n",
    "    source_counts = Counter([chunk['metadata']['filename'] for chunk in chunks])\n",
    "    print(f\"üìÑ Top 10 Documents by Chunk Count:\")\n",
    "    for filename, count in source_counts.most_common(10):\n",
    "        print(f\"   {filename}: {count} chunks\")\n",
    "    print()\n",
    "    \n",
    "    # Count chunks by layer\n",
    "    layer_counts = Counter()\n",
    "    for chunk in chunks:\n",
    "        for layer in chunk['metadata']['relevant_layers']:\n",
    "            layer_counts[layer] += 1\n",
    "    \n",
    "    print(f\"üóÇÔ∏è Chunks by Layer:\")\n",
    "    for layer, count in layer_counts.most_common():\n",
    "        print(f\"   {layer}: {count} chunks\")\n",
    "    print()\n",
    "    \n",
    "    # Count chunks by confidence\n",
    "    confidence_counts = Counter([chunk['metadata']['confidence'] for chunk in chunks])\n",
    "    print(f\"‚≠ê Chunks by Confidence:\")\n",
    "    for confidence, count in confidence_counts.most_common():\n",
    "        print(f\"   {confidence}: {count} chunks\")\n",
    "    print()\n",
    "    \n",
    "    # Example: Find chunks related to specific layers\n",
    "    print(f\"üîç Example: Finding chunks about 'groundwater_inundation'\")\n",
    "    groundwater_chunks = [\n",
    "        chunk for chunk in chunks \n",
    "        if 'groundwater_inundation' in chunk['metadata']['relevant_layers']\n",
    "    ]\n",
    "    print(f\"   Found {len(groundwater_chunks)} chunks\")\n",
    "    if groundwater_chunks:\n",
    "        print(f\"   Sample chunk from: {groundwater_chunks[0]['metadata']['filename']}\")\n",
    "        print(f\"   Preview: {groundwater_chunks[0]['text'][:200]}...\")\n",
    "\n",
    "# Run the analysis (uncomment to use)\n",
    "analyze_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82nirb8u13g",
   "metadata": {},
   "source": [
    "# PostgreSQL Vector Database Integration\n",
    "\n",
    "Upload semantic chunks with embeddings to PostgreSQL using pgvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cqopaix2mnr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to database...\n",
      "üì¶ Enabling pgvector extension...\n",
      "üìä Creating tables...\n",
      "‚úÖ Database initialized successfully!\n",
      "Database URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text, Boolean, JSON, ARRAY, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from pgvector.sqlalchemy import Vector\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Database configuration from environment\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://***:***@localhost:5432/climate_viewer\")\n",
    "\n",
    "# Create base class for declarative models\n",
    "Base = declarative_base()\n",
    "\n",
    "class DocumentChunk(Base):\n",
    "    \"\"\"\n",
    "    SQLAlchemy model for storing document chunks with embeddings.\n",
    "    \"\"\"\n",
    "    __tablename__ = \"document_chunks\"\n",
    "    \n",
    "    # Primary key\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    # Chunk identification\n",
    "    chunk_id = Column(String(255), unique=True, nullable=False, index=True)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    \n",
    "    # Content\n",
    "    text = Column(Text, nullable=False)\n",
    "    \n",
    "    # Vector embedding (1536 dimensions for text-embedding-3-small)\n",
    "    embedding = Column(Vector(1536))\n",
    "    \n",
    "    # Metadata fields\n",
    "    filename = Column(String(255), nullable=False, index=True)\n",
    "    source_file = Column(String(512))\n",
    "    relevant = Column(Boolean, default=True)\n",
    "    confidence = Column(String(50), index=True)\n",
    "    \n",
    "    # Data layers as array\n",
    "    relevant_layers = Column(ARRAY(String), index=True)\n",
    "    \n",
    "    # Text fields for search\n",
    "    reasoning = Column(Text)\n",
    "    key_findings = Column(JSON)  # Store as JSON array\n",
    "    \n",
    "    # Quantitative data\n",
    "    locations = Column(ARRAY(String))\n",
    "    slr_projections = Column(ARRAY(String))\n",
    "    measurements = Column(ARRAY(String))\n",
    "    timeframes = Column(ARRAY(String))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<DocumentChunk(id={self.id}, chunk_id='{self.chunk_id}', filename='{self.filename}')>\"\n",
    "\n",
    "def init_database(database_url: str = None):\n",
    "    \"\"\"\n",
    "    Initialize the database with pgvector extension and create tables.\n",
    "    \n",
    "    Args:\n",
    "        database_url: PostgreSQL connection string\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine\n",
    "    \"\"\"\n",
    "    if database_url is None:\n",
    "        database_url = DATABASE_URL\n",
    "    \n",
    "    print(f\"üîó Connecting to database...\")\n",
    "    engine = create_engine(database_url, echo=False)\n",
    "    \n",
    "    # Enable pgvector extension\n",
    "    with engine.connect() as conn:\n",
    "        print(\"üì¶ Enabling pgvector extension...\")\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))        \n",
    "        conn.commit()\n",
    "    \n",
    "    # Create tables\n",
    "    print(\"üìä Creating tables...\")\n",
    "    Base.metadata.create_all(engine)\n",
    "    \n",
    "    print(\"‚úÖ Database initialized successfully!\")\n",
    "    return engine\n",
    "\n",
    "# Test the connection (uncomment to run)\n",
    "engine = init_database()\n",
    "print(f\"Database URL: {DATABASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a62ox6wmkr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Connecting to database...\n",
      "üì¶ Enabling pgvector extension...\n",
      "üìä Creating tables...\n",
      "‚úÖ Database initialized successfully!\n",
      "üìÇ Loading chunks from outputs/semantic_chunks.json...\n",
      "   Loaded 568 chunks\n",
      "üî¢ Generating embeddings for 568 texts...\n",
      "   Model: text-embedding-3-small\n",
      "   Batch size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [00:12<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generated 568 embeddings\n",
      "\n",
      "üíæ Uploading chunks to database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  24%|‚ñà‚ñà‚ñç       | 137/568 [00:00<00:01, 277.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 100 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 240/568 [00:00<00:01, 324.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 200 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 347/568 [00:01<00:00, 316.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 300 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 451/568 [00:01<00:00, 332.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 400 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 568/568 [00:01<00:00, 300.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 500 chunks...\n",
      "\n",
      "‚úÖ Upload complete!\n",
      "   Inserted: 568 chunks\n",
      "   Skipped: 0 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def generate_embeddings(texts: List[str], model: str = \"text-embedding-3-small\", batch_size: int = 100) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        model: OpenAI embedding model name\n",
    "        batch_size: Number of texts to process per API call\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    all_embeddings = []\n",
    "    \n",
    "    print(f\"üî¢ Generating embeddings for {len(texts)} texts...\")\n",
    "    print(f\"   Model: {model}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=model,\n",
    "                input=batch\n",
    "            )\n",
    "            \n",
    "            # Extract embeddings from response\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error generating embeddings for batch {i//batch_size}: {e}\")\n",
    "            # Add None placeholders for failed batch\n",
    "            all_embeddings.extend([None] * len(batch))\n",
    "    \n",
    "    print(f\"‚úÖ Generated {len([e for e in all_embeddings if e is not None])} embeddings\")\n",
    "    return all_embeddings\n",
    "\n",
    "def upload_chunks_to_database(\n",
    "    chunks_path: str,\n",
    "    database_url: str = None,\n",
    "    batch_size: int = 100,\n",
    "    embedding_batch_size: int = 100\n",
    "):\n",
    "    \"\"\"\n",
    "    Load chunks from JSON, generate embeddings, and upload to PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        chunks_path: Path to semantic_chunks.json\n",
    "        database_url: PostgreSQL connection string\n",
    "        batch_size: Number of chunks to insert per transaction\n",
    "        embedding_batch_size: Number of texts to embed per API call\n",
    "    \"\"\"\n",
    "    # Initialize database\n",
    "    engine = init_database(database_url)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    \n",
    "    # Load chunks from JSON\n",
    "    print(f\"üìÇ Loading chunks from {chunks_path}...\")\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"   Loaded {len(chunks)} chunks\")\n",
    "    \n",
    "    # Extract texts for embedding\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(texts, batch_size=embedding_batch_size)\n",
    "    \n",
    "    # Upload to database\n",
    "    print(f\"\\nüíæ Uploading chunks to database...\")\n",
    "    session = Session()\n",
    "    \n",
    "    inserted_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    try:\n",
    "        for i, (chunk, embedding) in enumerate(tqdm(zip(chunks, embeddings), total=len(chunks), desc=\"Uploading\")):\n",
    "            # Skip if embedding generation failed\n",
    "            if embedding is None:\n",
    "                print(f\"‚ö†Ô∏è Skipping chunk {i} - no embedding\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if chunk already exists\n",
    "            existing = session.query(DocumentChunk).filter_by(chunk_id=chunk['chunk_id']).first()\n",
    "            if existing:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create DocumentChunk object\n",
    "            doc_chunk = DocumentChunk(\n",
    "                chunk_id=chunk['chunk_id'],\n",
    "                chunk_index=chunk['chunk_index'],\n",
    "                text=chunk['text'],\n",
    "                embedding=embedding,\n",
    "                filename=chunk['metadata']['filename'],\n",
    "                source_file=chunk['metadata'].get('source_file'),\n",
    "                relevant=chunk['metadata'].get('relevant', True),\n",
    "                confidence=chunk['metadata'].get('confidence'),\n",
    "                relevant_layers=chunk['metadata'].get('relevant_layers', []),\n",
    "                reasoning=chunk['metadata'].get('reasoning'),\n",
    "                key_findings=chunk['metadata'].get('key_findings'),\n",
    "                locations=chunk['metadata'].get('locations', []),\n",
    "                slr_projections=chunk['metadata'].get('slr_projections', []),\n",
    "                measurements=chunk['metadata'].get('measurements', []),\n",
    "                timeframes=chunk['metadata'].get('timeframes', [])\n",
    "            )\n",
    "            \n",
    "            session.add(doc_chunk)\n",
    "            inserted_count += 1\n",
    "            \n",
    "            # Commit in batches\n",
    "            if inserted_count % batch_size == 0:\n",
    "                session.commit()\n",
    "                print(f\"   Committed {inserted_count} chunks...\")\n",
    "        \n",
    "        # Final commit\n",
    "        session.commit()\n",
    "        print(f\"\\n‚úÖ Upload complete!\")\n",
    "        print(f\"   Inserted: {inserted_count} chunks\")\n",
    "        print(f\"   Skipped: {skipped_count} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"‚ùå Error during upload: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "upload_chunks_to_database(\n",
    "    chunks_path=\"outputs/semantic_chunks.json\",\n",
    "    batch_size=100,\n",
    "    embedding_batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "moxxs78mmo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Similarity: 0.686\n",
      "   File: HabelEtal_WR_2017.md\n",
      "   Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "   Text: ### 1.1.2. Local sea-level rise\n",
      "\n",
      "In Honolulu, the semi-diurnal tide range is 0.58 m and the local rate of SLR is 1.41 ¬± 0.21 mm/yr based on monthly mean sea-level measurements at the Honolulu tide sta...\n",
      "\n",
      "2. Similarity: 0.654\n",
      "   File: Habel_et_al_flood_comparison.md\n",
      "   Layers: ['groundwater_inundation']\n",
      "   Text: Here a method is developed that identifies flooding extents and infrastructure vulnerabilities that are likely to result from alternate flood sources over coming decades. The method includes simulatio...\n",
      "\n",
      "3. Similarity: 0.651\n",
      "   File: annurev-marine-020923-120737.md\n",
      "   Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "   Text: ## 2. IMPACTS OF SEA-LEVEL-RISE-INFLUENCED COASTAL GROUNDWATER\n",
      "\n",
      "Municipalities worldwide host complex infrastructure networks that exist partially or entirely belowground. Components of this infrastru...\n",
      "\n",
      "4. Similarity: 0.631\n",
      "   File: Habel_et_al_flood_comparison.md\n",
      "   Layers: ['groundwater_inundation']\n",
      "   Text: Prevention of such flooding generally requires construction of water-tight continuous structures, like those implemented in New Orleans and the Netherlands.\n",
      "\n",
      "-   Storm-drain backflow is like direct ma...\n",
      "\n",
      "5. Similarity: 0.624\n",
      "   File: 230125_Final Booklet.md\n",
      "   Layers: ['groundwater_inundation', 'drainage_backflow']\n",
      "   Text: Identify buildings with at-grade or below-grade spaces that appear vulnerable to flooding based on our observation and reasoning.\n",
      "4.  Identify older buildings for retrofit, recognizing that they are m...\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def vector_search(\n",
    "    query: str,\n",
    "    database_url: str = None,\n",
    "    top_k: int = 5,\n",
    "    filters: Dict[str, Any] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform vector similarity search on the database.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        database_url: PostgreSQL connection string\n",
    "        top_k: Number of results to return\n",
    "        filters: Optional filters (e.g., {\"confidence\": \"HIGH\", \"relevant_layers\": [\"groundwater_inundation\"]})\n",
    "        \n",
    "    Returns:\n",
    "        List of matching chunks with metadata\n",
    "    \"\"\"\n",
    "    if database_url is None:\n",
    "        database_url = DATABASE_URL\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = response.data[0].embedding\n",
    "    \n",
    "    # Create database session\n",
    "    engine = create_engine(database_url, echo=False)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    try:\n",
    "        # Build query\n",
    "        query_obj = session.query(\n",
    "            DocumentChunk,\n",
    "            DocumentChunk.embedding.cosine_distance(query_embedding).label('distance')\n",
    "        )\n",
    "        \n",
    "        # Apply filters\n",
    "        if filters:\n",
    "            if 'confidence' in filters:\n",
    "                query_obj = query_obj.filter(DocumentChunk.confidence == filters['confidence'])\n",
    "            if 'relevant_layers' in filters:\n",
    "                # Check if arrays have any overlapping elements using PostgreSQL && operator\n",
    "                # This checks if any element in relevant_layers array matches any element in filter array\n",
    "                from sqlalchemy import cast, ARRAY, String\n",
    "                filter_array = cast(filters['relevant_layers'], ARRAY(String))\n",
    "                query_obj = query_obj.filter(\n",
    "                    DocumentChunk.relevant_layers.op('&&')(filter_array)\n",
    "                )\n",
    "            if 'filename' in filters:\n",
    "                query_obj = query_obj.filter(DocumentChunk.filename == filters['filename'])\n",
    "        \n",
    "        # Order by similarity and limit\n",
    "        results = query_obj.order_by('distance').limit(top_k).all()\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for chunk, distance in results:\n",
    "            formatted_results.append({\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'text': chunk.text,\n",
    "                'distance': float(distance),\n",
    "                'similarity': 1 - float(distance),  # Convert distance to similarity\n",
    "                'metadata': {\n",
    "                    'filename': chunk.filename,\n",
    "                    'confidence': chunk.confidence,\n",
    "                    'relevant_layers': chunk.relevant_layers,\n",
    "                    'locations': chunk.locations,\n",
    "                    'slr_projections': chunk.slr_projections,\n",
    "                    'key_findings': chunk.key_findings\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "results = vector_search(\n",
    "    query=\"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    top_k=5,\n",
    "    filters={\"relevant_layers\": [\"groundwater_inundation\"]}\n",
    ")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"   File: {result['metadata']['filename']}\")\n",
    "    print(f\"   Layers: {result['metadata']['relevant_layers']}\")\n",
    "    print(f\"   Text: {result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443dkpjzk5y",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "\n",
    "### 1. Set up your environment variables\n",
    "\n",
    "Create a `.env` file in the notebooks directory with:\n",
    "\n",
    "```bash\n",
    "DATABASE_URL=postgresql://***:***@localhost:5432/climate_viewer\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```\n",
    "\n",
    "### 2. Initialize the database\n",
    "\n",
    "```python\n",
    "engine = init_database()\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Enable the `pgvector` extension\n",
    "- Create the `document_chunks` table with all necessary fields\n",
    "- Set up vector indexing\n",
    "\n",
    "### 3. Upload chunks to PostgreSQL\n",
    "\n",
    "```python\n",
    "upload_chunks_to_database(\n",
    "    chunks_path=\"outputs/semantic_chunks.json\",\n",
    "    batch_size=100,\n",
    "    embedding_batch_size=100\n",
    ")\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Load chunks from JSON\n",
    "- Generate embeddings using OpenAI API (text-embedding-3-small)\n",
    "- Upload chunks with embeddings to PostgreSQL\n",
    "- Skip duplicates automatically\n",
    "\n",
    "### 4. Search the database\n",
    "\n",
    "```python\n",
    "results = vector_search(\n",
    "    query=\"What are the impacts of sea level rise on groundwater?\",\n",
    "    top_k=5,\n",
    "    filters={\"relevant_layers\": [\"groundwater_inundation\"]}\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"File: {result['metadata']['filename']}\")\n",
    "    print(f\"Text: {result['text'][:200]}...\")\n",
    "```\n",
    "\n",
    "### Database Schema\n",
    "\n",
    "The `document_chunks` table includes:\n",
    "- **Vector embeddings** (1536 dimensions) for similarity search\n",
    "- **All metadata** from your analysis (layers, confidence, locations, etc.)\n",
    "- **Indexed fields** for fast filtering by filename, confidence, and layers\n",
    "- **Full-text content** for each chunk\n",
    "\n",
    "### Vector Search Features\n",
    "\n",
    "- **Cosine similarity** for finding semantically similar chunks\n",
    "- **Filter by layers** (e.g., only groundwater_inundation chunks)\n",
    "- **Filter by confidence** (HIGH, MEDIUM, LOW)\n",
    "- **Filter by filename** to search within specific papers\n",
    "- **Combine filters** for precise searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fsux2h4mmcu",
   "metadata": {},
   "source": [
    "# RAG with LlamaIndex\n",
    "\n",
    "Use LlamaIndex for advanced RAG with the PostgreSQL vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0qcf04lu3rql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LlamaIndex configured with:\n",
      "   LLM: gpt-4o-mini\n",
      "   Embeddings: text-embedding-3-small\n",
      "================================================================================\n",
      "üîß DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "üìã Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "üìã Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "üìã Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "üìã Step 4: Creating PGVectorStore...\n",
      "   ‚úÖ PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "üìã Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   ‚úÖ Added 568 nodes to docstore\n",
      "   ‚úÖ Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "üìã Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 5\n",
      "   ‚úÖ Retriever created successfully\n",
      "\n",
      "üìã Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.3\n",
      "   ‚úÖ Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query engine ready!\n",
      "   Retrieval: top 5 chunks\n",
      "   Min similarity: 0.3\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from sqlalchemy.engine import make_url\n",
    "import os\n",
    "\n",
    "# Configure LlamaIndex global settings\n",
    "Settings.llm = LlamaOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LlamaIndex configured with:\")\n",
    "print(f\"   LLM: {Settings.llm.model}\")\n",
    "print(f\"   Embeddings: text-embedding-3-small\")\n",
    "\n",
    "def create_llamaindex_query_engine(\n",
    "    database_url: str = None,\n",
    "    table_name: str = \"document_chunks\",\n",
    "    similarity_top_k: int = 5,\n",
    "    similarity_cutoff: float = 0.3,  # Lowered default from 0.7 to get more results\n",
    "    debug: bool = True  # Enable detailed debugging\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LlamaIndex query engine connected to PostgreSQL vector store.\n",
    "    \n",
    "    Args:\n",
    "        database_url: PostgreSQL connection string (postgresql://***:***@host:port/db)\n",
    "        table_name: Name of the table with vectors\n",
    "        similarity_top_k: Number of chunks to retrieve\n",
    "        similarity_cutoff: Minimum similarity score (0-1)\n",
    "        debug: Enable detailed debugging output\n",
    "        \n",
    "    Returns:\n",
    "        LlamaIndex QueryEngine\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üîß DEBUG: Creating LlamaIndex Query Engine\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Get database URL\n",
    "    if database_url is None:\n",
    "        database_url = os.getenv(\"DATABASE_URL\")\n",
    "        if debug:\n",
    "            print(f\"üìã Step 1: Database URL from environment\")\n",
    "            print(f\"   URL: {database_url}\")\n",
    "    \n",
    "    if not database_url:\n",
    "        raise ValueError(\"DATABASE_URL not set. Please set it in your .env file.\")\n",
    "    \n",
    "    # Step 2: Parse connection string\n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 2: Parsing connection string...\")\n",
    "    url = make_url(database_url)\n",
    "    if debug:\n",
    "        print(f\"   Host: {url.host or 'localhost'}\")\n",
    "        print(f\"   Port: {url.port or 5432}\")\n",
    "        print(f\"   User: {url.username}\")\n",
    "        print(f\"   Database: {url.database}\")\n",
    "        print(f\"   Table: {table_name}\")\n",
    "    \n",
    "    # Step 3: Verify database connection and table\n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 3: Verifying database connection and table...\")\n",
    "        from sqlalchemy import create_engine, text\n",
    "        try:\n",
    "            test_engine = create_engine(database_url, echo=False)\n",
    "            with test_engine.connect() as conn:\n",
    "                # Check if table exists\n",
    "                result = conn.execute(text(f\"\"\"\n",
    "                    SELECT EXISTS (\n",
    "                        SELECT FROM information_schema.tables \n",
    "                        WHERE table_name = '{table_name}'\n",
    "                    )\n",
    "                \"\"\"))\n",
    "                table_exists = result.scalar()\n",
    "                print(f\"   Table exists: {table_exists}\")\n",
    "                \n",
    "                if table_exists:\n",
    "                    # Count rows\n",
    "                    result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name}\"))\n",
    "                    row_count = result.scalar()\n",
    "                    print(f\"   Total rows: {row_count}\")\n",
    "                    \n",
    "                    # Count rows with embeddings\n",
    "                    result = conn.execute(text(f\"\"\"\n",
    "                        SELECT COUNT(*) FROM {table_name} \n",
    "                        WHERE embedding IS NOT NULL\n",
    "                    \"\"\"))\n",
    "                    embedding_count = result.scalar()\n",
    "                    print(f\"   Rows with embeddings: {embedding_count}\")\n",
    "                    \n",
    "                    if embedding_count == 0:\n",
    "                        print(\"   ‚ö†Ô∏è  WARNING: No embeddings found in table!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Warning: Could not verify database: {e}\")\n",
    "    \n",
    "    # Step 4: Create PGVectorStore\n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 4: Creating PGVectorStore...\")\n",
    "    try:\n",
    "        vector_store = PGVectorStore.from_params(\n",
    "            host=url.host or \"localhost\",\n",
    "            port=str(url.port) if url.port else \"5432\",\n",
    "            user=url.username,\n",
    "            password=url.password,\n",
    "            database=url.database,\n",
    "            table_name=table_name,\n",
    "            embed_dim=1536,  # text-embedding-3-small dimension\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"   ‚úÖ PGVectorStore created successfully\")\n",
    "            print(f\"   Embedding dimension: 1536\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating PGVectorStore: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Step 5: Create index from vector store and populate docstore\n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 5: Creating VectorStoreIndex from vector store...\")\n",
    "    try:\n",
    "        from llama_index.core import StorageContext\n",
    "        from llama_index.core.schema import TextNode\n",
    "        from llama_index.core import Document\n",
    "        \n",
    "        # Create storage context with vector store\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        # Load nodes from database to populate docstore\n",
    "        if debug:\n",
    "            print(f\"   Loading nodes from database to populate docstore...\")\n",
    "        \n",
    "        # Query the database to get all chunks\n",
    "        from sqlalchemy import create_engine, text\n",
    "        db_engine = create_engine(database_url, echo=False)\n",
    "        nodes_to_add = []\n",
    "        \n",
    "        with db_engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"\"\"\n",
    "                SELECT chunk_id, text, filename, confidence, relevant_layers, \n",
    "                       locations, slr_projections, measurements, timeframes,\n",
    "                       key_findings, reasoning, source_file, relevant\n",
    "                FROM {table_name}\n",
    "            \"\"\"))\n",
    "            \n",
    "            for row in result:\n",
    "                # Create metadata dict\n",
    "                metadata = {\n",
    "                    \"filename\": row[2] or \"\",\n",
    "                    \"source_file\": row[11] or \"\",\n",
    "                    \"relevant\": row[12] if row[12] is not None else True,\n",
    "                    \"confidence\": row[3] or \"\",\n",
    "                    \"relevant_layers\": row[4] or [],\n",
    "                    \"locations\": row[5] or [],\n",
    "                    \"slr_projections\": row[6] or [],\n",
    "                    \"measurements\": row[7] or [],\n",
    "                    \"timeframes\": row[8] or [],\n",
    "                }\n",
    "                \n",
    "                if row[9]:  # key_findings\n",
    "                    metadata[\"key_findings\"] = row[9]\n",
    "                if row[10]:  # reasoning\n",
    "                    metadata[\"reasoning\"] = row[10]\n",
    "                \n",
    "                # Create TextNode\n",
    "                node = TextNode(\n",
    "                    text=row[1] or \"\",\n",
    "                    id_=row[0],  # Use chunk_id as node ID\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                nodes_to_add.append(node)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Loaded {len(nodes_to_add)} nodes from database\")\n",
    "        \n",
    "        # Add nodes to docstore\n",
    "        if nodes_to_add:\n",
    "            if debug:\n",
    "                print(f\"   Attempting to add {len(nodes_to_add)} nodes to docstore...\")\n",
    "            \n",
    "            # Try batch add first (more efficient)\n",
    "            try:\n",
    "                if debug:\n",
    "                    print(f\"   Trying batch add_documents...\")\n",
    "                storage_context.docstore.add_documents(nodes_to_add, allow_update=True)\n",
    "                if debug:\n",
    "                    print(f\"   ‚úÖ Batch add_documents succeeded\")\n",
    "            except Exception as batch_error:\n",
    "                if debug:\n",
    "                    print(f\"   ‚ö†Ô∏è  Batch add failed: {batch_error}\")\n",
    "                    print(f\"   Trying individual adds...\")\n",
    "                # Fallback to individual adds\n",
    "                success_count = 0\n",
    "                for i, node in enumerate(nodes_to_add):\n",
    "                    try:\n",
    "                        storage_context.docstore.add_documents([node], allow_update=True)\n",
    "                        success_count += 1\n",
    "                    except Exception as e:\n",
    "                        if debug and i < 3:  # Only show first 3 errors\n",
    "                            print(f\"   ‚ö†Ô∏è  Could not add node {node.node_id}: {e}\")\n",
    "                if debug:\n",
    "                    print(f\"   ‚úÖ Added {success_count}/{len(nodes_to_add)} nodes individually\")\n",
    "            \n",
    "            # Verify nodes were added\n",
    "            if debug:\n",
    "                try:\n",
    "                    docstore = storage_context.docstore\n",
    "                    if hasattr(docstore, 'docs'):\n",
    "                        actual_count = len(docstore.docs)\n",
    "                        print(f\"   Verified: {actual_count} nodes in docstore\")\n",
    "                        if actual_count == 0:\n",
    "                            print(f\"   ‚ö†Ô∏è  WARNING: add_documents didn't work! Docstore is still empty.\")\n",
    "                            print(f\"   Trying alternative method...\")\n",
    "                            # Try direct dictionary access as last resort\n",
    "                            if hasattr(docstore, 'docs'):\n",
    "                                for node in nodes_to_add[:10]:  # Try first 10 as test\n",
    "                                    docstore.docs[node.node_id] = node\n",
    "                                print(f\"   Test: Added 10 nodes via direct access\")\n",
    "                                print(f\"   Docstore now has: {len(docstore.docs)} nodes\")\n",
    "                except Exception as verify_error:\n",
    "                    if debug:\n",
    "                        print(f\"   Could not verify docstore: {verify_error}\")\n",
    "        \n",
    "        # Verify docstore before creating index\n",
    "        if debug:\n",
    "            try:\n",
    "                docstore = storage_context.docstore\n",
    "                if hasattr(docstore, 'docs'):\n",
    "                    print(f\"   Documents in docstore BEFORE index creation: {len(docstore.docs)}\")\n",
    "                elif hasattr(docstore, 'get_all_document_hashes'):\n",
    "                    all_hashes = docstore.get_all_document_hashes()\n",
    "                    print(f\"   Documents in docstore BEFORE index creation: {len(all_hashes)}\")\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"   Could not check docstore before index: {e}\")\n",
    "        \n",
    "        # Create index with populated storage context\n",
    "        # IMPORTANT: Pass storage_context to ensure docstore is preserved\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   ‚úÖ Index created successfully\")\n",
    "            # Verify docstore size AFTER index creation\n",
    "            try:\n",
    "                docstore = index.storage_context.docstore\n",
    "                if hasattr(docstore, 'docs'):\n",
    "                    doc_count = len(docstore.docs)\n",
    "                    print(f\"   Documents in docstore AFTER index creation: {doc_count}\")\n",
    "                    if doc_count == 0:\n",
    "                        print(f\"   ‚ö†Ô∏è  WARNING: Docstore is empty after index creation!\")\n",
    "                        print(f\"   This means the storage_context wasn't preserved.\")\n",
    "                elif hasattr(docstore, 'get_all_document_hashes'):\n",
    "                    # Alternative way to check docstore size\n",
    "                    all_hashes = docstore.get_all_document_hashes()\n",
    "                    print(f\"   Documents in docstore AFTER index creation: {len(all_hashes)}\")\n",
    "                else:\n",
    "                    # Try to access directly (using hasattr to avoid linter errors)\n",
    "                    if hasattr(docstore, '_node_id_to_ref_doc_info'):\n",
    "                        ref_doc_info = getattr(docstore, '_node_id_to_ref_doc_info', {})\n",
    "                        ref_doc_count = len(ref_doc_info) if ref_doc_info else 0\n",
    "                        print(f\"   Node references in docstore: {ref_doc_count}\")\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"   Could not verify docstore size: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating index: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    # Step 6: Create retriever\n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 6: Creating VectorIndexRetriever...\")\n",
    "        print(f\"   Similarity top_k: {similarity_top_k}\")\n",
    "    try:\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=index,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"   ‚úÖ Retriever created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating retriever: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Step 7: Create query engine with post-processors\n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 7: Creating RetrieverQueryEngine...\")\n",
    "        print(f\"   Similarity cutoff: {similarity_cutoff}\")\n",
    "    try:\n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[\n",
    "                SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
    "            ]\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"   ‚úÖ Query engine created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error creating query engine: {e}\")\n",
    "        raise\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ Query engine ready!\")\n",
    "        print(f\"   Retrieval: top {similarity_top_k} chunks\")\n",
    "        print(f\"   Min similarity: {similarity_cutoff}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"‚úÖ Query engine ready!\")\n",
    "        print(f\"   Retrieval: top {similarity_top_k} chunks\")\n",
    "        print(f\"   Min similarity: {similarity_cutoff}\\n\")\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "# Example: Create query engine (uncomment to run)\n",
    "# Note: similarity_cutoff of 0.3 is now the default (lowered from 0.7)\n",
    "# Adjust based on your needs - lower = more results, higher = more relevant\n",
    "query_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    similarity_cutoff=0.3,  # Default is now 0.3\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ok19r4un2gm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîß DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "üìã Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "üìã Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "üìã Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "üìã Step 4: Creating PGVectorStore...\n",
      "   ‚úÖ PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "üìã Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   ‚úÖ Added 568 nodes to docstore\n",
      "   ‚úÖ Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "üìã Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 5\n",
      "   ‚úÖ Retriever created successfully\n",
      "\n",
      "üìã Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.3\n",
      "   ‚úÖ Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query engine ready!\n",
      "   Retrieval: top 5 chunks\n",
      "   Min similarity: 0.3\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîç DEBUG: Querying RAG System\n",
      "================================================================================\n",
      "üìã Using provided query engine: <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "   Retriever: <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "   Post-processors: 1\n",
      "\n",
      "üìã Question: What are the impacts of sea level rise on groundwater in Honolulu?\n",
      "\n",
      "üìã Step 1: Generating query embedding...\n",
      "   Retrieving nodes with retriever...\n",
      "   ‚úÖ Retrieved 0 nodes from retriever\n",
      "   ‚ö†Ô∏è  WARNING: Retriever returned 0 nodes!\n",
      "   This means the issue is in retrieval, not response synthesis.\n",
      "\n",
      "üìã Step 2: Calling query_engine.query()...\n",
      "   This will: retrieve nodes ‚Üí apply post-processors ‚Üí synthesize response\n",
      "   Executing: response = query_engine.query(question)\n",
      "   ‚úÖ Query completed successfully\n",
      "\n",
      "üìã Step 3: Analyzing response...\n",
      "   Response type: <class 'llama_index.core.base.response.schema.Response'>\n",
      "   Has response text: True\n",
      "   Response text length: 14\n",
      "   Response text preview: Empty Response...\n",
      "   Has source_nodes: True\n",
      "   Number of source nodes: 0\n",
      "   ‚ö†Ô∏è  WARNING: Response has 0 source nodes!\n",
      "   This could mean:\n",
      "      - All nodes were filtered by SimilarityPostprocessor\n",
      "      - Response synthesis failed\n",
      "      - No nodes were retrieved\n",
      "   Response metadata: None\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query completed\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Empty Response\n",
      "\n",
      "================================================================================\n",
      "SOURCES (0 chunks):\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def rag_query_llamaindex(\n",
    "    question: str,\n",
    "    query_engine=None,\n",
    "    similarity_top_k: int = 5,\n",
    "    similarity_cutoff: float = 0.3,  # Lowered from 0.7 to get more results\n",
    "    response_mode: str = \"compact\",\n",
    "    debug: bool = True  # Enable detailed debugging\n",
    "):\n",
    "    \"\"\"\n",
    "    Query the RAG system using LlamaIndex.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        query_engine: Pre-configured query engine (if None, will create one)\n",
    "        similarity_top_k: Number of chunks to retrieve\n",
    "        similarity_cutoff: Minimum similarity score (lower = more results, default 0.3)\n",
    "        response_mode: How to synthesize response (\"compact\", \"tree_summarize\", \"simple_summarize\")\n",
    "        debug: Enable detailed debugging output\n",
    "        \n",
    "    Returns:\n",
    "        LlamaIndex Response object with answer and source nodes\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"üîç DEBUG: Querying RAG System\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Create query engine if not provided\n",
    "    if query_engine is None:\n",
    "        if debug:\n",
    "            print(\"üìã Creating new query engine...\")\n",
    "        query_engine = create_llamaindex_query_engine(\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            similarity_cutoff=similarity_cutoff,\n",
    "            debug=debug\n",
    "        )\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"üìã Using provided query engine: {type(query_engine)}\")\n",
    "            print(f\"   Retriever: {type(query_engine._retriever)}\")\n",
    "            print(f\"   Post-processors: {len(query_engine._node_postprocessors)}\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nüìã Question: {question}\")\n",
    "        print(f\"\\nüìã Step 1: Generating query embedding...\")\n",
    "    \n",
    "    # Step 1: Test retrieval BEFORE query to see what's happening\n",
    "    if debug:\n",
    "        try:\n",
    "            from llama_index.core import QueryBundle\n",
    "            retriever = query_engine._retriever\n",
    "            query_bundle = QueryBundle(question)\n",
    "            \n",
    "            print(f\"   Retrieving nodes with retriever...\")\n",
    "            nodes_before_query = retriever.retrieve(query_bundle)\n",
    "            print(f\"   ‚úÖ Retrieved {len(nodes_before_query)} nodes from retriever\")\n",
    "            \n",
    "            if len(nodes_before_query) > 0:\n",
    "                print(f\"\\n   Top {min(3, len(nodes_before_query))} nodes BEFORE query engine:\")\n",
    "                for i, node in enumerate(nodes_before_query[:3], 1):\n",
    "                    print(f\"      {i}. Score: {node.score:.4f}\")\n",
    "                    print(f\"         File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "                    print(f\"         Text preview: {node.text[:100]}...\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: Retriever returned 0 nodes!\")\n",
    "                print(f\"   This means the issue is in retrieval, not response synthesis.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Could not test retrieval: {e}\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\nüìã Step 2: Calling query_engine.query()...\")\n",
    "        print(f\"   This will: retrieve nodes ‚Üí apply post-processors ‚Üí synthesize response\")\n",
    "    \n",
    "    # Query the engine - THIS IS THE LINE WITH THE ISSUE\n",
    "    try:\n",
    "        if debug:\n",
    "            print(f\"   Executing: response = query_engine.query(question)\")\n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   ‚úÖ Query completed successfully\")\n",
    "            print(f\"\\nüìã Step 3: Analyzing response...\")\n",
    "            print(f\"   Response type: {type(response)}\")\n",
    "            print(f\"   Has response text: {hasattr(response, 'response')}\")\n",
    "            \n",
    "            if hasattr(response, 'response'):\n",
    "                response_text = getattr(response, 'response', None)\n",
    "                if response_text:\n",
    "                    response_str = str(response_text)\n",
    "                    print(f\"   Response text length: {len(response_str)}\")\n",
    "                    print(f\"   Response text preview: {response_str[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"   Response text: None/Empty\")\n",
    "            \n",
    "            print(f\"   Has source_nodes: {hasattr(response, 'source_nodes')}\")\n",
    "            if hasattr(response, 'source_nodes'):\n",
    "                print(f\"   Number of source nodes: {len(response.source_nodes)}\")\n",
    "                \n",
    "                if len(response.source_nodes) > 0:\n",
    "                    print(f\"\\n   Source nodes details:\")\n",
    "                    for i, node in enumerate(response.source_nodes[:3], 1):\n",
    "                        print(f\"      {i}. Score: {node.score:.4f}\")\n",
    "                        print(f\"         File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "                else:\n",
    "                    print(f\"   ‚ö†Ô∏è  WARNING: Response has 0 source nodes!\")\n",
    "                    print(f\"   This could mean:\")\n",
    "                    print(f\"      - All nodes were filtered by SimilarityPostprocessor\")\n",
    "                    print(f\"      - Response synthesis failed\")\n",
    "                    print(f\"      - No nodes were retrieved\")\n",
    "            \n",
    "            if hasattr(response, 'metadata'):\n",
    "                print(f\"   Response metadata: {response.metadata}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå ERROR during query_engine.query(): {e}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback:\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"‚úÖ Query completed\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "    return response\n",
    "\n",
    "def debug_retrieval(question: str, query_engine, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Debug function to see what chunks are being retrieved before filtering.\n",
    "    \"\"\"\n",
    "    from llama_index.core import QueryBundle\n",
    "    \n",
    "    print(f\"üîç Debug: Retrieving top {top_k} chunks for: {question}\\n\")\n",
    "    \n",
    "    # Get the retriever from the query engine\n",
    "    retriever = query_engine._retriever\n",
    "    \n",
    "    # Retrieve nodes\n",
    "    query_bundle = QueryBundle(question)\n",
    "    nodes = retriever.retrieve(query_bundle)\n",
    "    \n",
    "    print(f\"üìä Retrieved {len(nodes)} chunks (before post-processing):\\n\")\n",
    "    for i, node in enumerate(nodes[:top_k], 1):\n",
    "        print(f\"{i}. Similarity: {node.score:.4f}\")\n",
    "        print(f\"   File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"   Layers: {node.metadata.get('relevant_layers', [])}\")\n",
    "        print(f\"   Text preview: {node.text[:150]}...\\n\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "def print_rag_response(response):\n",
    "    \"\"\"\n",
    "    Pretty print a LlamaIndex response with sources.\n",
    "    \n",
    "    Args:\n",
    "        response: LlamaIndex Response object\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(response.response)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SOURCES ({len(response.source_nodes)} chunks):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, node in enumerate(response.source_nodes, 1):\n",
    "        print(f\"\\n[Source {i}]\")\n",
    "        print(f\"  File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"  Similarity: {node.score:.3f}\")\n",
    "        print(f\"  Confidence: {node.metadata.get('confidence', 'Unknown')}\")\n",
    "        print(f\"  Layers: {node.metadata.get('relevant_layers', [])}\")\n",
    "        \n",
    "        if node.metadata.get('locations'):\n",
    "            print(f\"  Locations: {', '.join(node.metadata['locations'])}\")\n",
    "        if node.metadata.get('slr_projections'):\n",
    "            print(f\"  SLR Projections: {', '.join(node.metadata['slr_projections'])}\")\n",
    "        \n",
    "        print(f\"  Text: {node.text[:200]}...\")\n",
    "\n",
    "# Example usage\n",
    "# Create query engine with lower similarity cutoff to get more results\n",
    "query_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    similarity_cutoff=0.3  # Lower threshold - adjust based on your needs\n",
    ")\n",
    "\n",
    "response = rag_query_llamaindex(\n",
    "    question=\"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    query_engine=query_engine\n",
    ")\n",
    "\n",
    "print_rag_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668e332",
   "metadata": {},
   "source": [
    "# Diagnostics: Debugging Empty Response\n",
    "\n",
    "The following cells diagnose why vector search is returning 0 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fac062ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Total chunks in database: 568\n",
      "üìä Chunks with embeddings: 568\n",
      "\n",
      "üìã Sample chunks from database:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Filename: Vitousek_SCD08.md\n",
      "Confidence: HIGH\n",
      "Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "Locations: ['Windward Oahu', 'Waimanalo', 'Oahu']\n",
      "Text preview: ## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "This paper outlines a practical approach to mapping extreme wave inundation and the inf...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Filename: Vitousek_SCD08.md\n",
      "Confidence: HIGH\n",
      "Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "Locations: ['Windward Oahu', 'Waimanalo', 'Oahu']\n",
      "Text preview: Our approach follows Ruggiero et. ...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Filename: Vitousek_SCD08.md\n",
      "Confidence: HIGH\n",
      "Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "Locations: ['Windward Oahu', 'Waimanalo', 'Oahu']\n",
      "Text preview: al. who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of the Oregon coast.\n",
      "\n",
      "In this study, NOAA's wave-moni...\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC: Check what's actually in the database\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(\"postgresql://***:***@localhost:5432/climate_viewer_dev\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Count total chunks\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM document_chunks\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"üìä Total chunks in database: {count}\")\n",
    "    \n",
    "    # Check if embeddings exist\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM document_chunks WHERE embedding IS NOT NULL\"))\n",
    "    embedding_count = result.scalar()\n",
    "    print(f\"üìä Chunks with embeddings: {embedding_count}\")\n",
    "    \n",
    "    # Sample some chunks to verify content\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT text, filename, confidence, relevant_layers, locations \n",
    "        FROM document_chunks \n",
    "        LIMIT 3\n",
    "    \"\"\"))\n",
    "    print(\"\\nüìã Sample chunks from database:\")\n",
    "    for i, row in enumerate(result, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(f\"Filename: {row[1]}\")\n",
    "        print(f\"Confidence: {row[2]}\")\n",
    "        print(f\"Layers: {row[3]}\")\n",
    "        print(f\"Locations: {row[4]}\")\n",
    "        print(f\"Text preview: {row[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d57e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Querying with similarity_cutoff=0.0 (no filtering)...\n",
      "\n",
      "================================================================================\n",
      "üîß DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "üìã Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "üìã Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "üìã Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "üìã Step 4: Creating PGVectorStore...\n",
      "   ‚úÖ PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "üìã Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   ‚úÖ Added 568 nodes to docstore\n",
      "   ‚úÖ Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "üìã Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 10\n",
      "   ‚úÖ Retriever created successfully\n",
      "\n",
      "üìã Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.0\n",
      "   ‚úÖ Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query engine ready!\n",
      "   Retrieval: top 10 chunks\n",
      "   Min similarity: 0.0\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîç DEBUG: Querying RAG System\n",
      "================================================================================\n",
      "üìã Using provided query engine: <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "   Retriever: <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "   Post-processors: 1\n",
      "\n",
      "üìã Question: What are the impacts of sea level rise on groundwater in Honolulu?\n",
      "\n",
      "üìã Step 1: Generating query embedding...\n",
      "   Retrieving nodes with retriever...\n",
      "   ‚úÖ Retrieved 0 nodes from retriever\n",
      "   ‚ö†Ô∏è  WARNING: Retriever returned 0 nodes!\n",
      "   This means the issue is in retrieval, not response synthesis.\n",
      "\n",
      "üìã Step 2: Calling query_engine.query()...\n",
      "   This will: retrieve nodes ‚Üí apply post-processors ‚Üí synthesize response\n",
      "   Executing: response = query_engine.query(question)\n",
      "   ‚úÖ Query completed successfully\n",
      "\n",
      "üìã Step 3: Analyzing response...\n",
      "   Response type: <class 'llama_index.core.base.response.schema.Response'>\n",
      "   Has response text: True\n",
      "   Response text length: 14\n",
      "   Response text preview: Empty Response...\n",
      "   Has source_nodes: True\n",
      "   Number of source nodes: 0\n",
      "   ‚ö†Ô∏è  WARNING: Response has 0 source nodes!\n",
      "   This could mean:\n",
      "      - All nodes were filtered by SimilarityPostprocessor\n",
      "      - Response synthesis failed\n",
      "      - No nodes were retrieved\n",
      "   Response metadata: None\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query completed\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Empty Response\n",
      "\n",
      "================================================================================\n",
      "SOURCES (0 chunks):\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC: Try query with NO similarity threshold\n",
    "print(\"üîç Querying with similarity_cutoff=0.0 (no filtering)...\\n\")\n",
    "\n",
    "test_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    similarity_cutoff=0.0  # Accept ALL results\n",
    ")\n",
    "\n",
    "result = rag_query_llamaindex(\n",
    "    \"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    query_engine=test_engine\n",
    ")\n",
    "\n",
    "print_rag_response(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cdb54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Current embedding model: model_name='text-embedding-3-small' embed_batch_size=100 callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10a2c41d0> num_workers=None embeddings_cache=None additional_kwargs={} api_key='sk-***REDACTED***' api_base='https://api.openai.com/v1' api_version='' max_retries=10 timeout=60.0 default_headers=None reuse_client=True dimensions=None\n",
      "   Model name: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC: Check which embedding model is being used\n",
    "from llama_index.core import Settings\n",
    "\n",
    "print(f\"ü§ñ Current embedding model: {Settings.embed_model}\")\n",
    "print(f\"   Model name: {Settings.embed_model.model_name if hasattr(Settings.embed_model, 'model_name') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e22889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Testing raw retrieval (no filtering)...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorIndexRetriever\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîç Testing raw retrieval (no filtering)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m retriever = VectorIndexRetriever(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     index=\u001b[43mindex\u001b[49m,\n\u001b[32m      8\u001b[39m     similarity_top_k=\u001b[32m10\u001b[39m,  \u001b[38;5;66;03m# Get more results\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Test the same query\u001b[39;00m\n\u001b[32m     12\u001b[39m test_question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the impacts of sea level rise on groundwater in Honolulu?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC: Test raw retrieval WITHOUT similarity filtering\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "print(\"üîç Testing raw retrieval (no filtering)...\\n\")\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,  # Get more results\n",
    ")\n",
    "\n",
    "# Test the same query\n",
    "test_question = \"What are the impacts of sea level rise on groundwater in Honolulu?\"\n",
    "nodes = retriever.retrieve(test_question)\n",
    "\n",
    "print(f\"üìä Retrieved {len(nodes)} nodes BEFORE similarity filtering:\\n\")\n",
    "for i, node in enumerate(nodes[:5], 1):  # Show top 5\n",
    "    print(f\"{i}. Similarity Score: {node.score:.4f}\")\n",
    "    print(f\"   Text preview: {node.text[:200]}...\")\n",
    "    print()\n",
    "\n",
    "if len(nodes) == 0:\n",
    "    print(\"‚ö†Ô∏è  WARNING: No nodes retrieved even without filtering!\")\n",
    "    print(\"   This suggests an issue with the index or embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c6a099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Querying with similarity_cutoff=0.0 (no filtering)...\n",
      "\n",
      "================================================================================\n",
      "üîß DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "üìã Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "üìã Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "üìã Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "üìã Step 4: Creating PGVectorStore...\n",
      "   ‚úÖ PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "üìã Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   ‚úÖ Added 568 nodes to docstore\n",
      "   ‚úÖ Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "üìã Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 10\n",
      "   ‚úÖ Retriever created successfully\n",
      "\n",
      "üìã Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.0\n",
      "   ‚úÖ Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query engine ready!\n",
      "   Retrieval: top 10 chunks\n",
      "   Min similarity: 0.0\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "üîç DEBUG: Querying RAG System\n",
      "================================================================================\n",
      "üìã Using provided query engine: <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "   Retriever: <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "   Post-processors: 1\n",
      "\n",
      "üìã Question: What are the impacts of sea level rise on groundwater in Honolulu?\n",
      "\n",
      "üìã Step 1: Generating query embedding...\n",
      "   Retrieving nodes with retriever...\n",
      "   ‚úÖ Retrieved 0 nodes from retriever\n",
      "   ‚ö†Ô∏è  WARNING: Retriever returned 0 nodes!\n",
      "   This means the issue is in retrieval, not response synthesis.\n",
      "\n",
      "üìã Step 2: Calling query_engine.query()...\n",
      "   This will: retrieve nodes ‚Üí apply post-processors ‚Üí synthesize response\n",
      "   Executing: response = query_engine.query(question)\n",
      "   ‚úÖ Query completed successfully\n",
      "\n",
      "üìã Step 3: Analyzing response...\n",
      "   Response type: <class 'llama_index.core.base.response.schema.Response'>\n",
      "   Has response text: True\n",
      "   Response text length: 14\n",
      "   Response text preview: Empty Response...\n",
      "   Has source_nodes: True\n",
      "   Number of source nodes: 0\n",
      "   ‚ö†Ô∏è  WARNING: Response has 0 source nodes!\n",
      "   This could mean:\n",
      "      - All nodes were filtered by SimilarityPostprocessor\n",
      "      - Response synthesis failed\n",
      "      - No nodes were retrieved\n",
      "   Response metadata: None\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Query completed\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# üîç DIAGNOSTIC: Try query with NO similarity threshold\n",
    "print(\"üîç Querying with similarity_cutoff=0.0 (no filtering)...\\n\")\n",
    "\n",
    "test_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    similarity_cutoff=0.0  # Accept ALL results\n",
    ")\n",
    "\n",
    "result = rag_query_llamaindex(\n",
    "    \"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    query_engine=test_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2lbaiajp10y",
   "metadata": {},
   "source": [
    "## Advanced RAG Features with LlamaIndex\n",
    "\n",
    "LlamaIndex provides powerful features out of the box:\n",
    "\n",
    "### 1. **Query Engines**\n",
    "- **Compact Mode**: Concatenates chunks and sends to LLM (default)\n",
    "- **Tree Summarize**: Hierarchical summarization for long contexts\n",
    "- **Simple Summarize**: Simple concatenation with summarization\n",
    "\n",
    "### 2. **Retrieval Modes**\n",
    "- **Vector Search**: Semantic similarity (what we're using)\n",
    "- **Hybrid Search**: Combines vector + keyword search\n",
    "- **Auto-Retrieval**: LLM-powered query planning\n",
    "\n",
    "### 3. **Post-Processors**\n",
    "- **SimilarityPostprocessor**: Filter by similarity threshold\n",
    "- **KeywordNodePostprocessor**: Filter by keywords\n",
    "- **MetadataReplacementPostProcessor**: Replace node text with metadata\n",
    "- **SentenceEmbeddingOptimizer**: Optimize context window usage\n",
    "\n",
    "### 4. **Metadata Filtering**\n",
    "You can filter by your custom metadata:\n",
    "```python\n",
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "filters = MetadataFilters(filters=[\n",
    "    ExactMatchFilter(key=\"confidence\", value=\"HIGH\"),\n",
    "    ExactMatchFilter(key=\"relevant_layers\", value=\"groundwater_inundation\")\n",
    "])\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    "    filters=filters\n",
    ")\n",
    "```\n",
    "\n",
    "### 5. **Chat Engine** (Multi-turn conversations)\n",
    "```python\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Tell me about groundwater impacts\")\n",
    "response = chat_engine.chat(\"What about Honolulu specifically?\")  # Remembers context\n",
    "```\n",
    "\n",
    "### 6. **Streaming Responses**\n",
    "```python\n",
    "streaming_response = query_engine.query(\"Your question...\")\n",
    "for text in streaming_response.response_gen:\n",
    "    print(text, end=\"\")\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy as API**: Wrap this in FastAPI for web access\n",
    "2. **Add caching**: Use LlamaIndex's caching for faster responses\n",
    "3. **Implement chat history**: Store conversation context\n",
    "4. **Add reranking**: Use cross-encoder models for better retrieval\n",
    "5. **Custom prompts**: Tailor system prompts for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extm57lrba8",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Combine vector search with LLM to answer questions using your document database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11098100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in database: 568\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Text preview: ## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "This paper outlines a practical approach to mapping extreme wave inundation and the inf...\n",
      "Metadata: (\"## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\\n\\nThis paper outlines a practical ap ... (3353 characters truncated) ... ontal error (Circular Error Envelope - CE95) and a vertical accuracy of 1 cm.\\n\\nThe evaluation of runup elevations requires a statistical approach. \",)\n",
      "\n",
      "Text preview: Our approach follows Ruggiero et. ...\n",
      "Metadata: ('Our approach follows Ruggiero et. ',)\n",
      "\n",
      "Text preview: al. who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of the Oregon coast.\n",
      "\n",
      "In this study, NOAA's wave-moni...\n",
      "Metadata: (\"al. who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of ... (1513 characters truncated) ...  following the method of Fletcher et. al. Annual Erosion Hazard Rates (AEHR) are determined using basis function methods following Frazer et. al.\\n\\n\",)\n",
      "\n",
      "Text preview: In approach #3, a third generation spectral wave model, SWAN (Simulating WAves Nearshore - Booij et. al....\n",
      "Metadata: ('In approach #3, a third generation spectral wave model, SWAN (Simulating WAves Nearshore - Booij et. al.',)\n",
      "\n",
      "Text preview: ; Ris et. ...\n",
      "Metadata: ('; Ris et. ',)\n"
     ]
    }
   ],
   "source": [
    "# Check database content\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(\"postgresql://***:***@localhost:5432/climate_viewer_dev\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Count total chunks\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM document_chunks\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"Total chunks in database: {count}\")\n",
    "    \n",
    "    # Sample some text\n",
    "    result = conn.execute(text(\"SELECT text FROM document_chunks LIMIT 5\"))\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for row in result:\n",
    "        print(f\"\\nText preview: {row[0][:200]}...\")\n",
    "        print(f\"Metadata: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5750a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "Sean Vitousek 1 , Charles H. Fletcher 1 , Matthew M. Barbee 1\n",
      "\n",
      "1. Department of Geology and Geophysics, 1680 East-West Rd. POST Room 721, Honolulu, Hawaii, 96822, USA. seanfkv@hawaii.edu, fletcher@soest.hawaii.edu, and mbarbee@hawaii.edu.\n",
      "\n",
      "Abstract :  This paper outlines a practical approach to mapping extreme wave inundation and the influence of sea-level rise and coastal erosion.  The concept is presented for windward Oahu, Hawai'i.  Statistical models of extreme wave height and recently developed empirical runup equations (Stockdon et al. 2006) provide extreme runup levels, which overlay georeferenced aerial photos and high-resolution LIDAR elevation models.  The alongshore wave height variability  that  contributes  to  alongshore  runup variability is accounted for by the SWAN spectral wave model.  Sea level is found to play a significant role in future inundation levels.\n",
      "\n",
      "## INTRODUCTION\n",
      "\n",
      "Sea-level rise of 0.5-1.4 m (Rahmstorf 2007) is expected by the end of the 21 st century. Previous work on the long-term impacts of sea-level rise has focused on flooding of lowlying coasts.  Raising the static water level on a digital elevation model (i.e., passive flooding)  displays  the  dramatic  long-term  effects  of  sea-level  rise.    The  short-term effects of sea-level rise include increased frequency and severity of wave overtopping and wave attack, resulting in coastal inundation, erosion and morphological changes. The nature of these short-term impacts on coastal communities is still undetermined and may pose significant risk.\n",
      "\n",
      "Managing the risk of coastal flooding is a balancing act between the base elevations of nearshore infrastructure and base flood elevations. Flood Insurance Rate Maps (FIRMs) are created to manage development siting.\n",
      "\n",
      "To evaluate the risk of coastal flooding to nearshore communities, we use a practical approach to map extreme wave inundation and the influence of sea-level rise.\n",
      "\n",
      "These methods are applied to a case study of Waimanalo, Hawai'i, a low-lying coastal community protected by a fringing reef on the East Coast of Oahu, Figure 1.\n",
      "\n",
      "Figure 1 - Location Map of case study: Waimanalo, Oahu, Hawai'i\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## METHODS\n",
      "\n",
      "We employ 3 approaches to map extreme runup and the influence of sea-level rise.\n",
      "\n",
      "- 1)  The 1 st ( standard ) approach:\n",
      "- a) Determine recurring wave height statistics.\n",
      "- b) Translate the wave height to wave runup using empirical equations.\n",
      "- c) Map the runup elevation spatially on a digital elevation model.\n",
      "- d) Apply sea-level rise to assess risk.\n",
      "- 2)  The 2 nd approach ( includes historical erosion analysis ):\n",
      "- a) Determine recurring wave height statistics.\n",
      "- b) Translate the wave height to wave runup using empirical equations.\n",
      "- c) Map the runup elevation spatially on a digital elevation model.\n",
      "- d) Apply sea-level rise and an erosion rate to assess risk.\n",
      "- 3)  The 3 rd approach ( includes nearshore wave transformation ):\n",
      "- a) Determine recurring wave height statistics.\n",
      "- b)  Model the nearshore wave transformation using the SWAN model.\n",
      "- c) Translate the wave height to wave runup using empirical equations.\n",
      "- d) Map the runup elevation spatially on a digital elevation model.\n",
      "- e) Apply sea-level rise to assess risk.\n",
      "\n",
      "As  mentioned  previously,  managing  the  risk  of  coastal  flooding  is  a  balancing  act between infrastructure elevations and total runup elevations.  The key to evaluating the potential risk is estimating both of these elevations as accurately as possible.\n",
      "\n",
      "To determine infrastructure and ground elevations, we use high-resolution aerial LIDAR surveys conducted by the NOAA Coastal Services Center Coastal Remote Sensing\n",
      "\n",
      "Program in winter 2005, with 95% of the data having less than 31.8 cm horizontal error (Circular Error Envelope - CE95) and a vertical accuracy of 1 cm.\n",
      "\n",
      "The evaluation of runup elevations requires a statistical approach.  Our approach follows Ruggiero et. al. (2001) who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of the Oregon coast.\n",
      "\n",
      "In this study, NOAA's wave-monitoring buoy (NDBC buoy 51001) provides data for statistical analyses of maximum return wave heights for Waimanalo. The Generalized Extreme Value (GEV) distribution gives the relationship between wave height and return period.    The  maximum  return  runup  elevations  are  then  derived  from  empirical relationships to the maximum return wave heights.  In this approach, we apply the maximum return runup elevations at the Mean Higher High Water (MHHW) mark. Consideration of the joint probability of high waves and high tides may improve the total water level prediction; however, it is left to future work.\n",
      "\n",
      "Our approach to determining the total runup juncture on a beach profile is analogous to FEMA's  approach  to  determining  FIRMs  using  their  computer  code  RUNUP  2.0. Several shore-normal transects in the form of rectangular grid lines (5 m resolution) are cast on the shoreline of interest, and the analysis is performed on an individual transect basis.  Beach profiles at each shore-normal grid line are extracted from high-resolution LIDAR elevation models (Figure 2).  The LIDAR elevation model may be quite irregular or  noisy,  requiring  smoothing  of  the  elevation  model  or  the  alongshore  inundation contours.\n",
      "\n",
      "In approach #2, a historical shoreline analysis provides an erosion rate that is applied to the  zero  elevation  contour.    The  historical  shoreline  analysis  was  performed  using digitized shorelines extracted from historical aerial photographs and USGS T-sheets, following the method of Fletcher et. al. (2003).  Annual Erosion Hazard Rates (AEHR) are determined using basis function methods following Frazer et. al. (accepted).\n",
      "\n",
      "Figure 2 - The Waimanalo runup grid (A) and elevation (B) from a broad view. Beach profiles (E) are extracted from transects along shore-normal grid lines (C) where the elevation is given by a LIDAR survey (D).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "In approach #3, a third generation spectral wave model, SWAN (Simulating WAves Nearshore - Booij et. al. 1999; Ris et. al. 1999), is run on nested grids (a 25 m grid extending to deep water and the 5 m nearshore grid - shown in Figure 3). The model is forced with parametric wave boundary conditions determined by the GEV distribution. Using SWAN, the spatial variability of the wave heights due to nearshore processes (including shoaling, refraction, convergence, divergence and breaking) can be resolved in the runup model.\n",
      "\n",
      "Figure 3 - A: the nearshore (5 m) and regional (25 m) SWAN grids for Waimanalo and B: the bathymetry\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Once the return runup elevation and its spatial location is found for each grid line, the elevation model may be subject to static sea-level rise.  The potential for increased flooding can be evaluated by observing the inland extension of the flooding contours.\n",
      "\n",
      "## RESULTS\n",
      "\n",
      "## Extreme wave statistics\n",
      "\n",
      "Introduced by Jenkinson (1955), the Generalized Extreme Value (GEV) distribution uses Gumbel (type I), Frechet (type II), and Weibull (type III) distributions for different values of the shape parameter, 0 Œ∫ = , 0 Œ∫ &lt; , 0 Œ∫ &gt; respectively.   Iterative maximumlikelihood estimates (MLE) fit the observed data to find the best estimates of the shape ( ),  scale  ( Œ∫ œÉ )  and  location  ( ¬µ )  parameters  of  the  GEV  cumulative  distribution function, , given by: ( F x )\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "Based on given probability distributions  and  the  return  period  probability  equation . . 1 R T R r i p T = -, wave height for a return period of interest is found.  Northeast swell poses the greatest hazard to Waimanalo.  The largest wave heights in Hawai'i are experienced during northwest and north swell.  Thus, in the recurring wave height analysis, buoy data is  windowed to northeast swell directions (35-75 o ).  The  relationship  between  wave height and return period for Waimanalo is shown in Figure 4.\n",
      "\n",
      "## Extreme Runup\n",
      "\n",
      "Empirical  equations  give  runup  elevation  in  both  RUNUP  2.0  and  our  approach. RUNUP  2.0  uses  different  equations  for  different  shorelines  (sandy,  riprap  and\n",
      "\n",
      "impermeable), based on the surf similarity parameter: , where is the beach slope, H is the deep-water significant wave height, and L is the deep-water wavelength.  Our approach uses a recently developed equation (Stockdon et. al. 2006) for the 2% exceedance runup: 1/2 tan /( / ) o o H L Œæ Œ∏ = o tan Œ∏ o\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "which similarly gives runup as a function of beach slope (foreshore slope f Œ≤ ), deepwater wave height ( ) o H , and deep-water wavelength ( ) o L .  We use the Stockdon formula because it is complete: it formulates runup as the sum of setup, &lt;Œ∑ &gt; , and swash, , due to both incident and infragravity energy.  The relationship was derived from 10 datasets primarily from the continental US.  To facilitate the comparison of each dataset, Stockdon computed reverse shoaling of wave heights measured in intermediate depths (~8 m) to back calculate the deep-water wave height.  Reverse shoaling (using linear wave  theory  and  assuming  shore  normal  approach)  is  computed  by  dividing  the S\n",
      "\n",
      "<!-- formula-not-decoded -->\n",
      "\n",
      "The previous relationship of wave height vs. return period is translated into runup vs. return period (shown in Figure 4), assuming a representative wave period of 16 sec. and a beach slope 1/24 (determined from a survey of Waimanalo).\n",
      "\n",
      "Figure 4 - Return significant wave height [m] and return period [years] for Waimanalo for the northeast swell  window  75-35 o (Top)  and  corresponding  return  runup  elevation  [m]  and  return  period  [years] (Bottom) given by the Stockdon (2006) empirical runup relation using a nominal wave period of 16 sec. and a beach slope 1/24.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Mapping Extreme Runup (Approach #1)\n",
      "\n",
      "From the known total runup level elevations, we can estimate the inundation contours for each return period and their inland migration due to sea-level rise (shown in Figure 5).  It is important to keep in mind that approach #1 has no runup elevation variability.  The inundation variability is only a function of the beach topography.  The standard approach can be extended to account for runup variability as a function of a changing beach slope in the alongshore direction. However, when simply using the deep-water wave height in an empirical equation, the runup elevation is uniform in the alongshore direction.  Using a wave transformation model will resolve this variability.\n",
      "\n",
      "Figure 5 Illustrating approach #1 - inundation contours for different scenarios of sea-level rise (0, +0.25, +0.5 ,+1.0 m).\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Mapping Extreme Runup and shoreline change (Approach #2)\n",
      "\n",
      "Depending  on  the  dynamics  of  the  shoreline,  the  inland  migration  of  the  return inundation levels rise may be considerably slower or be overtaken by recession of the shoreline due to erosional processes.  In such a case, the erosion hazard will be greater than the inundation hazard to structures close to the coast.  Waimanalo is not likely to have a significant erosion hazard, as it experiences very mild shoreline change rates (~1m/year accretion - 1m/year erosion).  The case illustrated in Figure 6, shows the same inundation lines in approach #1 with a constant erosion rate of 0.6 m/yr to all transects.\n",
      "\n",
      "Figure 6 - Illustrating approach #2 - inundation contours for different scenarios of sea-level rise [0 (at present time), +0.25 (2025), +0.5 (2050), +1.0 m (2100)], and an eroded beach contour from a constant erosion rate of 0.6 m/yr.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "## Mapping Extreme Runup and nearshore wave processes (Approach #3)\n",
      "\n",
      "Waimanalo is fronted by a fringing reef that dissipates a significant amount of energy during large swell events.  SWAN accounts for the dissipation over the fringing reef due to friction and breaking.  Default parameters of friction (JONSWAP coefficient 0.067 m 2 /s -3 ) and breaking parameters ( Œ± =1.0 &amp; =0.85) are used. Œ≥\n",
      "\n",
      "The use of SWAN and reverse shoaling from intermediate depths on the reef (10, 15, 20 m) significantly reduces the projected wave heights and runup levels relative to the deepwater wave heights and runup levels.  These results are illustrated in Figure 7 and quantified in Table 1.\n",
      "\n",
      "Figure 7 - Illustrating approach #3 - Projected inundation lines under a scenario of +0.25 m of sea-level rise with the runup computed from wave heights at different depths (5 m, 10 m, 15 m, and from deep water). The reverse shoaling and dissipation on the reef face is responsible for the significant reduction in runup and inundation at the coast.\n",
      "\n",
      "<!-- image -->\n",
      "\n",
      "Table 1 - Wave height and runup reduction over the fringing reef computed by SWAN and reverse shoaling.\n",
      "\n",
      "## DISCUSSION\n",
      "\n",
      "The major problem with our approach and RUNUP 2.0 (used in the FIRMS) is the improper use of empirical equations.  The use of empirical equations facilitates the speed of computation and the ability to determine runup elevations on regional scales. However, there is a tradeoff between speed and scale of computation.  Full processbased numerical modeling requires extensive computational time and thus cannot be readily applied to regional scales.  Empirical models can be applied on regional scales at the expense of inaccuracies on local scales.\n",
      "\n",
      "A major limitation of the application of empirical equations is the performance on a fringing reef.  Empirical equations may perform quite well in predicting runup elevations in dynamically similar environments. On the other hand, they may perform quite poorly in dissimilar environments.  One example of the potential problems that may result from use of empirical equations is the use of deep-water wave height to calculate runup. Using SWAN to predict the reduction in wave heights near or inside a fringing reef will account for the reduction in runup at the shoreline.  Additionally, using SWAN rather than the individual transect method to compute the nearshore wave field, the spatial variability in wave height due to convergence and divergence is resolved.  SWAN is able to compute wave setup inside a fringing reef.  However, SWAN is not able to compute runup.  Using a runup-resolving nearshore wave model would directly compute runup at the expense of computational speed and regional applicability.  Nonetheless, the full process-based numerical modeling may be a considerably better approach.\n",
      "\n",
      "The majority of the incident swell energy is reduced over dissipative beaches (mildly sloping beaches with wide surf zones) and fringing reefs, while infragravity motions dominate  the  sea  level  and  runup  signals  at  the  shoreline  (Guza  &amp;  Thorton  1982; Ruessink 1998; Miles et. al. 2006; Kench &amp; Brander 2006).  Dissipation computed in SWAN and reverse shoaling significantly reduce the wave heights inside a fringing reef and corresponding runup elevations (as shown in Table 1).  Outside the breaking point, reverse shoaling has a larger effect on reducing the wave height. Inside the breaking point, wave breaking is responsible for the largest reduction in wave height (50-83%), although reverse shoaling is responsible for an additional reduction (25-50%).\n",
      "\n",
      "Our goal of mapping extreme runup levels on nearshore topography has encountered many limitations in the field of runup prediction.  However, as empirical equations are developed for several different geologic settings and nearshore process-based numerical models improve, the ability to map the extreme runup levels and determine the influence of sea-level rise will also improve.\n",
      "\n",
      "## CONCLUSIONS\n",
      "\n",
      "Practical approaches to mapping coastal inundation and the influence of sea-level rise can be quite informative, provided confident runup elevation estimates.  The use of empirical equations in mapping runup elevations should be approached with caution, especially in fringing reef environments.  The use of deep-water wave heights may significantly overestimate the runup at the shoreline.  The use of nearshore wave models may improve the predictions of runup in fringing reef environments.\n",
      "\n",
      "Following this approach we see that fringing reefs act as excellent natural breakwaters and may buffer adjacent coastlines against the short-term impacts of sealevel rise, particularly from the increase in frequency of wave-related inundation.\n",
      "\n",
      "## ACKNOWLEDGEMENTS\n",
      "\n",
      "This paper is funded by a grant/cooperative agreement from the National Oceanic and Atmospheric Administration, Project # R/EP-26, which is sponsored by the University of Hawaii Sea Grant College Program, SOEST, under Institutional Grant No. NA05OAR4171048 from NOAA Office of Sea Grant, Department of Commerce.  The views expressed herein are those of the author(s) and do not necessarily reflect the views of NOAA or any of its subagencies.\n",
      "\n",
      "## REFERENCES\n",
      "\n",
      "- Frazer,  L.  N.,  Genz,  A.  S.  and  Fletcher,  C.  H.,  (accepted).  'Toward  Parsimony  in Shoreline Change Prediction (I): New Methods,' Journal of Coastal Research.\n",
      "- Fletcher  C.H.,  Rooney,  J.,  Barbee,  M.M.,  Lim,  S.C.,  and  Richmond,  B.  (2003). 'Mapping Shoreline Change Using Digital Orthophotogrammetry on Maui, Hawaii' Journal of Coastal Research , 30, 106-124.\n",
      "- Booij, N., Ris, R.C., Holthuijsen, L.H. (1999). 'A third-generation wave model for coastal regions, Part I, Model description and validation,' Journal of Geophysical Research , 104, 7649-7666.\n",
      "- Ris, R.C., Booij, N., Holthuijsen, L.H. (1999). 'A third-generation wave model for coastal regions, Part II, Verification,' Journal of Geophysical Research , 104, 76677681.\n",
      "- Jenkinson,  A.  F.  (1955).  'The  frequency  distribution  of  the  annual  maximum  (or minimum) values of meteorological  elements', Royal  Meteorological  Society  Quarterly Journal , 81,158-171.\n",
      "- Kench P.S. and Brander, R.W. (2006). 'Wave processes on coral reef flats: Implications for reef geomorphology using Australian case studies,' Journal of Coastal Research , 22(1), 209-223.\n",
      "- Guza, R.T., and Thorton, E.B. (1982). 'Swash oscillations on a natural beach,' Journal of Geophysical Research , 87, 483-491.\n",
      "- Ruessink, B.G., Kleinhans, M.G., van den Beukel, P.G.L. (1998). 'Observations of swash under highly dissipative conditions,' Journal of Geophysical Research . 103, 3111-3118.\n",
      "- Miles, J., Butt, T., Russel, P. (2006). 'Swash zone sediment: dynamics: A compassion of a dissipative and an intermediate beach,' Marine Geology , 231 181-200.\n",
      "- Stockdon  H.F.,  Holman  R.A.,  Howd  P.A.,  Sallenger,  A.H.  (2006).  'Empirical parameterization of setup, swash, and runup,' Coastal Engineering , 53, 573-588.\n",
      "- Rahmstorf, S. (2007). 'A Semi-Empirical Approach to Projecting Future Sea-Level Rise' Science, 315, 368-370.\n",
      "- Ruggiero P., Komar, P.D., McDougal, W.G., Marra, J.J., and Beach, R.A. (2001). Wave runup, extreme water levels and the erosion of properties backing beaches, Journal of Coastal Research , 17(2), 407-419.\n"
     ]
    }
   ],
   "source": [
    "# Get all unique filenames in the database\n",
    "from sqlalchemy import create_engine, text\n",
    "import openai\n",
    "\n",
    "def extract_authors(file_text):\n",
    "    \"\"\"\n",
    "    Use LLM to extract authors from the file text\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Extract authors from the following research paper text:\n",
    "    {file_text}\n",
    "    \"\"\"\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "engine = create_engine(\"postgresql://dev_user:dev_password@localhost:5432/climate_viewer_dev\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(\"SELECT DISTINCT filename FROM document_chunks\"))\n",
    "    filenames = [row[0] for row in result]\n",
    "\n",
    "for filename in filenames:\n",
    "    with open(f\"outputs/full_text_v2/{filename}\") as f:\n",
    "        extract_authors(f.read())\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ba2191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
