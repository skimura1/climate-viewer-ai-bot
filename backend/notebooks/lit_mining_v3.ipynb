{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e02dc5fb",
   "metadata": {},
   "source": [
    "# DOCLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a2835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.base_models import ConversionStatus\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = False\n",
    "pipeline_options.do_table_structure = False\n",
    "pipeline_options.images_scale = 1.0\n",
    "\n",
    "pdf_format_options = PdfFormatOption(pipeline_options=pipeline_options)\n",
    "converter = DocumentConverter(\n",
    "    format_options={InputFormat.PDF: pdf_format_options}\n",
    ")\n",
    "\n",
    "directory = \"pdf_pub\"\n",
    "\n",
    "# Get all PDF files in the directory\n",
    "pdf_files = list(Path(directory).glob(\"*.pdf\"))\n",
    "\n",
    "conv_results = converter.convert_all(pdf_files, raises_on_error=False)\n",
    "\n",
    "# Process results\n",
    "for result in conv_results:\n",
    "    if result.status == ConversionStatus.SUCCESS:\n",
    "        # Save output\n",
    "        output_path = Path(\"./outputs/full_text_v2/\") / f\"{result.input.file.stem}.md\"\n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(result.document.export_to_markdown())\n",
    "        print(f\"\u2705 Converted: {result.input.file.name}\")\n",
    "    else:\n",
    "        print(f\"\u274c Failed: {result.input.file.name} - {result.errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9800a408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google import genai\n",
    "from google.genai.errors import APIError\n",
    "import dotenv\n",
    "import json\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize the Gemini Client\n",
    "try:\n",
    "    client = genai.Client()\n",
    "except KeyError:\n",
    "    print(\"Error: Please set the GEMINI_API_KEY environment variable.\")\n",
    "    exit()\n",
    "\n",
    "def sanitize_markdown(messy_markdown_content: str) -> dict:\n",
    "    \"\"\"\n",
    "    Uses Gemini 2.5 Flash to clean and standardize messy markdown based on strict rules.\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\udd04 Starting sanitization... (Input length: {len(messy_markdown_content)} characters)\")\n",
    "    \n",
    "    # 1. Define the Cleaning Prompt (System and User Instructions)\n",
    "    system_prompt = (\n",
    "        \"You are a highly specialized text sanitization expert. Your sole task is to clean and \"\n",
    "        \"reformat the user-provided Markdown document according to a set of strict rules. \"\n",
    "        \"Return ONLY the cleaned Markdown text. Do not add any conversational commentary, explanations, \"\n",
    "        \"or prefixes.\"\n",
    "    )\n",
    "\n",
    "    cleaning_instructions = f\"\"\"\n",
    "    REVIEW AND CLEAN THE FOLLOWING MARKDOWN DOCUMENT.\n",
    "\n",
    "    CRITICAL RULES FOR CLEANING:\n",
    "    1. Citation Removal: Remove ALL citation markers like [1], [2], (Smith et al., 2020), etc.\n",
    "    2. Heading Repair: Fix any merged headers (e.g., \"## SectionHeadingThe text...\" \u2192 \"## Section Heading\\n\\nThe text...\").\n",
    "    3. Exclusion: Ensure the following sections are completely REMOVED if present:\n",
    "        - Table data and captions.\n",
    "        - Figure captions and image descriptions.\n",
    "        - Mathematical equations and LaTeX notation.\n",
    "        - References/Bibliography section.\n",
    "        - Acknowledgments section.\n",
    "        - Appendices.\n",
    "        - Page numbers, headers, and footers.\n",
    "    4. Fidelity: Preserve Hawaiian terms EXACTLY as written (\u02bbokina, kahak\u014d, diacriticals).\n",
    "    5. Formatting: Maintain correct heading hierarchy (## for main sections, ### for subsections) and ensure a double line break (empty line) between paragraphs.\n",
    "\n",
    "    --- DOCUMENT TO CLEAN ---\n",
    "    {messy_markdown_content}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        print(\"\ud83d\udce1 Sending request to Gemini API...\")\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=[\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": cleaning_instructions}]},\n",
    "            ],\n",
    "            config=genai.types.GenerateContentConfig(\n",
    "                system_instruction=system_prompt,\n",
    "            )\n",
    "        )\n",
    "        sanitized_text = response.text\n",
    "        if sanitized_text is None:\n",
    "            print(\"\u26a0\ufe0f Warning: API returned None, returning empty string\")\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": \"API returned None\",\n",
    "                \"sanitized_text\": \"\",\n",
    "                \"input_length\": len(messy_markdown_content),\n",
    "                \"output_length\": 0\n",
    "            }\n",
    "        print(f\"\u2705 Sanitization complete! (Output length: {len(sanitized_text)} characters)\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"sanitized_text\": sanitized_text,\n",
    "            \"input_length\": len(messy_markdown_content),\n",
    "            \"output_length\": len(sanitized_text)\n",
    "        }\n",
    "    except APIError as e:\n",
    "        print(f\"\u274c API Error during sanitization: {e}\")\n",
    "        return {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"sanitized_text\": f\"API Error during Sanitization: {e}\",\n",
    "            \"input_length\": len(messy_markdown_content),\n",
    "            \"output_length\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894f1a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Total files: 112\n",
      "\ud83d\udd27 Max workers: 10, Rate limit: 1.0s\n",
      "------------------------------------------------------------\n",
      "\u23ed\ufe0f Skipped (cached): Cooper_etal_2013_2.md\n",
      "\u23ed\ufe0f Skipped (cached): Spirandellietal2016_ImprovingAdaptationPlanningforSLR.md\n",
      "\u23ed\ufe0f Skipped (cached): JCOASTRES-D-11-00114.md\n",
      "\u23ed\ufe0f Skipped (cached): Vitousek_SCD08.md\n",
      "\u23ed\ufe0f Skipped (cached): Bochicchio_Marine_Geo09.md\n",
      "\u23ed\ufe0f Skipped (cached): Neil_Tiffany_Chip_2009.md\n",
      "\u23ed\ufe0f Skipped (cached): Vitouseketal_NatureSR2017.md\n",
      "\u23ed\ufe0f Skipped (cached): FletcherFiersten_Hawaiichaptercoasts.md\n",
      "\u23ed\ufe0f Skipped (cached): Rubin_Fletcher_Sherman2001.md\n",
      "\u23ed\ufe0f Skipped (cached): fletcher2009_sealevelreview.md\n",
      "\u23ed\ufe0f Skipped (cached): Harney_Fletcher_JSR_2003.md\n",
      "\u23ed\ufe0f Skipped (cached): HabelEtal_WR_2017.md\n",
      "\u23ed\ufe0f Skipped (cached): Andrade_et_al_2023-coas-40-02-338-352.md\n",
      "\u23ed\ufe0f Skipped (cached): Vitousek_PSC08.md\n",
      "\u23ed\ufe0f Skipped (cached): CS2003_Norcross_LongshoreTransport.md\n",
      "\u23ed\ufe0f Skipped (cached): Conger_marinegeo_2009.md\n",
      "\u23ed\ufe0f Skipped (cached): Anderson_et_al_2015_NaturalHazards.md\n",
      "\u23ed\ufe0f Skipped (cached): Fletcher-Chapter7-climate-change.md\n",
      "\u23ed\ufe0f Skipped (cached): Rotzoll Fletcher NCC 2012.md\n",
      "\u23ed\ufe0f Skipped (cached): CNMI Climate 2016.md\n",
      "\u23ed\ufe0f Skipped (cached): computation_of_energetic_nearshore_waves.md\n",
      "\u23ed\ufe0f Skipped (cached): Act-238_HSEO_Decarbonization_Report.md\n",
      "\u23ed\ufe0f Skipped (cached): d41586-024-00917-9.md\n",
      "\u23ed\ufe0f Skipped (cached): ClimateBrief_low.md\n",
      "\u23ed\ufe0f Skipped (cached): KCAP_ClimateWP_22_0302.md\n",
      "\u23ed\ufe0f Skipped (cached): GeologyofHawaiiReefs.md\n",
      "\u23ed\ufe0f Skipped (cached): Anderson_etal_2015_JCR.md\n",
      "\u23ed\ufe0f Skipped (cached): Kane_et_al_2015_ClimateChange.md\n",
      "\u23ed\ufe0f Skipped (cached): ClimateChange_in_FSM.md\n",
      "\u23ed\ufe0f Skipped (cached): MappingShorelineCh106-124.md\n",
      "\u23ed\ufe0f Skipped (cached): GeologyGeomorphology_NWHI_Coral_Reefs2008.md\n",
      "\u23ed\ufe0f Skipped (cached): CoastalSedimentary.md\n",
      "\u23ed\ufe0f Skipped (cached): HarneyCoralReefs2000.md\n",
      "\u23ed\ufe0f Skipped (cached): ClimateChange_in_FSM_Exec_Summary.md\n",
      "\u23ed\ufe0f Skipped (cached): Norcross_SCD08.md\n",
      "\u23ed\ufe0f Skipped (cached): EngelsJSR04.md\n",
      "\u23ed\ufe0f Skipped (cached): VitouseketalProceeding07.md\n",
      "\u23ed\ufe0f Skipped (cached): ClimateChangeFSM.md\n",
      "\u23ed\ufe0f Skipped (cached): Fletcher_KaPili_Kai_09.md\n",
      "\u23ed\ufe0f Skipped (cached): Coyne-MappingCoastalErosion-1999.md\n",
      "\u23ed\ufe0f Skipped (cached): 1-s2.0-S002532272200041X-main.md\n",
      "\u23ed\ufe0f Skipped (cached): 230125_Final Booklet.md\n",
      "\u23ed\ufe0f Skipped (cached): Romine_Fletcher_inpress_HI_ShoreChange_Summary_JCR.md\n",
      "\u23ed\ufe0f Skipped (cached): annurev-marine-020923-120737.md\n",
      "\u23ed\ufe0f Skipped (cached): 230131_Final Booklet.md\n",
      "\u23ed\ufe0f Skipped (cached): KaneEtAl2014_SLRCriticalElevation.md\n",
      "\u23ed\ufe0f Skipped (cached): sherman_JSR_1999.md\n",
      "\u23ed\ufe0f Skipped (cached): Sherman et al_QuatRes_v81_p138-150.md\n",
      "\u23ed\ufe0f Skipped (cached): Conger_TGARS.md\n",
      "\u23ed\ufe0f Skipped (cached): remotesensing-12-00154.md\n",
      "\u23ed\ufe0f Skipped (cached): Genz_06-0756.md\n",
      "\u23ed\ufe0f Skipped (cached): WaikikiUAS_Defense_OnlineVersion.md\n",
      "\u23ed\ufe0f Skipped (cached): Earth s Future - 2020 - Kane - Rethinking Reef Island Stability in Relation to Anthropogenic Sea Level Rise.md\n",
      "\u23ed\ufe0f Skipped (cached): RooneyCoastalSed2003.md\n",
      "\u23ed\ufe0f Skipped (cached): Cooper_etal_2012.md\n",
      "\u23ed\ufe0f Skipped (cached): Kane2017_QuaternaryResearch.md\n",
      "\u23ed\ufe0f Skipped (cached): coastal_land_subsidence.md\n",
      "\u23ed\ufe0f Skipped (cached): Anderson_Frazer_JCR_preprint.md\n",
      "\u23ed\ufe0f Skipped (cached): i2761.md\n",
      "\u23ed\ufe0f Skipped (cached): Romine Fletcher 2013 Oahu Armoring.md\n",
      "\u23ed\ufe0f Skipped (cached): anderson_et_al_GRL_2009.md\n",
      "\u23ed\ufe0f Skipped (cached): GenzetalProceeding.md\n",
      "\u23ed\ufe0f Skipped (cached): Romine_coas-25-04-17.md\n",
      "\u23ed\ufe0f Skipped (cached): Jrooney2000.md\n",
      "\u23ed\ufe0f Skipped (cached): Genz_06-0757.md\n",
      "\u23ed\ufe0f Skipped (cached): RichmondHCH2001.md\n",
      "\u23ed\ufe0f Skipped (cached): ofr2011-1051_report_508.md\n",
      "\u23ed\ufe0f Skipped (cached): OCCL23-Sea-Level-Rise-Report-FY22-1.md\n",
      "\u23ed\ufe0f Skipped (cached): Fletcher-Chapter6-slr-hawaii.md\n",
      "\u23ed\ufe0f Skipped (cached): anderson_et_al_auxiliary_materials.md\n",
      "\u23ed\ufe0f Skipped (cached): Anderson_et_al_SciRep_2018_SLR_modeling.md\n",
      "\u23ed\ufe0f Skipped (cached): KaneEtAl2014_RankedManagementConcerns.md\n",
      "\u23ed\ufe0f Skipped (cached): Maui Shoreline Rules Chapter 203 - Dr. Chip Fletcher Testimony.md\n",
      "\u23ed\ufe0f Skipped (cached): s10584-018-2327-7.md\n",
      "\u23ed\ufe0f Skipped (cached): Cooper_etal_2013.md\n",
      "\u23ed\ufe0f Skipped (cached): IsounCoralReefs03.md\n",
      "\u23ed\ufe0f Skipped (cached): wave_driven_cross_shore.md\n",
      "\u23ed\ufe0f Skipped (cached): Romine et al 2016 Beach Erosion Under Rising Sea Level.md\n",
      "\u23ed\ufe0f Skipped (cached): ARCC2023Proceedings.md\n",
      "\u23ed\ufe0f Skipped (cached): Anderson_etal_2014_JCR.md\n",
      "\u23ed\ufe0f Skipped (cached): Kurylyketal.2025NCities.md\n",
      "\u23ed\ufe0f Skipped (cached): Habel_2019_Environ._Res._Commun._1_041005.md\n",
      "\u23ed\ufe0f Skipped (cached): CoralReefsEngels.md\n",
      "\u23ed\ufe0f Skipped (cached): 1-s2.0-S2213305421000163-main.md\n",
      "\u23ed\ufe0f Skipped (cached): AmSamoa Climate 2016.md\n",
      "\u23ed\ufe0f Skipped (cached): FletcherEtAl1990.md\n",
      "\u23ed\ufe0f Skipped (cached): remotesensing-14-05108.md\n",
      "\u23ed\ufe0f Skipped (cached): Habel_Waikiki_replen_Coastal_eng_2016.md\n",
      "\u23ed\ufe0f Skipped (cached): s41597-024-03160-z.md\n",
      "\u23ed\ufe0f Skipped (cached): Paoa_et_al-2023-Scientific_Reports.md\n",
      "\u23ed\ufe0f Skipped (cached): 43UHawLRev464.md\n",
      "\u23ed\ufe0f Skipped (cached): Use surplus to protect Sunset Beach.md\n",
      "\u23ed\ufe0f Skipped (cached): s41598-020-70577-y.md\n",
      "\u23ed\ufe0f Skipped (cached): SLR_Constraint_District_Ordinance.md\n",
      "\u23ed\ufe0f Skipped (cached): Romine_SCD08.md\n",
      "\u23ed\ufe0f Skipped (cached): Lander_et al_Envisioning_In-Situ_Sea_Level_Rise_Adaptation_for_Coastal_Cities.md\n",
      "\u23ed\ufe0f Skipped (cached): Hawaii_Natural_Resources_Law_Enforcement_Manual.10.17.2022.md\n",
      "\u23ed\ufe0f Skipped (cached): Romine et al 2013 Beach Erosion and SLR in HI.md\n",
      "\u23ed\ufe0f Skipped (cached): Bochicchio_etal_2009.md\n",
      "\u23ed\ufe0f Skipped (cached): Guam Climate 2016.md\n",
      "\u23ed\ufe0f Skipped (cached): Cochrane_etal2015.md\n",
      "\u23ed\ufe0f Skipped (cached): FletcherEtAl1993_SLRAccelerationandDrowningofDelawareBayCoast18k.md\n",
      "\u23ed\ufe0f Skipped (cached): s10584-023-03602-4.md\n",
      "\u23ed\ufe0f Skipped (cached): Habel_et_al_flood_comparison.md\n",
      "\u23ed\ufe0f Skipped (cached): Kaneetal2012.md\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 910 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 75114 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 118224 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 62246 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 66169 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 72882 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\ud83d\udd04 Starting sanitization... (Input length: 121565 characters)\n",
      "\ud83d\udce1 Sending request to Gemini API...\n",
      "\u26a0\ufe0f Warning: API returned None, returning empty string\n",
      "\u2705 Processed: BeachManagementPlan_1992_scanned.md\n",
      "\u2705 Sanitization complete! (Output length: 41179 characters)\n",
      "\u2705 Processed: Andrade et al 2023.md\n",
      "\u2705 Sanitization complete! (Output length: 44132 characters)\n",
      "\u2705 Processed: 2024_Illustrating_Urban_Plans_Meguro_et_al.md\n",
      "\u2705 Sanitization complete! (Output length: 40929 characters)\n",
      "\u2705 Processed: Grossman_Fletcher_JSR_2003.md\n",
      "\u2705 Sanitization complete! (Output length: 82293 characters)\n",
      "\u2705 Processed: HawaiiReef_07.md\n",
      "\u2705 Sanitization complete! (Output length: 46620 characters)\n",
      "\u2705 Processed: KailuaBay.md\n",
      "\u2705 Sanitization complete! (Output length: 74303 characters)\n",
      "\u2705 Processed: fletcher_2024_pnas_nexus.md\n",
      "------------------------------------------------------------\n",
      "\ud83c\udf89 Completed in 214.86 seconds\n",
      "\ud83d\udcc8 Stats: 7 processed, 105 cached, 0 failed\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from threading import Semaphore\n",
    "\n",
    "markdown_file_path = Path('outputs/full_text_v2/')\n",
    "markdown_files = list(markdown_file_path.glob(\"*.md\"))\n",
    "output_file_path = Path('outputs/cleaned_full_text_v2/')\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 10  # Number of parallel requests (adjust based on your API quota)\n",
    "RATE_LIMIT_DELAY = 1.0  # Delay between requests in seconds\n",
    "\n",
    "# Semaphore to control rate limiting\n",
    "rate_limiter = Semaphore(MAX_WORKERS)\n",
    "\n",
    "def process_file(file_path: Path) -> tuple[str, bool, str]:\n",
    "    \"\"\"\n",
    "    Process a single markdown file with caching support.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filename, success, message)\n",
    "    \"\"\"\n",
    "    output_file = output_file_path / file_path.name\n",
    "    \n",
    "    # Check cache: skip if already processed\n",
    "    if output_file.exists():\n",
    "        return (file_path.name, True, \"\u23ed\ufe0f Skipped (cached)\")\n",
    "    \n",
    "    try:\n",
    "        # Read input file\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Rate limiting\n",
    "        with rate_limiter:\n",
    "            sanitized_text = sanitize_markdown(content)\n",
    "            time.sleep(RATE_LIMIT_DELAY)  # Prevent overwhelming API\n",
    "        \n",
    "        # Write output file\n",
    "        with open(output_file, \"w\") as f:\n",
    "            f.write(sanitized_text)\n",
    "        \n",
    "        return (file_path.name, True, \"\u2705 Processed\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        return (file_path.name, False, f\"\u274c Error: {str(e)}\")\n",
    "\n",
    "# Process files in parallel\n",
    "print(f\"\ud83d\udcca Total files: {len(markdown_files)}\")\n",
    "print(f\"\ud83d\udd27 Max workers: {MAX_WORKERS}, Rate limit: {RATE_LIMIT_DELAY}s\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "cached_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_file = {executor.submit(process_file, file): file for file in markdown_files}\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for future in as_completed(future_to_file):\n",
    "        filename, success, message = future.result()\n",
    "        print(f\"{message}: {filename}\")\n",
    "        \n",
    "        if \"cached\" in message.lower():\n",
    "            cached_count += 1\n",
    "        elif success:\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"\ud83c\udf89 Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"\ud83d\udcc8 Stats: {processed_count} processed, {cached_count} cached, {failed_count} failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f169e355",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List, Optional, Dict, Any\n",
    "from pydantic import BaseModel, Field, ValidationError, field_validator\n",
    "from enum import Enum\n",
    "from google.genai.errors import APIError\n",
    "import dotenv\n",
    "from google import genai\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Initialize the Gemini Client\n",
    "try:\n",
    "    client = genai.Client()\n",
    "except KeyError:\n",
    "    print(\"Error: Please set the GEMINI_API_KEY environment variable.\")\n",
    "    exit()\n",
    "\n",
    "# Define the confidence levels as an enum\n",
    "class ConfidenceLevel(str, Enum):\n",
    "    HIGH = \"HIGH\"\n",
    "    MEDIUM = \"MEDIUM\"\n",
    "    LOW = \"LOW\"\n",
    "\n",
    "# Define valid layer types\n",
    "class LayerType(str, Enum):\n",
    "    PASSIVE_MARINE_FLOODING = \"passive_marine_flooding\"\n",
    "    GROUNDWATER_INUNDATION = \"groundwater_inundation\"\n",
    "    LOW_LYING_FLOODING = \"low_lying_flooding\"\n",
    "    COMPOUND_FLOODING = \"compound_flooding\"\n",
    "    DRAINAGE_BACKFLOW = \"drainage_backflow\"\n",
    "    FUTURE_EROSION_HAZARD_ZONE = \"future_erosion_hazard_zone\"\n",
    "    ANNUAL_HIGH_WAVE_FLOODING = \"annual_high_wave_flooding\"\n",
    "    EMERGENT_AND_SHALLOW_GROUNDWATER = \"emergent_and_shallow_groundwater\"\n",
    "\n",
    "# Layer validation keywords - used to verify layer assignments\n",
    "LAYER_KEYWORDS = {\n",
    "    \"passive_marine_flooding\": [\"marine inundation\", \"coastal flooding\", \"inundation zone\", \"bathtub model\", \"mhhw\", \"hydrologically connected\"],\n",
    "    \"groundwater_inundation\": [\"modflow\", \"groundwater\", \"water table rise\", \"subsurface flooding\", \"flood depth\", \"aquifer\"],\n",
    "    \"low_lying_flooding\": [\"critical elevation\", \"elevation threshold\", \"low-lying\", \"not hydrologically connected\", \"dem analysis\"],\n",
    "    \"compound_flooding\": [\"compound flooding\", \"combined effects\", \"multiple flood\", \"concurrent flooding\"],\n",
    "    \"drainage_backflow\": [\"storm drain\", \"drainage backflow\", \"sewer flooding\", \"drainage network\"],\n",
    "    \"future_erosion_hazard_zone\": [\"erosion rate\", \"m/year\", \"shoreline change\", \"coastal retreat\", \"shoreline retreat\"],\n",
    "    \"annual_high_wave_flooding\": [\"bosz\", \"wave runup\", \"wave-driven flooding\", \"extreme wave\", \"overwash\", \"gev\"],\n",
    "    \"emergent_and_shallow_groundwater\": [\"shallow groundwater\", \"water table depth\", \"groundwater level\", \"subsurface water\"]\n",
    "}\n",
    "\n",
    "# Define the Pydantic model for the analysis result\n",
    "class PaperAnalysis(BaseModel):\n",
    "    relevant: bool = Field(..., description=\"Whether the paper is relevant for the database\")\n",
    "    confidence: ConfidenceLevel = Field(..., description=\"Confidence level of the analysis\")\n",
    "    relevant_layers: List[str] = Field(\n",
    "        default_factory=list, \n",
    "        max_length=2,\n",
    "        description=\"Most relevant data layers (max 2)\"\n",
    "    )\n",
    "    reasoning: str = Field(..., description=\"Explanation of the classification\")\n",
    "    key_findings: Optional[List[str]] = Field(default_factory=list, description=\"Key findings from the paper\")\n",
    "    quantitative_data: Dict[str, Any] = Field(\n",
    "        default_factory=dict,\n",
    "        description=\"Quantitative data extracted from the paper\"\n",
    "    )\n",
    "    \n",
    "    @field_validator('relevant_layers')\n",
    "    @classmethod\n",
    "    def validate_layers(cls, v):\n",
    "        \"\"\"Validate that layers are from the allowed set\"\"\"\n",
    "        valid_layers = [layer.value for layer in LayerType]\n",
    "        for layer in v:\n",
    "            if layer not in valid_layers:\n",
    "                print(f\"\u26a0\ufe0f Warning: Invalid layer '{layer}' - will be ignored\")\n",
    "        return [layer for layer in v if layer in valid_layers]\n",
    "    \n",
    "    class Config:\n",
    "        use_enum_values = True\n",
    "\n",
    "def validate_layer_assignment(full_text: str, assigned_layers: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validates that assigned layers have supporting evidence in the text.\n",
    "    \n",
    "    Args:\n",
    "        full_text: The full text of the paper\n",
    "        assigned_layers: List of layers assigned by the AI\n",
    "        \n",
    "    Returns:\n",
    "        Dict with validation results and confidence scores\n",
    "    \"\"\"\n",
    "    validation_results = {}\n",
    "    text_lower = full_text.lower()\n",
    "    \n",
    "    for layer in assigned_layers:\n",
    "        if layer not in LAYER_KEYWORDS:\n",
    "            validation_results[layer] = {\n",
    "                \"valid\": False,\n",
    "                \"confidence\": 0.0,\n",
    "                \"found_keywords\": [],\n",
    "                \"warning\": \"Unknown layer type\"\n",
    "            }\n",
    "            continue\n",
    "        \n",
    "        keywords = LAYER_KEYWORDS[layer]\n",
    "        found_keywords = []\n",
    "        \n",
    "        for keyword in keywords:\n",
    "            if keyword.lower() in text_lower:\n",
    "                found_keywords.append(keyword)\n",
    "        \n",
    "        # Calculate confidence based on keyword matches\n",
    "        confidence = len(found_keywords) / len(keywords) if keywords else 0.0\n",
    "        \n",
    "        validation_results[layer] = {\n",
    "            \"valid\": len(found_keywords) > 0,\n",
    "            \"confidence\": round(confidence, 2),\n",
    "            \"found_keywords\": found_keywords,\n",
    "            \"warning\": None if len(found_keywords) > 0 else f\"No supporting keywords found for {layer}\"\n",
    "        }\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def analyze_paper(full_text: str) -> dict:\n",
    "    \"\"\"\n",
    "    Analyzes a scientific paper and returns a validated dictionary with the analysis results.\n",
    "    Uses Gemini 2.0 Flash Thinking for deeper reasoning and includes few-shot examples.\n",
    "    \n",
    "    Args:\n",
    "        full_text (str): The full text of the paper to analyze.\n",
    "        \n",
    "    Returns:\n",
    "        dict: A validated dictionary containing the analysis results with keys:\n",
    "            - relevant (bool)\n",
    "            - confidence (str): HIGH, MEDIUM, or LOW\n",
    "            - relevant_layers (list): Up to 2 most relevant layers\n",
    "            - reasoning (str)\n",
    "            - quantitative_data (dict)\n",
    "            - layer_validation (dict): Validation results for assigned layers\n",
    "            - key_findings (list, optional)\n",
    "    \"\"\"\n",
    "\n",
    "    # --- SYSTEM PROMPT WITH FEW-SHOT EXAMPLES ---\n",
    "    system_prompt = \"\"\"\n",
    "    **SYSTEM INSTRUCTION: Geospatial Database Analyst (Strict JSON Output)**\n",
    "\n",
    "    Your role is to act as a specialized data analyst indexing scientific papers for the 'Hawaiian Sea Level Rise Database.' \n",
    "    You MUST adhere to all rules below and return ONLY a single, valid JSON object. \n",
    "    Do not include any text outside the JSON structure.\n",
    "    \n",
    "    **FEW-SHOT EXAMPLES:**\n",
    "    \n",
    "    Example 1 - HIGH Confidence:\n",
    "    Paper: \"Sea level rise impacts on groundwater inundation in Honolulu\"\n",
    "    Abstract mentions: \"MODFLOW modeling of Oahu aquifer shows 0.5m SLR causes water table rise of 0.3-0.4m in urban Honolulu, \n",
    "    affecting 2,500 properties by 2050.\"\n",
    "    Classification: HIGH confidence, relevant=true, layers=[\"groundwater_inundation\"]\n",
    "    Reasoning: Hawaii-specific location (Honolulu, Oahu), quantitative projections (0.5m SLR, 2,500 properties, 2050), \n",
    "    specific methodology (MODFLOW).\n",
    "    \n",
    "    Example 2 - MEDIUM Confidence:\n",
    "    Paper: \"Beach erosion patterns in tropical island environments\"\n",
    "    Abstract mentions: \"Study of 15 tropical islands including Hawaii shows erosion rates correlate with wave exposure. \n",
    "    Framework applicable to Pacific islands.\"\n",
    "    Classification: MEDIUM confidence, relevant=true, layers=[\"future_erosion_hazard_zone\"]\n",
    "    Reasoning: Hawaii mentioned but broader geographic focus, methodology applicable to Hawaii but not Hawaii-specific data.\n",
    "    \n",
    "    Example 3 - LOW Confidence:\n",
    "    Paper: \"Global sea level rise projections for the 21st century\"\n",
    "    Abstract mentions: \"IPCC AR6 scenarios project 0.5-1.0m global SLR by 2100. Hawaii tide gauge data referenced briefly.\"\n",
    "    Classification: LOW confidence, relevant=false, layers=[]\n",
    "    Reasoning: Hawaii only mentioned in passing, global focus without Hawaii-specific findings or actionable local data.\n",
    "    \"\"\"\n",
    "    \n",
    "    analysis_instructions = f\"\"\"\n",
    "    === FULL TEXT FOR ANALYSIS ===\n",
    "    {full_text}\n",
    "\n",
    "    === CORE EXECUTION STEPS ===\n",
    "    1.  **Review:** Scan the full text, prioritizing the **Methods, Results, and Discussion** sections.\n",
    "    2.  **Identify:** Extract all specific Hawaiian locations, quantitative measurements, and time projections.\n",
    "    3.  **Classify Confidence:** Determine the **Final Confidence** (HIGH/MEDIUM/LOW) using the **CONFIDENCE CRITERIA** table below.\n",
    "    4.  **Assign Layers:** Select the **1 or 2 MOST RELEVANT** layers from the **LAYER DEFINITIONS** table, based ONLY on quantitative findings in the Results/Discussion. **DO NOT** select layers based solely on methodology.\n",
    "    5.  **Extract Data:** Pull out specific quantitative data (measurements, rates, dates, locations) into the quantitative_data object.\n",
    "    6.  **Justify:** Write clear reasoning explaining your classification.\n",
    "\n",
    "    === CONFIDENCE CRITERIA (Reference Table) ===\n",
    "\n",
    "    | Level | Requirement |\n",
    "    | :--- | :--- |\n",
    "    | **HIGH** | Focuses specifically on Hawaiian locations **AND** contains quantitative data/projections **AND** includes clear, Hawaii-specific methodology. |\n",
    "    | **MEDIUM** | Methodology is applicable to Hawaii but not Hawaii-specific data **OR** mentions Hawaii but focuses on broader Pacific/global context **OR** findings are qualitative/conceptual. |\n",
    "    | **LOW** | Hawaii mentioned only in passing, no actionable data, or methodology is irrelevant to the Hawaiian context. |\n",
    "\n",
    "    === LAYER DEFINITIONS (Max 2 Layers) ===\n",
    "\n",
    "    | Layer ID | Mechanism/Focus | Keywords & Evidence (MUST be present in Results/Discussion) |\n",
    "    | :--- | :--- | :--- |\n",
    "    | **passive_marine_flooding** | Direct ocean water inundation (marine connected) | \"marine inundation\", \"coastal flooding\", \"inundation zone\", \"bathtub model\", \"MHHW datum\", \"hydrologically connected\" |\n",
    "    | **groundwater_inundation** | Flooding from rising groundwater table | \"**MODFLOW**\", \"groundwater\", \"water table rise\", \"subsurface flooding\", \"flood depths\", \"aquifer\" |\n",
    "    | **low_lying_flooding** | Low elevation areas (not marine connected) | \"critical elevation\", \"below [X]m/ft\", \"elevation threshold\", \"low-lying areas\", \"not hydrologically connected\", \"DEM analysis\" |\n",
    "    | **compound_flooding** | Multiple simultaneous flood mechanisms | \"compound flooding\", \"combined effects\", \"rainfall + high tide\", \"storm surge + rain\", \"concurrent flooding\" |\n",
    "    | **drainage_backflow** | Stormwater/sewer system flooding | \"storm drain\", \"drainage backflow\", \"sewer flooding\", \"urban coastal drainage\", \"gravity-flow networks\" |\n",
    "    | **future_erosion_hazard_zone** | Shoreline retreat rates/predictions | \"erosion rate\", \"[X] m/year\", \"shoreline change\", \"coastal retreat\" |\n",
    "    | **annual_high_wave_flooding** | Wave-driven coastal flooding events | \"**BOSZ**\", \"wave runup\", \"wave-driven flooding\", \"extreme waves\", \"overwash\", \"**GEV** analysis\" |\n",
    "    | **emergent_and_shallow_groundwater** | Groundwater near or at surface (depth to water table) | \"shallow groundwater\", \"water table depth\", \"groundwater level\", \"subsurface water\", \"GWI modeling output\" |\n",
    "\n",
    "    === LAYER SELECTION RULES ===\n",
    "    1. Select ONLY layers with explicit evidence in Results/Discussion sections.\n",
    "    2. Maximum **2 layers** per paper - choose the most prominent findings.\n",
    "    3. If paper covers multiple aspects, prioritize quantitative results over methodology.\n",
    "    4. Don't assign layers based solely on Methods - findings must be present.\n",
    "    5. If uncertain between layers, choose the one with more quantitative support.\n",
    "    6. Only assign a layer if you found specific keywords or evidence from the LAYER DEFINITIONS table above.\n",
    "\n",
    "    === RELEVANCE CRITERION ===\n",
    "    The 'relevant' field must be set to **true** ONLY if the final confidence level is determined to be **HIGH** or **MEDIUM**. \n",
    "    If the confidence is **LOW**, the paper is considered not relevant for indexing, and the field must be set to **false**.\n",
    "\n",
    "    === QUANTITATIVE DATA EXTRACTION ===\n",
    "    Extract into quantitative_data object:\n",
    "    - locations: List of specific Hawaiian place names mentioned\n",
    "    - slr_projections: Sea level rise values and years (e.g., \"0.5m by 2050\")\n",
    "    - measurements: Specific measurements (erosion rates, flood depths, etc.)\n",
    "    - timeframes: Study periods or projection years\n",
    "    \n",
    "    === TARGET JSON SCHEMA ===\n",
    "    Return a JSON object with these exact fields.\n",
    "    \"\"\"\n",
    "\n",
    "    # Gemini API JSON schema for structured output\n",
    "    JSON_SCHEMA = {\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"relevant\": {\"type\": \"BOOLEAN\"},\n",
    "            \"confidence\": {\"type\": \"STRING\", \"enum\": [\"HIGH\", \"MEDIUM\", \"LOW\"]},\n",
    "            \"relevant_layers\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"items\": {\"type\": \"STRING\"},\n",
    "                \"maxItems\": 2\n",
    "            },\n",
    "            \"reasoning\": {\"type\": \"STRING\"},\n",
    "            \"key_findings\": {\n",
    "                \"type\": \"ARRAY\",\n",
    "                \"items\": {\"type\": \"STRING\"}\n",
    "            },\n",
    "            \"quantitative_data\": {\n",
    "                \"type\": \"OBJECT\",\n",
    "                \"properties\": {\n",
    "                    \"locations\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "                    \"slr_projections\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "                    \"measurements\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}},\n",
    "                    \"timeframes\": {\"type\": \"ARRAY\", \"items\": {\"type\": \"STRING\"}}\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\n",
    "            \"relevant\", \"confidence\", \"relevant_layers\", \"reasoning\", \n",
    "            \"quantitative_data\"\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(\"\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\")\n",
    "        response = client.models.generate_content(\n",
    "            model='gemini-2.5-flash',\n",
    "            contents=[\n",
    "                {\"role\": \"user\", \"parts\": [{\"text\": analysis_instructions}]},\n",
    "            ],\n",
    "            config=genai.types.GenerateContentConfig(\n",
    "                system_instruction=system_prompt,\n",
    "                response_mime_type=\"application/json\",\n",
    "                response_schema=JSON_SCHEMA,\n",
    "                temperature=0.1,  # Lower temperature for more consistent analysis\n",
    "                max_output_tokens=4096  # Allow longer responses for detailed analysis\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        result_json_str = response.text\n",
    "        if result_json_str is None:\n",
    "            print(\"\u26a0\ufe0f Warning: API returned None\")\n",
    "            return PaperAnalysis(\n",
    "                relevant=False,\n",
    "                confidence=ConfidenceLevel.LOW,\n",
    "                relevant_layers=[],\n",
    "                reasoning=\"API error - no response received\"\n",
    "            ).model_dump()\n",
    "        \n",
    "        # Parse JSON and validate with Pydantic\n",
    "        result_dict = json.loads(result_json_str)\n",
    "        validated_result = PaperAnalysis(**result_dict)\n",
    "        \n",
    "        # Validate layer assignments against actual text\n",
    "        if validated_result.relevant_layers:\n",
    "            layer_validation = validate_layer_assignment(full_text, validated_result.relevant_layers)\n",
    "            \n",
    "            # Log validation warnings\n",
    "            for layer, validation in layer_validation.items():\n",
    "                if not validation[\"valid\"]:\n",
    "                    print(f\"\u26a0\ufe0f Layer Validation Warning: {validation['warning']}\")\n",
    "                else:\n",
    "                    print(f\"\u2705 Layer '{layer}' validated (confidence: {validation['confidence']}, keywords: {len(validation['found_keywords'])})\")\n",
    "        else:\n",
    "            layer_validation = {}\n",
    "        \n",
    "        print(f\"\u2705 Analysis complete! Confidence: {validated_result.confidence}\")\n",
    "        print(f\"   Relevant: {validated_result.relevant}, Layers: {validated_result.relevant_layers}\")\n",
    "        \n",
    "        # Return as dictionary with validation results\n",
    "        result = validated_result.model_dump()\n",
    "        result['layer_validation'] = layer_validation\n",
    "        return result\n",
    "        \n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"\u274c JSON Parsing Error: {e}\")\n",
    "        return PaperAnalysis(\n",
    "            relevant=False,\n",
    "            confidence=ConfidenceLevel.LOW,\n",
    "            relevant_layers=[],\n",
    "            reasoning=f\"Failed to parse API response: {str(e)}\"\n",
    "        ).model_dump()\n",
    "        \n",
    "    except ValidationError as e:\n",
    "        print(f\"\u274c Pydantic Validation Error: {e}\")\n",
    "        # Try to return the raw data with error info\n",
    "        return {\n",
    "            \"status\": \"validation_error\",\n",
    "            \"error\": str(e),\n",
    "            \"relevant\": False,\n",
    "            \"confidence\": \"LOW\",\n",
    "            \"relevant_layers\": [],\n",
    "            \"reasoning\": f\"Data validation failed: {str(e)}\"\n",
    "        }\n",
    "        \n",
    "    except APIError as e:\n",
    "        print(f\"\u274c API Error during analysis: {e}\")\n",
    "        return PaperAnalysis(\n",
    "            relevant=False,\n",
    "            confidence=ConfidenceLevel.LOW,\n",
    "            relevant_layers=[],\n",
    "            reasoning=f\"API Error: {str(e)}\"\n",
    "        ).model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd968037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Total files: 117\n",
      "\ud83d\udd27 Max workers: 10, Rate limit: 1s\n",
      "\ud83d\udcdd Output will be written to: outputs/combined_analysis_results.json\n",
      "------------------------------------------------------------\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: MEDIUM\n",
      "   Relevant: True, Layers: ['groundwater_inundation']\n",
      "\u2705 Layer 'annual_high_wave_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "\u2705 Layer 'passive_marine_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'future_erosion_hazard_zone']\n",
      "\u2705 Processed: Vitousek_SCD08.md\n",
      "\u2705 Processed: d41586-024-00917-9.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: ClimateBrief_low.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: CS2003_Norcross_LongshoreTransport.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: GeologyofHawaiiReefs.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: JCOASTRES-D-11-00114.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Layer 'annual_high_wave_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding']\n",
      "\u2705 Processed: CNMI Climate 2016.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: computation_of_energetic_nearshore_waves.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 18 column 7 (char 1638)\n",
      "\u2705 Processed: Spirandellietal2016_ImprovingAdaptationPlanningforSLR.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 11 column 5 (char 789)\n",
      "\u2705 Processed: Rubin_Fletcher_Sherman2001.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Conger_marinegeo_2009.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'passive_marine_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.33, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'groundwater_inundation']\n",
      "\u2705 Processed: Cooper_etal_2013_2.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Layer 'emergent_and_shallow_groundwater' validated (confidence: 0.5, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 1 column 1772 (char 1771)\n",
      "\u2705 Processed: HabelEtal_WR_2017.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Bochicchio_Marine_Geo09.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Harney_Fletcher_JSR_2003.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Expecting value: line 66 column 1 (char 3171)\n",
      "\u2705 Processed: ClimateChange_in_FSM.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Anderson_etal_2015_JCR.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: MEDIUM\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: GeologyGeomorphology_NWHI_Coral_Reefs2008.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: FletcherFiersten_Hawaiichaptercoasts.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Fletcher-Chapter7-climate-change.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 1 column 2537 (char 2536)\n",
      "\u2705 Processed: Rotzoll Fletcher NCC 2012.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Expecting value: line 39 column 7 (char 1815)\n",
      "\u2705 Processed: MappingShorelineCh106-124.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 15 column 5 (char 1464)\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.8, keywords: 4)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: KCAP_ClimateWP_22_0302.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Anderson_et_al_2015_NaturalHazards.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'future_erosion_hazard_zone']\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 56 column 7 (char 3089)\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: Kane_et_al_2015_ClimateChange.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'annual_high_wave_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: MEDIUM\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding']\n",
      "\u2705 Processed: fletcher2009_sealevelreview.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Andrade_et_al_2023-coas-40-02-338-352.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Vitouseketal_NatureSR2017.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'annual_high_wave_flooding' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding']\n",
      "\u2705 Processed: Vitousek_PSC08.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: HarneyCoralReefs2000.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: CoastalSedimentary.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: ClimateChange_in_FSM_Exec_Summary.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Neil_Tiffany_Chip_2009.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 8 column 16 (char 152)\n",
      "\u2705 Processed: Norcross_SCD08.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 74 column 7 (char 2942)\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u2705 Processed: EngelsJSR04.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Act-238_HSEO_Decarbonization_Report.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "\u2705 Layer 'annual_high_wave_flooding' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone', 'annual_high_wave_flooding']\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.33, keywords: 2)\n",
      "\u2705 Layer 'emergent_and_shallow_groundwater' validated (confidence: 0.5, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "\u2705 Processed: 1-s2.0-S002532272200041X-main.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: remotesensing-12-00154.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: annurev-marine-020923-120737.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 11 column 5 (char 875)\n",
      "\u274c JSON Parsing Error: Expecting value: line 28 column 1 (char 1513)\n",
      "\u2705 Processed: 230131_Final Booklet.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'low_lying_flooding' validated (confidence: 0.4, keywords: 2)\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['low_lying_flooding', 'groundwater_inundation']\n",
      "\u2705 Processed: Coyne-MappingCoastalErosion-1999.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: KaneEtAl2014_SLRCriticalElevation.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for passive_marine_flooding\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for emergent_and_shallow_groundwater\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'emergent_and_shallow_groundwater']\n",
      "\u2705 Processed: Fletcher_KaPili_Kai_09.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: sherman_JSR_1999.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: ClimateChangeFSM.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: groundwater_inundation.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Layer 'drainage_backflow' validated (confidence: 0.25, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'drainage_backflow']\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: 230125_Final Booklet.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Kane2017_QuaternaryResearch.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u2705 Processed: VitouseketalProceeding07.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Genz_06-0756.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: WaikikiUAS_Defense_OnlineVersion.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 25 column 7 (char 1421)\n",
      "\u2705 Processed: coastal_land_subsidence.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 66 column 7 (char 3033)\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u2705 Processed: Fletcher-Chapter6-slr-hawaii.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Romine_Fletcher_inpress_HI_ShoreChange_Summary_JCR.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 29 column 7 (char 2109)\n",
      "\u2705 Processed: Anderson_et_al_SciRep_2018_SLR_modeling.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Conger_TGARS.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: Anderson_Frazer_JCR_preprint.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 7 column 5 (char 398)\n",
      "\u2705 Processed: Earth s Future - 2020 - Kane - Rethinking Reef Island Stability in Relation to Anthropogenic Sea Level Rise.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'passive_marine_flooding' validated (confidence: 0.67, keywords: 4)\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for emergent_and_shallow_groundwater\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'emergent_and_shallow_groundwater']\n",
      "\u2705 Processed: Cooper_etal_2012.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: GenzetalProceeding.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Expecting value: line 51 column 25 (char 2860)\n",
      "\u2705 Processed: Anderson_etal_2014_JCR.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Sherman et al_QuatRes_v81_p138-150.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: []\n",
      "\u2705 Processed: IsounCoralReefs03.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Jrooney2000.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: annual_wavedriven.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 1 column 481 (char 480)\n",
      "\u2705 Layer 'passive_marine_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for groundwater_inundation\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'groundwater_inundation']\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 57 column 7 (char 2990)\n",
      "\u2705 Processed: RooneyCoastalSed2003.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: Cooper_etal_2013.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: OCCL23-Sea-Level-Rise-Report-FY22-1.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: anderson_et_al_GRL_2009.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Romine et al 2016 Beach Erosion Under Rising Sea Level.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 17 column 5 (char 1658)\n",
      "\u2705 Processed: RichmondHCH2001.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: BeachManagementPlan_1992_scanned.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 55 column 7 (char 2727)\n",
      "\u2705 Processed: wave_driven_cross_shore.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Genz_06-0757.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Romine Fletcher 2013 Oahu Armoring.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u2705 Processed: Maui Shoreline Rules Chapter 203 - Dr. Chip Fletcher Testimony.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: i2761.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Romine_coas-25-04-17.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: anderson_et_al_auxiliary_materials.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: ARCC2023Proceedings.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: ofr2011-1051_report_508.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Habel_Waikiki_replen_Coastal_eng_2016.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'future_erosion_hazard_zone']\n",
      "\u2705 Processed: KaneEtAl2014_RankedManagementConcerns.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Layer 'emergent_and_shallow_groundwater' validated (confidence: 0.25, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "\u2705 Layer 'passive_marine_flooding' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'groundwater_inundation']\n",
      "\u2705 Processed: Habel_2019_Environ._Res._Commun._1_041005.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for annual_high_wave_flooding\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for passive_marine_flooding\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['annual_high_wave_flooding', 'passive_marine_flooding']\n",
      "\u2705 Processed: 1-s2.0-S2213305421000163-main.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: SLR_Constraint_District_Ordinance.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: Paoa_et_al-2023-Scientific_Reports.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: AmSamoa Climate 2016.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 8 column 3 (char 477)\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: s10584-018-2327-7.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 33 column 7 (char 1708)\n",
      "\u2705 Processed: Kurylyketal.2025NCities.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: CoralReefsEngels.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 1 column 678 (char 677)\n",
      "\u2705 Processed: drainage_backflow.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: remotesensing-14-05108.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 57 column 7 (char 2862)\n",
      "\u2705 Layer 'passive_marine_flooding' validated (confidence: 0.33, keywords: 2)\n",
      "\u2705 Layer 'low_lying_flooding' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['passive_marine_flooding', 'low_lying_flooding']\n",
      "\u2705 Processed: s41597-024-03160-z.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: passive_marine_low_lying.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'compound_flooding' validated (confidence: 0.5, keywords: 2)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['compound_flooding']\n",
      "\u2705 Processed: compound_flooding.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.8, keywords: 4)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: s41598-020-70577-y.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.4, keywords: 2)\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for passive_marine_flooding\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone', 'passive_marine_flooding']\n",
      "\u2705 Processed: Hawaii_Natural_Resources_Law_Enforcement_Manual.10.17.2022.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: FletcherEtAl1990.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: 43UHawLRev464.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: Cochrane_etal2015.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.2, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for future_erosion_hazard_zone\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for annual_high_wave_flooding\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone', 'annual_high_wave_flooding']\n",
      "\u2705 Processed: Kaneetal2012.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: Use surplus to protect Sunset Beach.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 58 column 7 (char 3293)\n",
      "\u2705 Processed: Romine_SCD08.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'future_erosion_hazard_zone' validated (confidence: 0.6, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['future_erosion_hazard_zone']\n",
      "\u2705 Processed: Romine et al 2013 Beach Erosion and SLR in HI.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Layer 'drainage_backflow' validated (confidence: 0.25, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['groundwater_inundation', 'drainage_backflow']\n",
      "\u2705 Processed: Lander_et al_Envisioning_In-Situ_Sea_Level_Rise_Adaptation_for_Coastal_Cities.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: Guam Climate 2016.md\n",
      "\ud83d\udce1 Sending request to Gemini 2.5 Flash for paper analysis...\n",
      "\u2705 Processed: fletcher_2024_pnas_nexus.md\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 49 column 7 (char 2953)\n",
      "\u2705 Processed: Andrade et al 2023.md\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 53 column 7 (char 2472)\n",
      "\u2705 Processed: Bochicchio_etal_2009.md\n",
      "\u2705 Processed: Grossman_Fletcher_JSR_2003.md\n",
      "\u2705 Processed: s10584-023-03602-4.md\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for compound_flooding\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.5, keywords: 3)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['compound_flooding', 'groundwater_inundation']\n",
      "\u2705 Processed: Habel_et_al_flood_comparison.md\n",
      "\u2705 Analysis complete! Confidence: LOW\n",
      "   Relevant: False, Layers: []\n",
      "\u2705 Processed: FletcherEtAl1993_SLRAccelerationandDrowningofDelawareBayCoast18k.md\n",
      "\u26a0\ufe0f Layer Validation Warning: No supporting keywords found for compound_flooding\n",
      "\u2705 Layer 'groundwater_inundation' validated (confidence: 0.17, keywords: 1)\n",
      "\u2705 Analysis complete! Confidence: HIGH\n",
      "   Relevant: True, Layers: ['compound_flooding', 'groundwater_inundation']\n",
      "\u26a0\ufe0f Warning: API returned None\n",
      "\u2705 Processed: 2024_Illustrating_Urban_Plans_Meguro_et_al.md\n",
      "\u2705 Processed: HawaiiReef_07.md\n",
      "\u274c JSON Parsing Error: Unterminated string starting at: line 55 column 7 (char 3117)\n",
      "\u2705 Processed: KailuaBay.md\n",
      "------------------------------------------------------------\n",
      "\ud83d\udcbe Writing all results to outputs/combined_analysis_results.json...\n",
      "------------------------------------------------------------\n",
      "\ud83c\udf89 Completed in 185.66 seconds\n",
      "\ud83d\udcc8 Stats: 117 processed, 0 failed\n",
      "\ud83d\udcc4 All results saved to: outputs/combined_analysis_results.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from threading import Semaphore\n",
    "import json\n",
    "\n",
    "input_file_path = Path('outputs/cleaned_full_text_v2/')\n",
    "input_files = list(input_file_path.glob(\"*.md\"))\n",
    "output_file_path = Path('outputs/')\n",
    "output_file = output_file_path / 'combined_analysis_results.json'\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "output_file_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "MAX_WORKERS = 10  # Number of parallel requests (adjust based on your API quota)\n",
    "RATE_LIMIT_DELAY = 1  # Delay between requests in seconds\n",
    "\n",
    "# Semaphore to control rate limiting\n",
    "rate_limiter = Semaphore(MAX_WORKERS)\n",
    "\n",
    "# Dictionary to store all results\n",
    "all_results = {}\n",
    "\n",
    "def process_file(file_path: Path) -> tuple[str, bool, str, dict]:\n",
    "    \"\"\"\n",
    "    Process a single markdown file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (filename, success, message, result_dict)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read input file\n",
    "        with open(file_path, \"r\") as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Rate limiting\n",
    "        with rate_limiter:\n",
    "            analysis_result = analyze_paper(content)\n",
    "            time.sleep(RATE_LIMIT_DELAY)  # Prevent overwhelming API\n",
    "        \n",
    "        # Store result with filename as key\n",
    "        return (file_path.name, True, \"\u2705 Processed\", analysis_result)\n",
    "    \n",
    "    except Exception as e:\n",
    "        error_result = {\n",
    "            \"status\": \"error\",\n",
    "            \"error\": str(e),\n",
    "            \"relevant\": False,\n",
    "            \"confidence\": \"LOW\"\n",
    "        }\n",
    "        return (file_path.name, False, f\"\u274c Error: {str(e)}\", error_result)\n",
    "\n",
    "# Process files in parallel\n",
    "print(f\"\ud83d\udcca Total files: {len(input_files)}\")\n",
    "print(f\"\ud83d\udd27 Max workers: {MAX_WORKERS}, Rate limit: {RATE_LIMIT_DELAY}s\")\n",
    "print(f\"\ud83d\udcdd Output will be written to: {output_file}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "processed_count = 0\n",
    "failed_count = 0\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_file = {executor.submit(process_file, file): file for file in input_files}\n",
    "    \n",
    "    # Process completed tasks as they finish\n",
    "    for future in as_completed(future_to_file):\n",
    "        filename, success, message, result = future.result()\n",
    "        print(f\"{message}: {filename}\")\n",
    "        \n",
    "        # Store result in dictionary\n",
    "        all_results[filename] = result\n",
    "        \n",
    "        if success:\n",
    "            processed_count += 1\n",
    "        else:\n",
    "            failed_count += 1\n",
    "\n",
    "# Write all results to a single JSON file\n",
    "print(\"-\" * 60)\n",
    "print(f\"\ud83d\udcbe Writing all results to {output_file}...\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"\ud83c\udf89 Completed in {elapsed_time:.2f} seconds\")\n",
    "print(f\"\ud83d\udcc8 Stats: {processed_count} processed, {failed_count} failed\")\n",
    "print(f\"\ud83d\udcc4 All results saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e92f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the analysis results json\n",
    "analysis_results = json.load(open('outputs/combined_analysis_results.json'))\n",
    "cleaned_analysis_results = {}\n",
    "\n",
    "for key, value in analysis_results.items():\n",
    "    if value['relevant']:\n",
    "        cleaned_analysis_results[key] = value\n",
    "    \n",
    "    for layer in value['relevant_layers']:\n",
    "        if not value['layer_validation'][layer]['valid']:\n",
    "            cleaned_analysis_results[key]['relevant_layers'].remove(layer)\n",
    "\n",
    "with open('outputs/cleaned_analysis_results.json', 'w') as f:\n",
    "    json.dump(cleaned_analysis_results, f, indent=2, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6f8d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "t9hdkg9knom",
   "metadata": {},
   "source": [
    "# LlamaIndex Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4me78u9eznr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required packages are now in pyproject.toml:\n",
    "# - llama-index-core\n",
    "# - llama-index-embeddings-openai\n",
    "# \n",
    "# If you need to sync dependencies, run: uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2axoznjmi7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Successfully imported OpenAIEmbedding\n",
      "\ud83d\udcca Loading metadata...\n",
      "   Found metadata for 49 documents\n",
      "\n",
      "\ud83d\udcda Creating Document objects...\n",
      "\u2705 Loaded Vitousek_SCD08.md with 13541 characters\n",
      "\u23ed\ufe0f Skipping Spirandellietal2016_ImprovingAdaptationPlanningforSLR.md - not in metadata (not relevant)\n",
      "\u2705 Loaded JCOASTRES-D-11-00114.md with 26926 characters\n",
      "\u2705 Loaded ClimateBrief_low.md with 14313 characters\n",
      "\u2705 Loaded computation_of_energetic_nearshore_waves.md with 44927 characters\n",
      "\u2705 Loaded CS2003_Norcross_LongshoreTransport.md with 20790 characters\n",
      "\u23ed\ufe0f Skipping Rubin_Fletcher_Sherman2001.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping GeologyofHawaiiReefs.md - not in metadata (not relevant)\n",
      "\u2705 Loaded d41586-024-00917-9.md with 5655 characters\n",
      "\u23ed\ufe0f Skipping Conger_marinegeo_2009.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Bochicchio_Marine_Geo09.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping CNMI Climate 2016.md - not in metadata (not relevant)\n",
      "\u2705 Loaded HabelEtal_WR_2017.md with 44938 characters\n",
      "\u2705 Loaded Cooper_etal_2013_2.md with 51085 characters\n",
      "\u23ed\ufe0f Skipping ClimateChange_in_FSM.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Harney_Fletcher_JSR_2003.md with 45295 characters\n",
      "\u2705 Loaded FletcherFiersten_Hawaiichaptercoasts.md with 20415 characters\n",
      "\u23ed\ufe0f Skipping GeologyGeomorphology_NWHI_Coral_Reefs2008.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Anderson_etal_2015_JCR.md with 41711 characters\n",
      "\u23ed\ufe0f Skipping Rotzoll Fletcher NCC 2012.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping MappingShorelineCh106-124.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping KCAP_ClimateWP_22_0302.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Anderson_et_al_2015_NaturalHazards.md with 46960 characters\n",
      "\u23ed\ufe0f Skipping fletcher2009_sealevelreview.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Vitouseketal_NatureSR2017.md with 25743 characters\n",
      "\u23ed\ufe0f Skipping Fletcher-Chapter7-climate-change.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Vitousek_PSC08.md with 26411 characters\n",
      "\u23ed\ufe0f Skipping HarneyCoralReefs2000.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Andrade_et_al_2023-coas-40-02-338-352.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Kane_et_al_2015_ClimateChange.md with 26152 characters\n",
      "\u23ed\ufe0f Skipping CoastalSedimentary.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Norcross_SCD08.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping EngelsJSR04.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Act-238_HSEO_Decarbonization_Report.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Neil_Tiffany_Chip_2009.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping ClimateChange_in_FSM_Exec_Summary.md - not in metadata (not relevant)\n",
      "\u2705 Loaded 1-s2.0-S002532272200041X-main.md with 58359 characters\n",
      "\u23ed\ufe0f Skipping 230131_Final Booklet.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Coyne-MappingCoastalErosion-1999.md - not in metadata (not relevant)\n",
      "\u2705 Loaded annurev-marine-020923-120737.md with 44965 characters\n",
      "\u23ed\ufe0f Skipping remotesensing-12-00154.md - not in metadata (not relevant)\n",
      "\u2705 Loaded KaneEtAl2014_SLRCriticalElevation.md with 26577 characters\n",
      "\u2705 Loaded sherman_JSR_1999.md with 11683 characters\n",
      "\u23ed\ufe0f Skipping VitouseketalProceeding07.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping ClimateChangeFSM.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Fletcher_KaPili_Kai_09.md - not in metadata (not relevant)\n",
      "\u2705 Loaded 230125_Final Booklet.md with 12159 characters\n",
      "\u23ed\ufe0f Skipping coastal_land_subsidence.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Kane2017_QuaternaryResearch.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Romine_Fletcher_inpress_HI_ShoreChange_Summary_JCR.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Conger_TGARS.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Anderson_et_al_SciRep_2018_SLR_modeling.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Genz_06-0756.md with 52742 characters\n",
      "\u2705 Loaded WaikikiUAS_Defense_OnlineVersion.md with 6110 characters\n",
      "\u23ed\ufe0f Skipping groundwater_inundation.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Earth s Future - 2020 - Kane - Rethinking Reef Island Stability in Relation to Anthropogenic Sea Level Rise.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Sherman et al_QuatRes_v81_p138-150.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Cooper_etal_2012.md with 38186 characters\n",
      "\u23ed\ufe0f Skipping Anderson_Frazer_JCR_preprint.md - not in metadata (not relevant)\n",
      "\u2705 Loaded IsounCoralReefs03.md with 37017 characters\n",
      "\u23ed\ufe0f Skipping Fletcher-Chapter6-slr-hawaii.md - not in metadata (not relevant)\n",
      "\u2705 Loaded GenzetalProceeding.md with 21758 characters\n",
      "\u23ed\ufe0f Skipping RooneyCoastalSed2003.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping OCCL23-Sea-Level-Rise-Report-FY22-1.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Jrooney2000.md with 33056 characters\n",
      "\u2705 Loaded Anderson_etal_2014_JCR.md with 45076 characters\n",
      "\u23ed\ufe0f Skipping RichmondHCH2001.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Cooper_etal_2013.md with 11941 characters\n",
      "\u23ed\ufe0f Skipping i2761.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Romine Fletcher 2013 Oahu Armoring.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping anderson_et_al_GRL_2009.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Romine et al 2016 Beach Erosion Under Rising Sea Level.md with 27075 characters\n",
      "\u23ed\ufe0f Skipping annual_wavedriven.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Romine_coas-25-04-17.md with 56031 characters\n",
      "\u2705 Loaded Maui Shoreline Rules Chapter 203 - Dr. Chip Fletcher Testimony.md with 8171 characters\n",
      "\u2705 Loaded Genz_06-0757.md with 41544 characters\n",
      "\u2705 Loaded wave_driven_cross_shore.md with 61754 characters\n",
      "\u2705 Loaded ofr2011-1051_report_508.md with 74165 characters\n",
      "\u2705 Loaded Habel_Waikiki_replen_Coastal_eng_2016.md with 34397 characters\n",
      "\u23ed\ufe0f Skipping BeachManagementPlan_1992_scanned.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping anderson_et_al_auxiliary_materials.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping ARCC2023Proceedings.md - not in metadata (not relevant)\n",
      "\u2705 Loaded KaneEtAl2014_RankedManagementConcerns.md with 25851 characters\n",
      "\u2705 Loaded SLR_Constraint_District_Ordinance.md with 30298 characters\n",
      "\u23ed\ufe0f Skipping s10584-018-2327-7.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Habel_2019_Environ._Res._Commun._1_041005.md with 29851 characters\n",
      "\u23ed\ufe0f Skipping CoralReefsEngels.md - not in metadata (not relevant)\n",
      "\u2705 Loaded 1-s2.0-S2213305421000163-main.md with 26302 characters\n",
      "\u23ed\ufe0f Skipping AmSamoa Climate 2016.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping remotesensing-14-05108.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Kurylyketal.2025NCities.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping s41597-024-03160-z.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping drainage_backflow.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Paoa_et_al-2023-Scientific_Reports.md - not in metadata (not relevant)\n",
      "\u2705 Loaded 43UHawLRev464.md with 93858 characters\n",
      "\u23ed\ufe0f Skipping Hawaii_Natural_Resources_Law_Enforcement_Manual.10.17.2022.md - not in metadata (not relevant)\n",
      "\u2705 Loaded s41598-020-70577-y.md with 28116 characters\n",
      "\u2705 Loaded passive_marine_low_lying.md with 3452 characters\n",
      "\u2705 Loaded compound_flooding.md with 2362 characters\n",
      "\u23ed\ufe0f Skipping FletcherEtAl1990.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Kaneetal2012.md with 26875 characters\n",
      "\u23ed\ufe0f Skipping Romine_SCD08.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Lander_et al_Envisioning_In-Situ_Sea_Level_Rise_Adaptation_for_Coastal_Cities.md with 30527 characters\n",
      "\u23ed\ufe0f Skipping Cochrane_etal2015.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Use surplus to protect Sunset Beach.md with 4514 characters\n",
      "\u2705 Loaded Romine et al 2013 Beach Erosion and SLR in HI.md with 33301 characters\n",
      "\u23ed\ufe0f Skipping Bochicchio_etal_2009.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Grossman_Fletcher_JSR_2003.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Guam Climate 2016.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping s10584-023-03602-4.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping fletcher_2024_pnas_nexus.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping HawaiiReef_07.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping Andrade et al 2023.md - not in metadata (not relevant)\n",
      "\u2705 Loaded Habel_et_al_flood_comparison.md with 40004 characters\n",
      "\u2705 Loaded 2024_Illustrating_Urban_Plans_Meguro_et_al.md with 44132 characters\n",
      "\u23ed\ufe0f Skipping FletcherEtAl1993_SLRAccelerationandDrowningofDelawareBayCoast18k.md - not in metadata (not relevant)\n",
      "\u23ed\ufe0f Skipping KailuaBay.md - not in metadata (not relevant)\n",
      "   Created 49 documents\n",
      "\n",
      "\u2702\ufe0f Chunking documents semantically...\n",
      "\ud83d\udd27 Initializing SemanticSplitterNodeParser...\n",
      "   - Embedding model: text-embedding-3-small\n",
      "   - Buffer size: 1\n",
      "   - Breakpoint threshold: 95\n",
      "\u2705 Configured OpenAI embedding model\n",
      "\ud83d\udcc4 Processing 49 documents...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88e0ae315b54f29beaae7f3d21527b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/49 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e329a7043e9346238ff3626db30644ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "136e6b21bf5248cb9a6758da44a5cfb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/184 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0007a879c6b94b04a37eb0c354a45ee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28793a6fe33d4345a648e09a8078158d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/303 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fee23b3c5a1a44febcf63396572bafcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/118 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c491d9ccb734685b81b94b3041a8c2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff94e84292914ed799553f4237f29d63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/279 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f13d6dbf2848059146589076b8295e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/353 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f617171973b4339a81c1c02e7a91633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/257 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e5aedd58a5466a88d60938dc2b7e82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/134 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b0e34bb8204477a324274d1c00314d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/253 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0075773112b459b9be2bd31abb5cff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/272 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac6aee321464897b63c52b664aafa64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/136 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b0c4a4fe79249e18e59b8b64d610739",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b976c0912f4a1ebe183859fca0d831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9a03655a1c54c578fe226b147a867d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/394 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb41ae0a325b42feb7c88fc8fe46453f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/255 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3ae8aa6077477e9e33dc4e2db2abd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f19cf3bba3491ba490fbd7a75cbf71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/87 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efc31822ce240a382d1bab832ae5b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/68 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "883fd5ab2fd1419f84ffadbd9f6baafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/352 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b0bf33b9e864297af3cee00901eb840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c457b89f42a04e2f8a8a969ef753167b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/229 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a11314332c34c2b8516d20057b25b90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/266 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cbc719bb0484fd6ac3b89c1314c5cee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/185 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40822a1abb1246949aaf9133c18b6118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/243 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ce63b9152c40e29f9f7077fc912f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/286 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36a27f04abcd4ef8ab8a8d0c0816396e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/77 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f4fd963f12453cb93870ef6679f257",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/147 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdb069dc305476d9781418bcdb9c55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/423 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "382c93cd543c488293345eaca399da67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b6720d46357445c92f481508e3e5669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fc760befcc4ccbbd653c7158f08880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/371 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9c4b3fc8c84ee89583f38a4765911a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/493 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e09694cbbca4cfebeb1339c3c035e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07d8a0a5e1a34d4e820d9e31cd2e3a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f966952b9b4ad9a805c312e69fdc1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/211 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "316cd32db30840049ad0f8107452a4f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/173 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b3cabde5504a789eeabe7147fa65c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/161 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dae81a3b6b43ba848a8b9a5ef1e61e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/491 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "194f1baa2be54c55bfbd3e6c777cd693",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6d2e48682ad41bd80e1feba3d2effd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df15dca0184244bfb02e2dbcc6cc13bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11570e0be14e450caa0736fb3cd90a10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/182 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28eb7591273e4adf85529f746b2990ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/181 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b84f1953b7947b1b291e53f094d6c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97389b5639ad4ee889eb85474f975f76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/238 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb92b7c50d4f4c94a4787f442eaa016b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/228 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0290c9230b4d447b8aeec38f7b124f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/231 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Created 568 semantic chunks\n",
      "\n",
      "\ud83d\udccb Sample chunk:\n",
      "   Chunk ID: 44cd5c1b-65d2-4b6c-9ffd-9c51370814ed\n",
      "   Source: Vitousek_SCD08.md\n",
      "   Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "   Text preview: ## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "This paper outlines a practical approach to mapping extreme wave inundation and the inf...\n",
      "\n",
      "\ud83d\udcbe Saving chunks to JSON...\n",
      "\ud83d\udcbe Saved 568 chunks to outputs/semantic_chunks.json\n",
      "\n",
      "\u2705 Done!\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from llama_index.core import Document, Settings\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Verify OpenAI API key is set\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "# Import and configure OpenAI embedding\n",
    "try:\n",
    "    from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "    print(\"\u2705 Successfully imported OpenAIEmbedding\")\n",
    "except ImportError as e:\n",
    "    print(f\"\u274c Import error: {e}\")\n",
    "    print(\"Please run: pip install llama-index-embeddings-openai\")\n",
    "    raise\n",
    "\n",
    "def load_metadata(metadata_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the cleaned analysis results metadata.\n",
    "    \n",
    "    Args:\n",
    "        metadata_path: Path to the cleaned_analysis_results.json file\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary mapping filenames to their metadata\n",
    "    \"\"\"\n",
    "    with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_documents_from_markdown(\n",
    "    markdown_dir: str,\n",
    "    metadata_dict: Dict[str, Any]\n",
    ") -> List[Document]:\n",
    "    \"\"\"\n",
    "    Create LlamaIndex Document objects from markdown files with attached metadata.\n",
    "    \n",
    "    Args:\n",
    "        markdown_dir: Directory containing cleaned markdown files\n",
    "        metadata_dict: Dictionary of metadata from cleaned_analysis_results.json\n",
    "        \n",
    "    Returns:\n",
    "        List of Document objects with metadata\n",
    "    \"\"\"\n",
    "    documents = []\n",
    "    markdown_path = Path(markdown_dir)\n",
    "    \n",
    "    for md_file in markdown_path.glob(\"*.md\"):\n",
    "        filename = md_file.name\n",
    "        \n",
    "        # Skip files not in metadata (not relevant)\n",
    "        if filename not in metadata_dict:\n",
    "            print(f\"\u23ed\ufe0f Skipping {filename} - not in metadata (not relevant)\")\n",
    "            continue\n",
    "        \n",
    "        # Read the markdown content\n",
    "        with open(md_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        # Get metadata for this file\n",
    "        file_metadata = metadata_dict[filename]\n",
    "        \n",
    "        # Create metadata dict for the document\n",
    "        # LlamaIndex will attach this to all chunks from this document\n",
    "        doc_metadata = {\n",
    "            \"filename\": filename,\n",
    "            \"source_file\": str(md_file),\n",
    "            \"relevant\": file_metadata.get(\"relevant\", False),\n",
    "            \"confidence\": file_metadata.get(\"confidence\", \"UNKNOWN\"),\n",
    "            \"relevant_layers\": file_metadata.get(\"relevant_layers\", []),\n",
    "            \"reasoning\": file_metadata.get(\"reasoning\", \"\"),\n",
    "            \"key_findings\": file_metadata.get(\"key_findings\", []),\n",
    "            \"locations\": file_metadata.get(\"quantitative_data\", {}).get(\"locations\", []),\n",
    "            \"slr_projections\": file_metadata.get(\"quantitative_data\", {}).get(\"slr_projections\", []),\n",
    "            \"measurements\": file_metadata.get(\"quantitative_data\", {}).get(\"measurements\", []),\n",
    "            \"timeframes\": file_metadata.get(\"quantitative_data\", {}).get(\"timeframes\", []),\n",
    "        }\n",
    "        \n",
    "        # Create Document object\n",
    "        doc = Document(\n",
    "            text=content,\n",
    "            metadata=doc_metadata,\n",
    "            id_=filename  # Use filename as document ID\n",
    "        )\n",
    "        \n",
    "        documents.append(doc)\n",
    "        print(f\"\u2705 Loaded {filename} with {len(content)} characters\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "def chunk_documents_semantic(\n",
    "    documents: List[Document],\n",
    "    buffer_size: int = 1,\n",
    "    breakpoint_percentile_threshold: int = 95,\n",
    "    embed_model_name: str = \"text-embedding-3-small\"\n",
    ") -> List[Any]:\n",
    "    \"\"\"\n",
    "    Chunk documents using LlamaIndex's SemanticSplitterNodeParser.\n",
    "    \n",
    "    Args:\n",
    "        documents: List of Document objects to chunk\n",
    "        buffer_size: Number of sentences to group together for embedding comparison\n",
    "        breakpoint_percentile_threshold: Percentile of cosine dissimilarity to use as breakpoint\n",
    "        embed_model_name: OpenAI embedding model to use\n",
    "        \n",
    "    Returns:\n",
    "        List of Node objects (chunks) with metadata\n",
    "    \"\"\"\n",
    "    print(f\"\ud83d\udd27 Initializing SemanticSplitterNodeParser...\")\n",
    "    print(f\"   - Embedding model: {embed_model_name}\")\n",
    "    print(f\"   - Buffer size: {buffer_size}\")\n",
    "    print(f\"   - Breakpoint threshold: {breakpoint_percentile_threshold}\")\n",
    "    \n",
    "    # Initialize the OpenAI embedding model\n",
    "    embed_model = OpenAIEmbedding(\n",
    "        model=embed_model_name,\n",
    "        api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    "    )\n",
    "    \n",
    "    # Set global embedding model to avoid HuggingFace default\n",
    "    Settings.embed_model = embed_model\n",
    "    \n",
    "    print(f\"\u2705 Configured OpenAI embedding model\")\n",
    "    \n",
    "    # Initialize the semantic splitter\n",
    "    splitter = SemanticSplitterNodeParser(\n",
    "        buffer_size=buffer_size,\n",
    "        breakpoint_percentile_threshold=breakpoint_percentile_threshold,\n",
    "        embed_model=embed_model\n",
    "    )\n",
    "    \n",
    "    print(f\"\ud83d\udcc4 Processing {len(documents)} documents...\")\n",
    "    \n",
    "    # Split documents into nodes (chunks)\n",
    "    nodes = splitter.get_nodes_from_documents(documents, show_progress=True)\n",
    "    \n",
    "    print(f\"\u2705 Created {len(nodes)} semantic chunks\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "def save_chunks_to_json(nodes: List[Any], output_path: str):\n",
    "    \"\"\"\n",
    "    Save the chunks and their metadata to a JSON file.\n",
    "    \n",
    "    Args:\n",
    "        nodes: List of Node objects\n",
    "        output_path: Path to save the JSON file\n",
    "    \"\"\"\n",
    "    chunks_data = []\n",
    "    \n",
    "    for i, node in enumerate(nodes):\n",
    "        chunk_dict = {\n",
    "            \"chunk_id\": node.node_id,\n",
    "            \"text\": node.get_content(),\n",
    "            \"metadata\": node.metadata,\n",
    "            \"chunk_index\": i\n",
    "        }\n",
    "        chunks_data.append(chunk_dict)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunks_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\ud83d\udcbe Saved {len(chunks_data)} chunks to {output_path}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Paths\n",
    "    MARKDOWN_DIR = \"outputs/cleaned_full_text_v2/\"\n",
    "    METADATA_PATH = \"outputs/cleaned_analysis_results.json\"\n",
    "    OUTPUT_PATH = \"outputs/semantic_chunks.json\"\n",
    "    \n",
    "    # Load metadata\n",
    "    print(\"\ud83d\udcca Loading metadata...\")\n",
    "    metadata = load_metadata(METADATA_PATH)\n",
    "    print(f\"   Found metadata for {len(metadata)} documents\")\n",
    "    \n",
    "    # Create documents\n",
    "    print(\"\\n\ud83d\udcda Creating Document objects...\")\n",
    "    documents = create_documents_from_markdown(MARKDOWN_DIR, metadata)\n",
    "    print(f\"   Created {len(documents)} documents\")\n",
    "    \n",
    "    # Chunk documents\n",
    "    print(\"\\n\u2702\ufe0f Chunking documents semantically...\")\n",
    "    chunks = chunk_documents_semantic(\n",
    "        documents,\n",
    "        buffer_size=1,  # Group 1 sentence at a time (more granular)\n",
    "        breakpoint_percentile_threshold=95  # Use 95th percentile as breakpoint\n",
    "    )\n",
    "    \n",
    "    # Display sample chunk\n",
    "    if chunks:\n",
    "        print(\"\\n\ud83d\udccb Sample chunk:\")\n",
    "        print(f\"   Chunk ID: {chunks[0].node_id}\")\n",
    "        print(f\"   Source: {chunks[0].metadata.get('filename')}\")\n",
    "        print(f\"   Layers: {chunks[0].metadata.get('relevant_layers')}\")\n",
    "        print(f\"   Text preview: {chunks[0].get_content()[:200]}...\")\n",
    "    \n",
    "    # Save chunks\n",
    "    print(\"\\n\ud83d\udcbe Saving chunks to JSON...\")\n",
    "    save_chunks_to_json(chunks, OUTPUT_PATH)\n",
    "    \n",
    "    print(\"\\n\u2705 Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mhxyvm3kc9i",
   "metadata": {},
   "source": [
    "## How It Works\n",
    "\n",
    "This implementation uses **LlamaIndex's SemanticSplitterNodeParser** to intelligently chunk your documents based on semantic meaning rather than arbitrary character/token limits.\n",
    "\n",
    "### Key Features:\n",
    "\n",
    "1. **Semantic Chunking**: Uses embeddings to identify natural breakpoints in text where the topic changes\n",
    "2. **Metadata Preservation**: All metadata from `cleaned_analysis_results.json` is attached to every chunk\n",
    "3. **Configurable Parameters**:\n",
    "   - `buffer_size`: Number of sentences grouped together for comparison (default: 1)\n",
    "   - `breakpoint_percentile_threshold`: Percentile threshold for determining splits (default: 95)\n",
    "   - `embed_model_name`: OpenAI embedding model to use (default: \"text-embedding-3-small\")\n",
    "\n",
    "### Metadata Attached to Each Chunk:\n",
    "\n",
    "- `filename`: Source markdown file\n",
    "- `confidence`: HIGH/MEDIUM/LOW confidence from analysis\n",
    "- `relevant_layers`: List of relevant data layers (e.g., \"groundwater_inundation\")\n",
    "- `reasoning`: Why the paper is relevant\n",
    "- `key_findings`: Key findings from the paper\n",
    "- `locations`: Hawaiian locations mentioned\n",
    "- `slr_projections`: Sea level rise projections\n",
    "- `measurements`: Specific measurements\n",
    "- `timeframes`: Study periods or projection years\n",
    "\n",
    "### Adjusting Chunk Size:\n",
    "\n",
    "- **Smaller chunks**: Increase `breakpoint_percentile_threshold` (90-99)\n",
    "- **Larger chunks**: Decrease `breakpoint_percentile_threshold` (80-90)\n",
    "- **More context**: Increase `buffer_size` (2-5 sentences)\n",
    "- **More granular**: Decrease `buffer_size` (1 sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ura8dcivz1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Chunk Analysis\n",
      "   Total chunks: 568\n",
      "\n",
      "\ud83d\udccf Chunk Size Statistics:\n",
      "   Min size: 2 characters\n",
      "   Max size: 26661 characters\n",
      "   Average size: 2777 characters\n",
      "\n",
      "\ud83d\udcc4 Top 10 Documents by Chunk Count:\n",
      "   ofr2011-1051_report_508.md: 26 chunks\n",
      "   43UHawLRev464.md: 26 chunks\n",
      "   Romine_coas-25-04-17.md: 23 chunks\n",
      "   1-s2.0-S002532272200041X-main.md: 21 chunks\n",
      "   wave_driven_cross_shore.md: 20 chunks\n",
      "   Cooper_etal_2013_2.md: 19 chunks\n",
      "   Genz_06-0756.md: 19 chunks\n",
      "   Genz_06-0757.md: 19 chunks\n",
      "   computation_of_energetic_nearshore_waves.md: 17 chunks\n",
      "   Anderson_etal_2014_JCR.md: 16 chunks\n",
      "\n",
      "\ud83d\uddc2\ufe0f Chunks by Layer:\n",
      "   future_erosion_hazard_zone: 340 chunks\n",
      "   groundwater_inundation: 138 chunks\n",
      "   passive_marine_flooding: 67 chunks\n",
      "   annual_high_wave_flooding: 65 chunks\n",
      "   emergent_and_shallow_groundwater: 45 chunks\n",
      "   drainage_backflow: 15 chunks\n",
      "   low_lying_flooding: 12 chunks\n",
      "   compound_flooding: 2 chunks\n",
      "\n",
      "\u2b50 Chunks by Confidence:\n",
      "   HIGH: 549 chunks\n",
      "   MEDIUM: 19 chunks\n",
      "\n",
      "\ud83d\udd0d Example: Finding chunks about 'groundwater_inundation'\n",
      "   Found 138 chunks\n",
      "   Sample chunk from: d41586-024-00917-9.md\n",
      "   Preview: ## Correspondence\n",
      "\n",
      "## Opportunity in ' Anthropocene' rejection\n",
      "\n",
      "After a decade and a half of discussion, the International Commission on Stratigraphy (ICS) has rejected the proposed definition of an A...\n"
     ]
    }
   ],
   "source": [
    "# Example: Analyzing the chunks after creation\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_chunks(chunks_path: str = \"outputs/semantic_chunks.json\"):\n",
    "    \"\"\"\n",
    "    Analyze the generated chunks to understand the distribution.\n",
    "    \"\"\"\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"\ud83d\udcca Chunk Analysis\")\n",
    "    print(f\"   Total chunks: {len(chunks)}\")\n",
    "    print()\n",
    "    \n",
    "    # Analyze chunk sizes\n",
    "    chunk_sizes = [len(chunk['text']) for chunk in chunks]\n",
    "    print(f\"\ud83d\udccf Chunk Size Statistics:\")\n",
    "    print(f\"   Min size: {min(chunk_sizes)} characters\")\n",
    "    print(f\"   Max size: {max(chunk_sizes)} characters\")\n",
    "    print(f\"   Average size: {sum(chunk_sizes) / len(chunk_sizes):.0f} characters\")\n",
    "    print()\n",
    "    \n",
    "    # Count chunks by source document\n",
    "    source_counts = Counter([chunk['metadata']['filename'] for chunk in chunks])\n",
    "    print(f\"\ud83d\udcc4 Top 10 Documents by Chunk Count:\")\n",
    "    for filename, count in source_counts.most_common(10):\n",
    "        print(f\"   {filename}: {count} chunks\")\n",
    "    print()\n",
    "    \n",
    "    # Count chunks by layer\n",
    "    layer_counts = Counter()\n",
    "    for chunk in chunks:\n",
    "        for layer in chunk['metadata']['relevant_layers']:\n",
    "            layer_counts[layer] += 1\n",
    "    \n",
    "    print(f\"\ud83d\uddc2\ufe0f Chunks by Layer:\")\n",
    "    for layer, count in layer_counts.most_common():\n",
    "        print(f\"   {layer}: {count} chunks\")\n",
    "    print()\n",
    "    \n",
    "    # Count chunks by confidence\n",
    "    confidence_counts = Counter([chunk['metadata']['confidence'] for chunk in chunks])\n",
    "    print(f\"\u2b50 Chunks by Confidence:\")\n",
    "    for confidence, count in confidence_counts.most_common():\n",
    "        print(f\"   {confidence}: {count} chunks\")\n",
    "    print()\n",
    "    \n",
    "    # Example: Find chunks related to specific layers\n",
    "    print(f\"\ud83d\udd0d Example: Finding chunks about 'groundwater_inundation'\")\n",
    "    groundwater_chunks = [\n",
    "        chunk for chunk in chunks \n",
    "        if 'groundwater_inundation' in chunk['metadata']['relevant_layers']\n",
    "    ]\n",
    "    print(f\"   Found {len(groundwater_chunks)} chunks\")\n",
    "    if groundwater_chunks:\n",
    "        print(f\"   Sample chunk from: {groundwater_chunks[0]['metadata']['filename']}\")\n",
    "        print(f\"   Preview: {groundwater_chunks[0]['text'][:200]}...\")\n",
    "\n",
    "# Run the analysis (uncomment to use)\n",
    "analyze_chunks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82nirb8u13g",
   "metadata": {},
   "source": [
    "# PostgreSQL Vector Database Integration\n",
    "\n",
    "Upload semantic chunks with embeddings to PostgreSQL using pgvector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cqopaix2mnr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd17 Connecting to database...\n",
      "\ud83d\udce6 Enabling pgvector extension...\n",
      "\ud83d\udcca Creating tables...\n",
      "\u2705 Database initialized successfully!\n",
      "Database URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine, Column, Integer, String, Text, Boolean, JSON, ARRAY, Float, text\n",
    "from sqlalchemy.orm import declarative_base, sessionmaker\n",
    "from pgvector.sqlalchemy import Vector\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# Database configuration from environment\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\", \"postgresql://***:***@localhost:5432/climate_viewer\")\n",
    "\n",
    "# Create base class for declarative models\n",
    "Base = declarative_base()\n",
    "\n",
    "class DocumentChunk(Base):\n",
    "    \"\"\"\n",
    "    SQLAlchemy model for storing document chunks with embeddings.\n",
    "    \"\"\"\n",
    "    __tablename__ = \"document_chunks\"\n",
    "    \n",
    "    # Primary key\n",
    "    id = Column(Integer, primary_key=True, autoincrement=True)\n",
    "    \n",
    "    # Chunk identification\n",
    "    chunk_id = Column(String(255), unique=True, nullable=False, index=True)\n",
    "    chunk_index = Column(Integer, nullable=False)\n",
    "    \n",
    "    # Content\n",
    "    text = Column(Text, nullable=False)\n",
    "    \n",
    "    # Vector embedding (1536 dimensions for text-embedding-3-small)\n",
    "    embedding = Column(Vector(1536))\n",
    "    \n",
    "    # Metadata fields\n",
    "    filename = Column(String(255), nullable=False, index=True)\n",
    "    source_file = Column(String(512))\n",
    "    relevant = Column(Boolean, default=True)\n",
    "    confidence = Column(String(50), index=True)\n",
    "    \n",
    "    # Data layers as array\n",
    "    relevant_layers = Column(ARRAY(String), index=True)\n",
    "    \n",
    "    # Text fields for search\n",
    "    reasoning = Column(Text)\n",
    "    key_findings = Column(JSON)  # Store as JSON array\n",
    "    \n",
    "    # Quantitative data\n",
    "    locations = Column(ARRAY(String))\n",
    "    slr_projections = Column(ARRAY(String))\n",
    "    measurements = Column(ARRAY(String))\n",
    "    timeframes = Column(ARRAY(String))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"<DocumentChunk(id={self.id}, chunk_id='{self.chunk_id}', filename='{self.filename}')>\"\n",
    "\n",
    "def init_database(database_url: str = None):\n",
    "    \"\"\"\n",
    "    Initialize the database with pgvector extension and create tables.\n",
    "    \n",
    "    Args:\n",
    "        database_url: PostgreSQL connection string\n",
    "        \n",
    "    Returns:\n",
    "        SQLAlchemy engine\n",
    "    \"\"\"\n",
    "    if database_url is None:\n",
    "        database_url = DATABASE_URL\n",
    "    \n",
    "    print(f\"\ud83d\udd17 Connecting to database...\")\n",
    "    engine = create_engine(database_url, echo=False)\n",
    "    \n",
    "    # Enable pgvector extension\n",
    "    with engine.connect() as conn:\n",
    "        print(\"\ud83d\udce6 Enabling pgvector extension...\")\n",
    "        conn.execute(text(\"CREATE EXTENSION IF NOT EXISTS vector\"))        \n",
    "        conn.commit()\n",
    "    \n",
    "    # Create tables\n",
    "    print(\"\ud83d\udcca Creating tables...\")\n",
    "    Base.metadata.create_all(engine)\n",
    "    \n",
    "    print(\"\u2705 Database initialized successfully!\")\n",
    "    return engine\n",
    "\n",
    "# Test the connection (uncomment to run)\n",
    "engine = init_database()\n",
    "print(f\"Database URL: {DATABASE_URL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a62ox6wmkr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd17 Connecting to database...\n",
      "\ud83d\udce6 Enabling pgvector extension...\n",
      "\ud83d\udcca Creating tables...\n",
      "\u2705 Database initialized successfully!\n",
      "\ud83d\udcc2 Loading chunks from outputs/semantic_chunks.json...\n",
      "   Loaded 568 chunks\n",
      "\ud83d\udd22 Generating embeddings for 568 texts...\n",
      "   Model: text-embedding-3-small\n",
      "   Batch size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:12<00:00,  2.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 Generated 568 embeddings\n",
      "\n",
      "\ud83d\udcbe Uploading chunks to database...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  24%|\u2588\u2588\u258d       | 137/568 [00:00<00:01, 277.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 100 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  42%|\u2588\u2588\u2588\u2588\u258f     | 240/568 [00:00<00:01, 324.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 200 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  61%|\u2588\u2588\u2588\u2588\u2588\u2588    | 347/568 [00:01<00:00, 316.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 300 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading:  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589  | 451/568 [00:01<00:00, 332.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 400 chunks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 568/568 [00:01<00:00, 300.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Committed 500 chunks...\n",
      "\n",
      "\u2705 Upload complete!\n",
      "   Inserted: 568 chunks\n",
      "   Skipped: 0 chunks\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from typing import List, Dict, Any\n",
    "import json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def generate_embeddings(texts: List[str], model: str = \"text-embedding-3-small\", batch_size: int = 100) -> List[List[float]]:\n",
    "    \"\"\"\n",
    "    Generate embeddings for a list of texts using OpenAI API.\n",
    "    \n",
    "    Args:\n",
    "        texts: List of text strings to embed\n",
    "        model: OpenAI embedding model name\n",
    "        batch_size: Number of texts to process per API call\n",
    "        \n",
    "    Returns:\n",
    "        List of embedding vectors\n",
    "    \"\"\"\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    all_embeddings = []\n",
    "    \n",
    "    print(f\"\ud83d\udd22 Generating embeddings for {len(texts)} texts...\")\n",
    "    print(f\"   Model: {model}\")\n",
    "    print(f\"   Batch size: {batch_size}\")\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            response = client.embeddings.create(\n",
    "                model=model,\n",
    "                input=batch\n",
    "            )\n",
    "            \n",
    "            # Extract embeddings from response\n",
    "            batch_embeddings = [item.embedding for item in response.data]\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            \n",
    "            # Rate limiting\n",
    "            time.sleep(0.1)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\u274c Error generating embeddings for batch {i//batch_size}: {e}\")\n",
    "            # Add None placeholders for failed batch\n",
    "            all_embeddings.extend([None] * len(batch))\n",
    "    \n",
    "    print(f\"\u2705 Generated {len([e for e in all_embeddings if e is not None])} embeddings\")\n",
    "    return all_embeddings\n",
    "\n",
    "def upload_chunks_to_database(\n",
    "    chunks_path: str,\n",
    "    database_url: str = None,\n",
    "    batch_size: int = 100,\n",
    "    embedding_batch_size: int = 100\n",
    "):\n",
    "    \"\"\"\n",
    "    Load chunks from JSON, generate embeddings, and upload to PostgreSQL.\n",
    "    \n",
    "    Args:\n",
    "        chunks_path: Path to semantic_chunks.json\n",
    "        database_url: PostgreSQL connection string\n",
    "        batch_size: Number of chunks to insert per transaction\n",
    "        embedding_batch_size: Number of texts to embed per API call\n",
    "    \"\"\"\n",
    "    # Initialize database\n",
    "    engine = init_database(database_url)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    \n",
    "    # Load chunks from JSON\n",
    "    print(f\"\ud83d\udcc2 Loading chunks from {chunks_path}...\")\n",
    "    with open(chunks_path, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    \n",
    "    print(f\"   Loaded {len(chunks)} chunks\")\n",
    "    \n",
    "    # Extract texts for embedding\n",
    "    texts = [chunk['text'] for chunk in chunks]\n",
    "    \n",
    "    # Generate embeddings\n",
    "    embeddings = generate_embeddings(texts, batch_size=embedding_batch_size)\n",
    "    \n",
    "    # Upload to database\n",
    "    print(f\"\\n\ud83d\udcbe Uploading chunks to database...\")\n",
    "    session = Session()\n",
    "    \n",
    "    inserted_count = 0\n",
    "    skipped_count = 0\n",
    "    \n",
    "    try:\n",
    "        for i, (chunk, embedding) in enumerate(tqdm(zip(chunks, embeddings), total=len(chunks), desc=\"Uploading\")):\n",
    "            # Skip if embedding generation failed\n",
    "            if embedding is None:\n",
    "                print(f\"\u26a0\ufe0f Skipping chunk {i} - no embedding\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Check if chunk already exists\n",
    "            existing = session.query(DocumentChunk).filter_by(chunk_id=chunk['chunk_id']).first()\n",
    "            if existing:\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "            \n",
    "            # Create DocumentChunk object\n",
    "            doc_chunk = DocumentChunk(\n",
    "                chunk_id=chunk['chunk_id'],\n",
    "                chunk_index=chunk['chunk_index'],\n",
    "                text=chunk['text'],\n",
    "                embedding=embedding,\n",
    "                filename=chunk['metadata']['filename'],\n",
    "                source_file=chunk['metadata'].get('source_file'),\n",
    "                relevant=chunk['metadata'].get('relevant', True),\n",
    "                confidence=chunk['metadata'].get('confidence'),\n",
    "                relevant_layers=chunk['metadata'].get('relevant_layers', []),\n",
    "                reasoning=chunk['metadata'].get('reasoning'),\n",
    "                key_findings=chunk['metadata'].get('key_findings'),\n",
    "                locations=chunk['metadata'].get('locations', []),\n",
    "                slr_projections=chunk['metadata'].get('slr_projections', []),\n",
    "                measurements=chunk['metadata'].get('measurements', []),\n",
    "                timeframes=chunk['metadata'].get('timeframes', [])\n",
    "            )\n",
    "            \n",
    "            session.add(doc_chunk)\n",
    "            inserted_count += 1\n",
    "            \n",
    "            # Commit in batches\n",
    "            if inserted_count % batch_size == 0:\n",
    "                session.commit()\n",
    "                print(f\"   Committed {inserted_count} chunks...\")\n",
    "        \n",
    "        # Final commit\n",
    "        session.commit()\n",
    "        print(f\"\\n\u2705 Upload complete!\")\n",
    "        print(f\"   Inserted: {inserted_count} chunks\")\n",
    "        print(f\"   Skipped: {skipped_count} chunks\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        session.rollback()\n",
    "        print(f\"\u274c Error during upload: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "upload_chunks_to_database(\n",
    "    chunks_path=\"outputs/semantic_chunks.json\",\n",
    "    batch_size=100,\n",
    "    embedding_batch_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "moxxs78mmo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Similarity: 0.686\n",
      "   File: HabelEtal_WR_2017.md\n",
      "   Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "   Text: ### 1.1.2. Local sea-level rise\n",
      "\n",
      "In Honolulu, the semi-diurnal tide range is 0.58 m and the local rate of SLR is 1.41 \u00b1 0.21 mm/yr based on monthly mean sea-level measurements at the Honolulu tide sta...\n",
      "\n",
      "2. Similarity: 0.654\n",
      "   File: Habel_et_al_flood_comparison.md\n",
      "   Layers: ['groundwater_inundation']\n",
      "   Text: Here a method is developed that identifies flooding extents and infrastructure vulnerabilities that are likely to result from alternate flood sources over coming decades. The method includes simulatio...\n",
      "\n",
      "3. Similarity: 0.651\n",
      "   File: annurev-marine-020923-120737.md\n",
      "   Layers: ['groundwater_inundation', 'emergent_and_shallow_groundwater']\n",
      "   Text: ## 2. IMPACTS OF SEA-LEVEL-RISE-INFLUENCED COASTAL GROUNDWATER\n",
      "\n",
      "Municipalities worldwide host complex infrastructure networks that exist partially or entirely belowground. Components of this infrastru...\n",
      "\n",
      "4. Similarity: 0.631\n",
      "   File: Habel_et_al_flood_comparison.md\n",
      "   Layers: ['groundwater_inundation']\n",
      "   Text: Prevention of such flooding generally requires construction of water-tight continuous structures, like those implemented in New Orleans and the Netherlands.\n",
      "\n",
      "-   Storm-drain backflow is like direct ma...\n",
      "\n",
      "5. Similarity: 0.624\n",
      "   File: 230125_Final Booklet.md\n",
      "   Layers: ['groundwater_inundation', 'drainage_backflow']\n",
      "   Text: Identify buildings with at-grade or below-grade spaces that appear vulnerable to flooding based on our observation and reasoning.\n",
      "4.  Identify older buildings for retrofit, recognizing that they are m...\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "def vector_search(\n",
    "    query: str,\n",
    "    database_url: str = None,\n",
    "    top_k: int = 5,\n",
    "    filters: Dict[str, Any] = None\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Perform vector similarity search on the database.\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        database_url: PostgreSQL connection string\n",
    "        top_k: Number of results to return\n",
    "        filters: Optional filters (e.g., {\"confidence\": \"HIGH\", \"relevant_layers\": [\"groundwater_inundation\"]})\n",
    "        \n",
    "    Returns:\n",
    "        List of matching chunks with metadata\n",
    "    \"\"\"\n",
    "    if database_url is None:\n",
    "        database_url = DATABASE_URL\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "    response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=query\n",
    "    )\n",
    "    query_embedding = response.data[0].embedding\n",
    "    \n",
    "    # Create database session\n",
    "    engine = create_engine(database_url, echo=False)\n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    try:\n",
    "        # Build query\n",
    "        query_obj = session.query(\n",
    "            DocumentChunk,\n",
    "            DocumentChunk.embedding.cosine_distance(query_embedding).label('distance')\n",
    "        )\n",
    "        \n",
    "        # Apply filters\n",
    "        if filters:\n",
    "            if 'confidence' in filters:\n",
    "                query_obj = query_obj.filter(DocumentChunk.confidence == filters['confidence'])\n",
    "            if 'relevant_layers' in filters:\n",
    "                # Check if arrays have any overlapping elements using PostgreSQL && operator\n",
    "                # This checks if any element in relevant_layers array matches any element in filter array\n",
    "                from sqlalchemy import cast, ARRAY, String\n",
    "                filter_array = cast(filters['relevant_layers'], ARRAY(String))\n",
    "                query_obj = query_obj.filter(\n",
    "                    DocumentChunk.relevant_layers.op('&&')(filter_array)\n",
    "                )\n",
    "            if 'filename' in filters:\n",
    "                query_obj = query_obj.filter(DocumentChunk.filename == filters['filename'])\n",
    "        \n",
    "        # Order by similarity and limit\n",
    "        results = query_obj.order_by('distance').limit(top_k).all()\n",
    "        \n",
    "        # Format results\n",
    "        formatted_results = []\n",
    "        for chunk, distance in results:\n",
    "            formatted_results.append({\n",
    "                'chunk_id': chunk.chunk_id,\n",
    "                'text': chunk.text,\n",
    "                'distance': float(distance),\n",
    "                'similarity': 1 - float(distance),  # Convert distance to similarity\n",
    "                'metadata': {\n",
    "                    'filename': chunk.filename,\n",
    "                    'confidence': chunk.confidence,\n",
    "                    'relevant_layers': chunk.relevant_layers,\n",
    "                    'locations': chunk.locations,\n",
    "                    'slr_projections': chunk.slr_projections,\n",
    "                    'key_findings': chunk.key_findings\n",
    "                }\n",
    "            })\n",
    "        \n",
    "        return formatted_results\n",
    "        \n",
    "    finally:\n",
    "        session.close()\n",
    "\n",
    "# Example usage (uncomment to run)\n",
    "results = vector_search(\n",
    "    query=\"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    top_k=5,\n",
    "    filters={\"relevant_layers\": [\"groundwater_inundation\"]}\n",
    ")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    print(f\"\\n{i}. Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"   File: {result['metadata']['filename']}\")\n",
    "    print(f\"   Layers: {result['metadata']['relevant_layers']}\")\n",
    "    print(f\"   Text: {result['text'][:200]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443dkpjzk5y",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "\n",
    "### 1. Set up your environment variables\n",
    "\n",
    "Create a `.env` file in the notebooks directory with:\n",
    "\n",
    "```bash\n",
    "DATABASE_URL=postgresql://***:***@localhost:5432/climate_viewer\n",
    "OPENAI_API_KEY=your-openai-api-key\n",
    "```\n",
    "\n",
    "### 2. Initialize the database\n",
    "\n",
    "```python\n",
    "engine = init_database()\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Enable the `pgvector` extension\n",
    "- Create the `document_chunks` table with all necessary fields\n",
    "- Set up vector indexing\n",
    "\n",
    "### 3. Upload chunks to PostgreSQL\n",
    "\n",
    "```python\n",
    "upload_chunks_to_database(\n",
    "    chunks_path=\"outputs/semantic_chunks.json\",\n",
    "    batch_size=100,\n",
    "    embedding_batch_size=100\n",
    ")\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Load chunks from JSON\n",
    "- Generate embeddings using OpenAI API (text-embedding-3-small)\n",
    "- Upload chunks with embeddings to PostgreSQL\n",
    "- Skip duplicates automatically\n",
    "\n",
    "### 4. Search the database\n",
    "\n",
    "```python\n",
    "results = vector_search(\n",
    "    query=\"What are the impacts of sea level rise on groundwater?\",\n",
    "    top_k=5,\n",
    "    filters={\"relevant_layers\": [\"groundwater_inundation\"]}\n",
    ")\n",
    "\n",
    "for result in results:\n",
    "    print(f\"Similarity: {result['similarity']:.3f}\")\n",
    "    print(f\"File: {result['metadata']['filename']}\")\n",
    "    print(f\"Text: {result['text'][:200]}...\")\n",
    "```\n",
    "\n",
    "### Database Schema\n",
    "\n",
    "The `document_chunks` table includes:\n",
    "- **Vector embeddings** (1536 dimensions) for similarity search\n",
    "- **All metadata** from your analysis (layers, confidence, locations, etc.)\n",
    "- **Indexed fields** for fast filtering by filename, confidence, and layers\n",
    "- **Full-text content** for each chunk\n",
    "\n",
    "### Vector Search Features\n",
    "\n",
    "- **Cosine similarity** for finding semantically similar chunks\n",
    "- **Filter by layers** (e.g., only groundwater_inundation chunks)\n",
    "- **Filter by confidence** (HIGH, MEDIUM, LOW)\n",
    "- **Filter by filename** to search within specific papers\n",
    "- **Combine filters** for precise searches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fsux2h4mmcu",
   "metadata": {},
   "source": [
    "# RAG with LlamaIndex\n",
    "\n",
    "Use LlamaIndex for advanced RAG with the PostgreSQL vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0qcf04lu3rql",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u2705 LlamaIndex configured with:\n",
      "   LLM: gpt-4o-mini\n",
      "   Embeddings: text-embedding-3-small\n",
      "================================================================================\n",
      "\ud83d\udd27 DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "\ud83d\udccb Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "\ud83d\udccb Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "\ud83d\udccb Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "\ud83d\udccb Step 4: Creating PGVectorStore...\n",
      "   \u2705 PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "\ud83d\udccb Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   \u2705 Added 568 nodes to docstore\n",
      "   \u2705 Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "\ud83d\udccb Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 5\n",
      "   \u2705 Retriever created successfully\n",
      "\n",
      "\ud83d\udccb Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.3\n",
      "   \u2705 Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query engine ready!\n",
      "   Retrieval: top 5 chunks\n",
      "   Min similarity: 0.3\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, StorageContext\n",
    "from llama_index.vector_stores.postgres import PGVectorStore\n",
    "from llama_index.llms.openai import OpenAI as LlamaOpenAI\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.postprocessor import SimilarityPostprocessor\n",
    "from sqlalchemy.engine import make_url\n",
    "import os\n",
    "\n",
    "# Configure LlamaIndex global settings\n",
    "Settings.llm = LlamaOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.1,\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "Settings.embed_model = OpenAIEmbedding(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"\u2705 LlamaIndex configured with:\")\n",
    "print(f\"   LLM: {Settings.llm.model}\")\n",
    "print(f\"   Embeddings: text-embedding-3-small\")\n",
    "\n",
    "def create_llamaindex_query_engine(\n",
    "    database_url: str = None,\n",
    "    table_name: str = \"document_chunks\",\n",
    "    similarity_top_k: int = 5,\n",
    "    similarity_cutoff: float = 0.3,  # Lowered default from 0.7 to get more results\n",
    "    debug: bool = True  # Enable detailed debugging\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a LlamaIndex query engine connected to PostgreSQL vector store.\n",
    "    \n",
    "    Args:\n",
    "        database_url: PostgreSQL connection string (postgresql://***:***@host:port/db)\n",
    "        table_name: Name of the table with vectors\n",
    "        similarity_top_k: Number of chunks to retrieve\n",
    "        similarity_cutoff: Minimum similarity score (0-1)\n",
    "        debug: Enable detailed debugging output\n",
    "        \n",
    "    Returns:\n",
    "        LlamaIndex QueryEngine\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\ud83d\udd27 DEBUG: Creating LlamaIndex Query Engine\")\n",
    "        print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Get database URL\n",
    "    if database_url is None:\n",
    "        database_url = os.getenv(\"DATABASE_URL\")\n",
    "        if debug:\n",
    "            print(f\"\ud83d\udccb Step 1: Database URL from environment\")\n",
    "            print(f\"   URL: {database_url}\")\n",
    "    \n",
    "    if not database_url:\n",
    "        raise ValueError(\"DATABASE_URL not set. Please set it in your .env file.\")\n",
    "    \n",
    "    # Step 2: Parse connection string\n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 2: Parsing connection string...\")\n",
    "    url = make_url(database_url)\n",
    "    if debug:\n",
    "        print(f\"   Host: {url.host or 'localhost'}\")\n",
    "        print(f\"   Port: {url.port or 5432}\")\n",
    "        print(f\"   User: {url.username}\")\n",
    "        print(f\"   Database: {url.database}\")\n",
    "        print(f\"   Table: {table_name}\")\n",
    "    \n",
    "    # Step 3: Verify database connection and table\n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 3: Verifying database connection and table...\")\n",
    "        from sqlalchemy import create_engine, text\n",
    "        try:\n",
    "            test_engine = create_engine(database_url, echo=False)\n",
    "            with test_engine.connect() as conn:\n",
    "                # Check if table exists\n",
    "                result = conn.execute(text(f\"\"\"\n",
    "                    SELECT EXISTS (\n",
    "                        SELECT FROM information_schema.tables \n",
    "                        WHERE table_name = '{table_name}'\n",
    "                    )\n",
    "                \"\"\"))\n",
    "                table_exists = result.scalar()\n",
    "                print(f\"   Table exists: {table_exists}\")\n",
    "                \n",
    "                if table_exists:\n",
    "                    # Count rows\n",
    "                    result = conn.execute(text(f\"SELECT COUNT(*) FROM {table_name}\"))\n",
    "                    row_count = result.scalar()\n",
    "                    print(f\"   Total rows: {row_count}\")\n",
    "                    \n",
    "                    # Count rows with embeddings\n",
    "                    result = conn.execute(text(f\"\"\"\n",
    "                        SELECT COUNT(*) FROM {table_name} \n",
    "                        WHERE embedding IS NOT NULL\n",
    "                    \"\"\"))\n",
    "                    embedding_count = result.scalar()\n",
    "                    print(f\"   Rows with embeddings: {embedding_count}\")\n",
    "                    \n",
    "                    if embedding_count == 0:\n",
    "                        print(\"   \u26a0\ufe0f  WARNING: No embeddings found in table!\")\n",
    "        except Exception as e:\n",
    "            print(f\"   \u26a0\ufe0f  Warning: Could not verify database: {e}\")\n",
    "    \n",
    "    # Step 4: Create PGVectorStore\n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 4: Creating PGVectorStore...\")\n",
    "    try:\n",
    "        vector_store = PGVectorStore.from_params(\n",
    "            host=url.host or \"localhost\",\n",
    "            port=str(url.port) if url.port else \"5432\",\n",
    "            user=url.username,\n",
    "            password=url.password,\n",
    "            database=url.database,\n",
    "            table_name=table_name,\n",
    "            embed_dim=1536,  # text-embedding-3-small dimension\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"   \u2705 PGVectorStore created successfully\")\n",
    "            print(f\"   Embedding dimension: 1536\")\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error creating PGVectorStore: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Step 5: Create index from vector store and populate docstore\n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 5: Creating VectorStoreIndex from vector store...\")\n",
    "    try:\n",
    "        from llama_index.core import StorageContext\n",
    "        from llama_index.core.schema import TextNode\n",
    "        from llama_index.core import Document\n",
    "        \n",
    "        # Create storage context with vector store\n",
    "        storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "        \n",
    "        # Load nodes from database to populate docstore\n",
    "        if debug:\n",
    "            print(f\"   Loading nodes from database to populate docstore...\")\n",
    "        \n",
    "        # Query the database to get all chunks\n",
    "        from sqlalchemy import create_engine, text\n",
    "        db_engine = create_engine(database_url, echo=False)\n",
    "        nodes_to_add = []\n",
    "        \n",
    "        with db_engine.connect() as conn:\n",
    "            result = conn.execute(text(f\"\"\"\n",
    "                SELECT chunk_id, text, filename, confidence, relevant_layers, \n",
    "                       locations, slr_projections, measurements, timeframes,\n",
    "                       key_findings, reasoning, source_file, relevant\n",
    "                FROM {table_name}\n",
    "            \"\"\"))\n",
    "            \n",
    "            for row in result:\n",
    "                # Create metadata dict\n",
    "                metadata = {\n",
    "                    \"filename\": row[2] or \"\",\n",
    "                    \"source_file\": row[11] or \"\",\n",
    "                    \"relevant\": row[12] if row[12] is not None else True,\n",
    "                    \"confidence\": row[3] or \"\",\n",
    "                    \"relevant_layers\": row[4] or [],\n",
    "                    \"locations\": row[5] or [],\n",
    "                    \"slr_projections\": row[6] or [],\n",
    "                    \"measurements\": row[7] or [],\n",
    "                    \"timeframes\": row[8] or [],\n",
    "                }\n",
    "                \n",
    "                if row[9]:  # key_findings\n",
    "                    metadata[\"key_findings\"] = row[9]\n",
    "                if row[10]:  # reasoning\n",
    "                    metadata[\"reasoning\"] = row[10]\n",
    "                \n",
    "                # Create TextNode\n",
    "                node = TextNode(\n",
    "                    text=row[1] or \"\",\n",
    "                    id_=row[0],  # Use chunk_id as node ID\n",
    "                    metadata=metadata\n",
    "                )\n",
    "                nodes_to_add.append(node)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   Loaded {len(nodes_to_add)} nodes from database\")\n",
    "        \n",
    "        # Add nodes to docstore\n",
    "        if nodes_to_add:\n",
    "            if debug:\n",
    "                print(f\"   Attempting to add {len(nodes_to_add)} nodes to docstore...\")\n",
    "            \n",
    "            # Try batch add first (more efficient)\n",
    "            try:\n",
    "                if debug:\n",
    "                    print(f\"   Trying batch add_documents...\")\n",
    "                storage_context.docstore.add_documents(nodes_to_add, allow_update=True)\n",
    "                if debug:\n",
    "                    print(f\"   \u2705 Batch add_documents succeeded\")\n",
    "            except Exception as batch_error:\n",
    "                if debug:\n",
    "                    print(f\"   \u26a0\ufe0f  Batch add failed: {batch_error}\")\n",
    "                    print(f\"   Trying individual adds...\")\n",
    "                # Fallback to individual adds\n",
    "                success_count = 0\n",
    "                for i, node in enumerate(nodes_to_add):\n",
    "                    try:\n",
    "                        storage_context.docstore.add_documents([node], allow_update=True)\n",
    "                        success_count += 1\n",
    "                    except Exception as e:\n",
    "                        if debug and i < 3:  # Only show first 3 errors\n",
    "                            print(f\"   \u26a0\ufe0f  Could not add node {node.node_id}: {e}\")\n",
    "                if debug:\n",
    "                    print(f\"   \u2705 Added {success_count}/{len(nodes_to_add)} nodes individually\")\n",
    "            \n",
    "            # Verify nodes were added\n",
    "            if debug:\n",
    "                try:\n",
    "                    docstore = storage_context.docstore\n",
    "                    if hasattr(docstore, 'docs'):\n",
    "                        actual_count = len(docstore.docs)\n",
    "                        print(f\"   Verified: {actual_count} nodes in docstore\")\n",
    "                        if actual_count == 0:\n",
    "                            print(f\"   \u26a0\ufe0f  WARNING: add_documents didn't work! Docstore is still empty.\")\n",
    "                            print(f\"   Trying alternative method...\")\n",
    "                            # Try direct dictionary access as last resort\n",
    "                            if hasattr(docstore, 'docs'):\n",
    "                                for node in nodes_to_add[:10]:  # Try first 10 as test\n",
    "                                    docstore.docs[node.node_id] = node\n",
    "                                print(f\"   Test: Added 10 nodes via direct access\")\n",
    "                                print(f\"   Docstore now has: {len(docstore.docs)} nodes\")\n",
    "                except Exception as verify_error:\n",
    "                    if debug:\n",
    "                        print(f\"   Could not verify docstore: {verify_error}\")\n",
    "        \n",
    "        # Verify docstore before creating index\n",
    "        if debug:\n",
    "            try:\n",
    "                docstore = storage_context.docstore\n",
    "                if hasattr(docstore, 'docs'):\n",
    "                    print(f\"   Documents in docstore BEFORE index creation: {len(docstore.docs)}\")\n",
    "                elif hasattr(docstore, 'get_all_document_hashes'):\n",
    "                    all_hashes = docstore.get_all_document_hashes()\n",
    "                    print(f\"   Documents in docstore BEFORE index creation: {len(all_hashes)}\")\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"   Could not check docstore before index: {e}\")\n",
    "        \n",
    "        # Create index with populated storage context\n",
    "        # IMPORTANT: Pass storage_context to ensure docstore is preserved\n",
    "        index = VectorStoreIndex.from_vector_store(\n",
    "            vector_store=vector_store,\n",
    "            storage_context=storage_context\n",
    "        )\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   \u2705 Index created successfully\")\n",
    "            # Verify docstore size AFTER index creation\n",
    "            try:\n",
    "                docstore = index.storage_context.docstore\n",
    "                if hasattr(docstore, 'docs'):\n",
    "                    doc_count = len(docstore.docs)\n",
    "                    print(f\"   Documents in docstore AFTER index creation: {doc_count}\")\n",
    "                    if doc_count == 0:\n",
    "                        print(f\"   \u26a0\ufe0f  WARNING: Docstore is empty after index creation!\")\n",
    "                        print(f\"   This means the storage_context wasn't preserved.\")\n",
    "                elif hasattr(docstore, 'get_all_document_hashes'):\n",
    "                    # Alternative way to check docstore size\n",
    "                    all_hashes = docstore.get_all_document_hashes()\n",
    "                    print(f\"   Documents in docstore AFTER index creation: {len(all_hashes)}\")\n",
    "                else:\n",
    "                    # Try to access directly (using hasattr to avoid linter errors)\n",
    "                    if hasattr(docstore, '_node_id_to_ref_doc_info'):\n",
    "                        ref_doc_info = getattr(docstore, '_node_id_to_ref_doc_info', {})\n",
    "                        ref_doc_count = len(ref_doc_info) if ref_doc_info else 0\n",
    "                        print(f\"   Node references in docstore: {ref_doc_count}\")\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"   Could not verify docstore size: {e}\")\n",
    "                    import traceback\n",
    "                    traceback.print_exc()\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error creating index: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    # Step 6: Create retriever\n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 6: Creating VectorIndexRetriever...\")\n",
    "        print(f\"   Similarity top_k: {similarity_top_k}\")\n",
    "    try:\n",
    "        retriever = VectorIndexRetriever(\n",
    "            index=index,\n",
    "            similarity_top_k=similarity_top_k,\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"   \u2705 Retriever created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error creating retriever: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Step 7: Create query engine with post-processors\n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 7: Creating RetrieverQueryEngine...\")\n",
    "        print(f\"   Similarity cutoff: {similarity_cutoff}\")\n",
    "    try:\n",
    "        query_engine = RetrieverQueryEngine(\n",
    "            retriever=retriever,\n",
    "            node_postprocessors=[\n",
    "                SimilarityPostprocessor(similarity_cutoff=similarity_cutoff)\n",
    "            ]\n",
    "        )\n",
    "        if debug:\n",
    "            print(f\"   \u2705 Query engine created successfully\")\n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c Error creating query engine: {e}\")\n",
    "        raise\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\u2705 Query engine ready!\")\n",
    "        print(f\"   Retrieval: top {similarity_top_k} chunks\")\n",
    "        print(f\"   Min similarity: {similarity_cutoff}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "    else:\n",
    "        print(\"\u2705 Query engine ready!\")\n",
    "        print(f\"   Retrieval: top {similarity_top_k} chunks\")\n",
    "        print(f\"   Min similarity: {similarity_cutoff}\\n\")\n",
    "    \n",
    "    return query_engine\n",
    "\n",
    "# Example: Create query engine (uncomment to run)\n",
    "# Note: similarity_cutoff of 0.3 is now the default (lowered from 0.7)\n",
    "# Adjust based on your needs - lower = more results, higher = more relevant\n",
    "query_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    similarity_cutoff=0.3,  # Default is now 0.3\n",
    "    debug=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ok19r4un2gm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "\ud83d\udd27 DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "\ud83d\udccb Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "\ud83d\udccb Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "\ud83d\udccb Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "\ud83d\udccb Step 4: Creating PGVectorStore...\n",
      "   \u2705 PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "\ud83d\udccb Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   \u2705 Added 568 nodes to docstore\n",
      "   \u2705 Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "\ud83d\udccb Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 5\n",
      "   \u2705 Retriever created successfully\n",
      "\n",
      "\ud83d\udccb Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.3\n",
      "   \u2705 Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query engine ready!\n",
      "   Retrieval: top 5 chunks\n",
      "   Min similarity: 0.3\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d DEBUG: Querying RAG System\n",
      "================================================================================\n",
      "\ud83d\udccb Using provided query engine: <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "   Retriever: <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "   Post-processors: 1\n",
      "\n",
      "\ud83d\udccb Question: What are the impacts of sea level rise on groundwater in Honolulu?\n",
      "\n",
      "\ud83d\udccb Step 1: Generating query embedding...\n",
      "   Retrieving nodes with retriever...\n",
      "   \u2705 Retrieved 0 nodes from retriever\n",
      "   \u26a0\ufe0f  WARNING: Retriever returned 0 nodes!\n",
      "   This means the issue is in retrieval, not response synthesis.\n",
      "\n",
      "\ud83d\udccb Step 2: Calling query_engine.query()...\n",
      "   This will: retrieve nodes \u2192 apply post-processors \u2192 synthesize response\n",
      "   Executing: response = query_engine.query(question)\n",
      "   \u2705 Query completed successfully\n",
      "\n",
      "\ud83d\udccb Step 3: Analyzing response...\n",
      "   Response type: <class 'llama_index.core.base.response.schema.Response'>\n",
      "   Has response text: True\n",
      "   Response text length: 14\n",
      "   Response text preview: Empty Response...\n",
      "   Has source_nodes: True\n",
      "   Number of source nodes: 0\n",
      "   \u26a0\ufe0f  WARNING: Response has 0 source nodes!\n",
      "   This could mean:\n",
      "      - All nodes were filtered by SimilarityPostprocessor\n",
      "      - Response synthesis failed\n",
      "      - No nodes were retrieved\n",
      "   Response metadata: None\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query completed\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Empty Response\n",
      "\n",
      "================================================================================\n",
      "SOURCES (0 chunks):\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def rag_query_llamaindex(\n",
    "    question: str,\n",
    "    query_engine=None,\n",
    "    similarity_top_k: int = 5,\n",
    "    similarity_cutoff: float = 0.3,  # Lowered from 0.7 to get more results\n",
    "    response_mode: str = \"compact\",\n",
    "    debug: bool = True  # Enable detailed debugging\n",
    "):\n",
    "    \"\"\"\n",
    "    Query the RAG system using LlamaIndex.\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        query_engine: Pre-configured query engine (if None, will create one)\n",
    "        similarity_top_k: Number of chunks to retrieve\n",
    "        similarity_cutoff: Minimum similarity score (lower = more results, default 0.3)\n",
    "        response_mode: How to synthesize response (\"compact\", \"tree_summarize\", \"simple_summarize\")\n",
    "        debug: Enable detailed debugging output\n",
    "        \n",
    "    Returns:\n",
    "        LlamaIndex Response object with answer and source nodes\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\ud83d\udd0d DEBUG: Querying RAG System\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "    # Create query engine if not provided\n",
    "    if query_engine is None:\n",
    "        if debug:\n",
    "            print(\"\ud83d\udccb Creating new query engine...\")\n",
    "        query_engine = create_llamaindex_query_engine(\n",
    "            similarity_top_k=similarity_top_k,\n",
    "            similarity_cutoff=similarity_cutoff,\n",
    "            debug=debug\n",
    "        )\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"\ud83d\udccb Using provided query engine: {type(query_engine)}\")\n",
    "            print(f\"   Retriever: {type(query_engine._retriever)}\")\n",
    "            print(f\"   Post-processors: {len(query_engine._node_postprocessors)}\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Question: {question}\")\n",
    "        print(f\"\\n\ud83d\udccb Step 1: Generating query embedding...\")\n",
    "    \n",
    "    # Step 1: Test retrieval BEFORE query to see what's happening\n",
    "    if debug:\n",
    "        try:\n",
    "            from llama_index.core import QueryBundle\n",
    "            retriever = query_engine._retriever\n",
    "            query_bundle = QueryBundle(question)\n",
    "            \n",
    "            print(f\"   Retrieving nodes with retriever...\")\n",
    "            nodes_before_query = retriever.retrieve(query_bundle)\n",
    "            print(f\"   \u2705 Retrieved {len(nodes_before_query)} nodes from retriever\")\n",
    "            \n",
    "            if len(nodes_before_query) > 0:\n",
    "                print(f\"\\n   Top {min(3, len(nodes_before_query))} nodes BEFORE query engine:\")\n",
    "                for i, node in enumerate(nodes_before_query[:3], 1):\n",
    "                    print(f\"      {i}. Score: {node.score:.4f}\")\n",
    "                    print(f\"         File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "                    print(f\"         Text preview: {node.text[:100]}...\")\n",
    "            else:\n",
    "                print(f\"   \u26a0\ufe0f  WARNING: Retriever returned 0 nodes!\")\n",
    "                print(f\"   This means the issue is in retrieval, not response synthesis.\")\n",
    "        except Exception as e:\n",
    "            print(f\"   \u26a0\ufe0f  Could not test retrieval: {e}\")\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"\\n\ud83d\udccb Step 2: Calling query_engine.query()...\")\n",
    "        print(f\"   This will: retrieve nodes \u2192 apply post-processors \u2192 synthesize response\")\n",
    "    \n",
    "    # Query the engine - THIS IS THE LINE WITH THE ISSUE\n",
    "    try:\n",
    "        if debug:\n",
    "            print(f\"   Executing: response = query_engine.query(question)\")\n",
    "        response = query_engine.query(question)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"   \u2705 Query completed successfully\")\n",
    "            print(f\"\\n\ud83d\udccb Step 3: Analyzing response...\")\n",
    "            print(f\"   Response type: {type(response)}\")\n",
    "            print(f\"   Has response text: {hasattr(response, 'response')}\")\n",
    "            \n",
    "            if hasattr(response, 'response'):\n",
    "                response_text = getattr(response, 'response', None)\n",
    "                if response_text:\n",
    "                    response_str = str(response_text)\n",
    "                    print(f\"   Response text length: {len(response_str)}\")\n",
    "                    print(f\"   Response text preview: {response_str[:100]}...\")\n",
    "                else:\n",
    "                    print(f\"   Response text: None/Empty\")\n",
    "            \n",
    "            print(f\"   Has source_nodes: {hasattr(response, 'source_nodes')}\")\n",
    "            if hasattr(response, 'source_nodes'):\n",
    "                print(f\"   Number of source nodes: {len(response.source_nodes)}\")\n",
    "                \n",
    "                if len(response.source_nodes) > 0:\n",
    "                    print(f\"\\n   Source nodes details:\")\n",
    "                    for i, node in enumerate(response.source_nodes[:3], 1):\n",
    "                        print(f\"      {i}. Score: {node.score:.4f}\")\n",
    "                        print(f\"         File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "                else:\n",
    "                    print(f\"   \u26a0\ufe0f  WARNING: Response has 0 source nodes!\")\n",
    "                    print(f\"   This could mean:\")\n",
    "                    print(f\"      - All nodes were filtered by SimilarityPostprocessor\")\n",
    "                    print(f\"      - Response synthesis failed\")\n",
    "                    print(f\"      - No nodes were retrieved\")\n",
    "            \n",
    "            if hasattr(response, 'metadata'):\n",
    "                print(f\"   Response metadata: {response.metadata}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"   \u274c ERROR during query_engine.query(): {e}\")\n",
    "        print(f\"   Error type: {type(e).__name__}\")\n",
    "        import traceback\n",
    "        print(f\"   Traceback:\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "    \n",
    "    if debug:\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"\u2705 Query completed\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "    return response\n",
    "\n",
    "def debug_retrieval(question: str, query_engine, top_k: int = 10):\n",
    "    \"\"\"\n",
    "    Debug function to see what chunks are being retrieved before filtering.\n",
    "    \"\"\"\n",
    "    from llama_index.core import QueryBundle\n",
    "    \n",
    "    print(f\"\ud83d\udd0d Debug: Retrieving top {top_k} chunks for: {question}\\n\")\n",
    "    \n",
    "    # Get the retriever from the query engine\n",
    "    retriever = query_engine._retriever\n",
    "    \n",
    "    # Retrieve nodes\n",
    "    query_bundle = QueryBundle(question)\n",
    "    nodes = retriever.retrieve(query_bundle)\n",
    "    \n",
    "    print(f\"\ud83d\udcca Retrieved {len(nodes)} chunks (before post-processing):\\n\")\n",
    "    for i, node in enumerate(nodes[:top_k], 1):\n",
    "        print(f\"{i}. Similarity: {node.score:.4f}\")\n",
    "        print(f\"   File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"   Layers: {node.metadata.get('relevant_layers', [])}\")\n",
    "        print(f\"   Text preview: {node.text[:150]}...\\n\")\n",
    "    \n",
    "    return nodes\n",
    "\n",
    "def print_rag_response(response):\n",
    "    \"\"\"\n",
    "    Pretty print a LlamaIndex response with sources.\n",
    "    \n",
    "    Args:\n",
    "        response: LlamaIndex Response object\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ANSWER:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(response.response)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"SOURCES ({len(response.source_nodes)} chunks):\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, node in enumerate(response.source_nodes, 1):\n",
    "        print(f\"\\n[Source {i}]\")\n",
    "        print(f\"  File: {node.metadata.get('filename', 'Unknown')}\")\n",
    "        print(f\"  Similarity: {node.score:.3f}\")\n",
    "        print(f\"  Confidence: {node.metadata.get('confidence', 'Unknown')}\")\n",
    "        print(f\"  Layers: {node.metadata.get('relevant_layers', [])}\")\n",
    "        \n",
    "        if node.metadata.get('locations'):\n",
    "            print(f\"  Locations: {', '.join(node.metadata['locations'])}\")\n",
    "        if node.metadata.get('slr_projections'):\n",
    "            print(f\"  SLR Projections: {', '.join(node.metadata['slr_projections'])}\")\n",
    "        \n",
    "        print(f\"  Text: {node.text[:200]}...\")\n",
    "\n",
    "# Example usage\n",
    "# Create query engine with lower similarity cutoff to get more results\n",
    "query_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=5,\n",
    "    similarity_cutoff=0.3  # Lower threshold - adjust based on your needs\n",
    ")\n",
    "\n",
    "response = rag_query_llamaindex(\n",
    "    question=\"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    query_engine=query_engine\n",
    ")\n",
    "\n",
    "print_rag_response(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a668e332",
   "metadata": {},
   "source": [
    "# Diagnostics: Debugging Empty Response\n",
    "\n",
    "The following cells diagnose why vector search is returning 0 chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fac062ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udcca Total chunks in database: 568\n",
      "\ud83d\udcca Chunks with embeddings: 568\n",
      "\n",
      "\ud83d\udccb Sample chunks from database:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Filename: Vitousek_SCD08.md\n",
      "Confidence: HIGH\n",
      "Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "Locations: ['Windward Oahu', 'Waimanalo', 'Oahu']\n",
      "Text preview: ## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "This paper outlines a practical approach to mapping extreme wave inundation and the inf...\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Filename: Vitousek_SCD08.md\n",
      "Confidence: HIGH\n",
      "Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "Locations: ['Windward Oahu', 'Waimanalo', 'Oahu']\n",
      "Text preview: Our approach follows Ruggiero et. ...\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Filename: Vitousek_SCD08.md\n",
      "Confidence: HIGH\n",
      "Layers: ['annual_high_wave_flooding', 'future_erosion_hazard_zone']\n",
      "Locations: ['Windward Oahu', 'Waimanalo', 'Oahu']\n",
      "Text preview: al. who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of the Oregon coast.\n",
      "\n",
      "In this study, NOAA's wave-moni...\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd0d DIAGNOSTIC: Check what's actually in the database\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(\"postgresql://***:***@localhost:5432/climate_viewer_dev\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Count total chunks\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM document_chunks\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"\ud83d\udcca Total chunks in database: {count}\")\n",
    "    \n",
    "    # Check if embeddings exist\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM document_chunks WHERE embedding IS NOT NULL\"))\n",
    "    embedding_count = result.scalar()\n",
    "    print(f\"\ud83d\udcca Chunks with embeddings: {embedding_count}\")\n",
    "    \n",
    "    # Sample some chunks to verify content\n",
    "    result = conn.execute(text(\"\"\"\n",
    "        SELECT text, filename, confidence, relevant_layers, locations \n",
    "        FROM document_chunks \n",
    "        LIMIT 3\n",
    "    \"\"\"))\n",
    "    print(\"\\n\ud83d\udccb Sample chunks from database:\")\n",
    "    for i, row in enumerate(result, 1):\n",
    "        print(f\"\\n--- Chunk {i} ---\")\n",
    "        print(f\"Filename: {row[1]}\")\n",
    "        print(f\"Confidence: {row[2]}\")\n",
    "        print(f\"Layers: {row[3]}\")\n",
    "        print(f\"Locations: {row[4]}\")\n",
    "        print(f\"Text preview: {row[0][:200]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d57e14d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Querying with similarity_cutoff=0.0 (no filtering)...\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd27 DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "\ud83d\udccb Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "\ud83d\udccb Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "\ud83d\udccb Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "\ud83d\udccb Step 4: Creating PGVectorStore...\n",
      "   \u2705 PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "\ud83d\udccb Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   \u2705 Added 568 nodes to docstore\n",
      "   \u2705 Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "\ud83d\udccb Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 10\n",
      "   \u2705 Retriever created successfully\n",
      "\n",
      "\ud83d\udccb Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.0\n",
      "   \u2705 Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query engine ready!\n",
      "   Retrieval: top 10 chunks\n",
      "   Min similarity: 0.0\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d DEBUG: Querying RAG System\n",
      "================================================================================\n",
      "\ud83d\udccb Using provided query engine: <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "   Retriever: <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "   Post-processors: 1\n",
      "\n",
      "\ud83d\udccb Question: What are the impacts of sea level rise on groundwater in Honolulu?\n",
      "\n",
      "\ud83d\udccb Step 1: Generating query embedding...\n",
      "   Retrieving nodes with retriever...\n",
      "   \u2705 Retrieved 0 nodes from retriever\n",
      "   \u26a0\ufe0f  WARNING: Retriever returned 0 nodes!\n",
      "   This means the issue is in retrieval, not response synthesis.\n",
      "\n",
      "\ud83d\udccb Step 2: Calling query_engine.query()...\n",
      "   This will: retrieve nodes \u2192 apply post-processors \u2192 synthesize response\n",
      "   Executing: response = query_engine.query(question)\n",
      "   \u2705 Query completed successfully\n",
      "\n",
      "\ud83d\udccb Step 3: Analyzing response...\n",
      "   Response type: <class 'llama_index.core.base.response.schema.Response'>\n",
      "   Has response text: True\n",
      "   Response text length: 14\n",
      "   Response text preview: Empty Response...\n",
      "   Has source_nodes: True\n",
      "   Number of source nodes: 0\n",
      "   \u26a0\ufe0f  WARNING: Response has 0 source nodes!\n",
      "   This could mean:\n",
      "      - All nodes were filtered by SimilarityPostprocessor\n",
      "      - Response synthesis failed\n",
      "      - No nodes were retrieved\n",
      "   Response metadata: None\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query completed\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ANSWER:\n",
      "================================================================================\n",
      "Empty Response\n",
      "\n",
      "================================================================================\n",
      "SOURCES (0 chunks):\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd0d DIAGNOSTIC: Try query with NO similarity threshold\n",
    "print(\"\ud83d\udd0d Querying with similarity_cutoff=0.0 (no filtering)...\\n\")\n",
    "\n",
    "test_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    similarity_cutoff=0.0  # Accept ALL results\n",
    ")\n",
    "\n",
    "result = rag_query_llamaindex(\n",
    "    \"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    query_engine=test_engine\n",
    ")\n",
    "\n",
    "print_rag_response(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1cdb54db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83e\udd16 Current embedding model: model_name='text-embedding-3-small' embed_batch_size=100 callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x10a2c41d0> num_workers=None embeddings_cache=None additional_kwargs={} api_key='sk-***REDACTED***' api_base='https://api.openai.com/v1' api_version='' max_retries=10 timeout=60.0 default_headers=None reuse_client=True dimensions=None\n",
      "   Model name: text-embedding-3-small\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd0d DIAGNOSTIC: Check which embedding model is being used\n",
    "from llama_index.core import Settings\n",
    "\n",
    "print(f\"\ud83e\udd16 Current embedding model: {Settings.embed_model}\")\n",
    "print(f\"   Model name: {Settings.embed_model.model_name if hasattr(Settings.embed_model, 'model_name') else 'Unknown'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5e22889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Testing raw retrieval (no filtering)...\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mretrievers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m VectorIndexRetriever\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\ud83d\udd0d Testing raw retrieval (no filtering)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      6\u001b[39m retriever = VectorIndexRetriever(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     index=\u001b[43mindex\u001b[49m,\n\u001b[32m      8\u001b[39m     similarity_top_k=\u001b[32m10\u001b[39m,  \u001b[38;5;66;03m# Get more results\u001b[39;00m\n\u001b[32m      9\u001b[39m )\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Test the same query\u001b[39;00m\n\u001b[32m     12\u001b[39m test_question = \u001b[33m\"\u001b[39m\u001b[33mWhat are the impacts of sea level rise on groundwater in Honolulu?\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd0d DIAGNOSTIC: Test raw retrieval WITHOUT similarity filtering\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "\n",
    "print(\"\ud83d\udd0d Testing raw retrieval (no filtering)...\\n\")\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=10,  # Get more results\n",
    ")\n",
    "\n",
    "# Test the same query\n",
    "test_question = \"What are the impacts of sea level rise on groundwater in Honolulu?\"\n",
    "nodes = retriever.retrieve(test_question)\n",
    "\n",
    "print(f\"\ud83d\udcca Retrieved {len(nodes)} nodes BEFORE similarity filtering:\\n\")\n",
    "for i, node in enumerate(nodes[:5], 1):  # Show top 5\n",
    "    print(f\"{i}. Similarity Score: {node.score:.4f}\")\n",
    "    print(f\"   Text preview: {node.text[:200]}...\")\n",
    "    print()\n",
    "\n",
    "if len(nodes) == 0:\n",
    "    print(\"\u26a0\ufe0f  WARNING: No nodes retrieved even without filtering!\")\n",
    "    print(\"   This suggests an issue with the index or embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7c6a099f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ud83d\udd0d Querying with similarity_cutoff=0.0 (no filtering)...\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd27 DEBUG: Creating LlamaIndex Query Engine\n",
      "================================================================================\n",
      "\ud83d\udccb Step 1: Database URL from environment\n",
      "   URL: postgresql://***:***@localhost:5432/climate_viewer_dev\n",
      "\n",
      "\ud83d\udccb Step 2: Parsing connection string...\n",
      "   Host: localhost\n",
      "   Port: 5432\n",
      "   User: dev_user\n",
      "   Database: climate_viewer_dev\n",
      "   Table: document_chunks\n",
      "\n",
      "\ud83d\udccb Step 3: Verifying database connection and table...\n",
      "   Table exists: True\n",
      "   Total rows: 568\n",
      "   Rows with embeddings: 568\n",
      "\n",
      "\ud83d\udccb Step 4: Creating PGVectorStore...\n",
      "   \u2705 PGVectorStore created successfully\n",
      "   Embedding dimension: 1536\n",
      "\n",
      "\ud83d\udccb Step 5: Creating VectorStoreIndex from vector store...\n",
      "   Loading nodes from database to populate docstore...\n",
      "   Loaded 568 nodes from database\n",
      "   \u2705 Added 568 nodes to docstore\n",
      "   \u2705 Index created successfully\n",
      "   Documents in docstore: 0\n",
      "\n",
      "\ud83d\udccb Step 6: Creating VectorIndexRetriever...\n",
      "   Similarity top_k: 10\n",
      "   \u2705 Retriever created successfully\n",
      "\n",
      "\ud83d\udccb Step 7: Creating RetrieverQueryEngine...\n",
      "   Similarity cutoff: 0.0\n",
      "   \u2705 Query engine created successfully\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query engine ready!\n",
      "   Retrieval: top 10 chunks\n",
      "   Min similarity: 0.0\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "\ud83d\udd0d DEBUG: Querying RAG System\n",
      "================================================================================\n",
      "\ud83d\udccb Using provided query engine: <class 'llama_index.core.query_engine.retriever_query_engine.RetrieverQueryEngine'>\n",
      "   Retriever: <class 'llama_index.core.indices.vector_store.retrievers.retriever.VectorIndexRetriever'>\n",
      "   Post-processors: 1\n",
      "\n",
      "\ud83d\udccb Question: What are the impacts of sea level rise on groundwater in Honolulu?\n",
      "\n",
      "\ud83d\udccb Step 1: Generating query embedding...\n",
      "   Retrieving nodes with retriever...\n",
      "   \u2705 Retrieved 0 nodes from retriever\n",
      "   \u26a0\ufe0f  WARNING: Retriever returned 0 nodes!\n",
      "   This means the issue is in retrieval, not response synthesis.\n",
      "\n",
      "\ud83d\udccb Step 2: Calling query_engine.query()...\n",
      "   This will: retrieve nodes \u2192 apply post-processors \u2192 synthesize response\n",
      "   Executing: response = query_engine.query(question)\n",
      "   \u2705 Query completed successfully\n",
      "\n",
      "\ud83d\udccb Step 3: Analyzing response...\n",
      "   Response type: <class 'llama_index.core.base.response.schema.Response'>\n",
      "   Has response text: True\n",
      "   Response text length: 14\n",
      "   Response text preview: Empty Response...\n",
      "   Has source_nodes: True\n",
      "   Number of source nodes: 0\n",
      "   \u26a0\ufe0f  WARNING: Response has 0 source nodes!\n",
      "   This could mean:\n",
      "      - All nodes were filtered by SimilarityPostprocessor\n",
      "      - Response synthesis failed\n",
      "      - No nodes were retrieved\n",
      "   Response metadata: None\n",
      "\n",
      "================================================================================\n",
      "\u2705 Query completed\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd0d DIAGNOSTIC: Try query with NO similarity threshold\n",
    "print(\"\ud83d\udd0d Querying with similarity_cutoff=0.0 (no filtering)...\\n\")\n",
    "\n",
    "test_engine = create_llamaindex_query_engine(\n",
    "    similarity_top_k=10,\n",
    "    similarity_cutoff=0.0  # Accept ALL results\n",
    ")\n",
    "\n",
    "result = rag_query_llamaindex(\n",
    "    \"What are the impacts of sea level rise on groundwater in Honolulu?\",\n",
    "    query_engine=test_engine\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2lbaiajp10y",
   "metadata": {},
   "source": [
    "## Advanced RAG Features with LlamaIndex\n",
    "\n",
    "LlamaIndex provides powerful features out of the box:\n",
    "\n",
    "### 1. **Query Engines**\n",
    "- **Compact Mode**: Concatenates chunks and sends to LLM (default)\n",
    "- **Tree Summarize**: Hierarchical summarization for long contexts\n",
    "- **Simple Summarize**: Simple concatenation with summarization\n",
    "\n",
    "### 2. **Retrieval Modes**\n",
    "- **Vector Search**: Semantic similarity (what we're using)\n",
    "- **Hybrid Search**: Combines vector + keyword search\n",
    "- **Auto-Retrieval**: LLM-powered query planning\n",
    "\n",
    "### 3. **Post-Processors**\n",
    "- **SimilarityPostprocessor**: Filter by similarity threshold\n",
    "- **KeywordNodePostprocessor**: Filter by keywords\n",
    "- **MetadataReplacementPostProcessor**: Replace node text with metadata\n",
    "- **SentenceEmbeddingOptimizer**: Optimize context window usage\n",
    "\n",
    "### 4. **Metadata Filtering**\n",
    "You can filter by your custom metadata:\n",
    "```python\n",
    "from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter\n",
    "\n",
    "filters = MetadataFilters(filters=[\n",
    "    ExactMatchFilter(key=\"confidence\", value=\"HIGH\"),\n",
    "    ExactMatchFilter(key=\"relevant_layers\", value=\"groundwater_inundation\")\n",
    "])\n",
    "\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    "    filters=filters\n",
    ")\n",
    "```\n",
    "\n",
    "### 5. **Chat Engine** (Multi-turn conversations)\n",
    "```python\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_question\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Tell me about groundwater impacts\")\n",
    "response = chat_engine.chat(\"What about Honolulu specifically?\")  # Remembers context\n",
    "```\n",
    "\n",
    "### 6. **Streaming Responses**\n",
    "```python\n",
    "streaming_response = query_engine.query(\"Your question...\")\n",
    "for text in streaming_response.response_gen:\n",
    "    print(text, end=\"\")\n",
    "```\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Deploy as API**: Wrap this in FastAPI for web access\n",
    "2. **Add caching**: Use LlamaIndex's caching for faster responses\n",
    "3. **Implement chat history**: Store conversation context\n",
    "4. **Add reranking**: Use cross-encoder models for better retrieval\n",
    "5. **Custom prompts**: Tailor system prompts for your use case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extm57lrba8",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "Combine vector search with LLM to answer questions using your document database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11098100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks in database: 568\n",
      "\n",
      "Sample chunks:\n",
      "\n",
      "Text preview: ## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\n",
      "\n",
      "This paper outlines a practical approach to mapping extreme wave inundation and the inf...\n",
      "Metadata: (\"## A PRACTICAL APPROACH TO MAPPING EXTREME WAVE INUNDATION: CONSEQUENCES OF SEA-LEVEL RISE AND COASTAL EROSION.\\n\\nThis paper outlines a practical ap ... (3353 characters truncated) ... ontal error (Circular Error Envelope - CE95) and a vertical accuracy of 1 cm.\\n\\nThe evaluation of runup elevations requires a statistical approach. \",)\n",
      "\n",
      "Text preview: Our approach follows Ruggiero et. ...\n",
      "Metadata: ('Our approach follows Ruggiero et. ',)\n",
      "\n",
      "Text preview: al. who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of the Oregon coast.\n",
      "\n",
      "In this study, NOAA's wave-moni...\n",
      "Metadata: (\"al. who estimated extreme water levels (sum of extreme runup and extreme tides) to determine the frequency of dune impact and resulting morphology of ... (1513 characters truncated) ...  following the method of Fletcher et. al. Annual Erosion Hazard Rates (AEHR) are determined using basis function methods following Frazer et. al.\\n\\n\",)\n",
      "\n",
      "Text preview: In approach #3, a third generation spectral wave model, SWAN (Simulating WAves Nearshore - Booij et. al....\n",
      "Metadata: ('In approach #3, a third generation spectral wave model, SWAN (Simulating WAves Nearshore - Booij et. al.',)\n",
      "\n",
      "Text preview: ; Ris et. ...\n",
      "Metadata: ('; Ris et. ',)\n"
     ]
    }
   ],
   "source": [
    "# Check database content\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "engine = create_engine(\"postgresql://***:***@localhost:5432/climate_viewer_dev\")\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    # Count total chunks\n",
    "    result = conn.execute(text(\"SELECT COUNT(*) FROM document_chunks\"))\n",
    "    count = result.scalar()\n",
    "    print(f\"Total chunks in database: {count}\")\n",
    "    \n",
    "    # Sample some text\n",
    "    result = conn.execute(text(\"SELECT text FROM document_chunks LIMIT 5\"))\n",
    "    print(\"\\nSample chunks:\")\n",
    "    for row in result:\n",
    "        print(f\"\\nText preview: {row[0][:200]}...\")\n",
    "        print(f\"Metadata: {row}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5750a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}